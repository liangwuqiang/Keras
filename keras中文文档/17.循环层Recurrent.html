<!DOCTYPE html>
    <html><head><meta charset="UTF-8">
    </head><body>
    <p><a href="http://keras-cn.readthedocs.io/en/latest/">原文链接</a></p>
        <div class="section">
<h1 id="recurrent">循环层Recurrent</h1>
<h2 id="recurrent_1">Recurrent层</h2>
<pre><code class="python">keras.layers.recurrent.Recurrent(return_sequences=False, go_backwards=False, stateful=False, unroll=False, implementation=0)
</code></pre>
<p>这是循环层的抽象类，请不要在模型中直接应用该层（因为它是抽象类，无法实例化任何对象）。请使用它的子类<code>LSTM</code>，<code>GRU</code>或<code>SimpleRNN</code>。</p>
<p>所有的循环层（<code>LSTM</code>,<code>GRU</code>,<code>SimpleRNN</code>）都服从本层的性质，并接受本层指定的所有关键字参数。</p>
<h3 id="_1">参数</h3>
<ul>
<li>
<p>weights：numpy array的list，用以初始化权重。该list形如<code>[(input_dim, output_dim),(output_dim, output_dim),(output_dim,)]</code></p>
</li>
<li>
<p>return_sequences：布尔值，默认<code>False</code>，控制返回类型。若为<code>True</code>则返回整个序列，否则仅返回输出序列的最后一个输出</p>
</li>
<li>
<p>go_backwards：布尔值，默认为<code>False</code>，若为<code>True</code>，则逆向处理输入序列并返回逆序后的序列</p>
</li>
<li>
<p>stateful：布尔值，默认为<code>False</code>，若为<code>True</code>，则一个batch中下标为i的样本的最终状态将会用作下一个batch同样下标的样本的初始状态。</p>
</li>
<li>
<p>unroll：布尔值，默认为<code>False</code>，若为<code>True</code>，则循环层将被展开，否则就使用符号化的循环。当使用TensorFlow为后端时，循环网络本来就是展开的，因此该层不做任何事情。层展开会占用更多的内存，但会加速RNN的运算。层展开只适用于短序列。</p>
</li>
<li>
<p>implementation：0，1或2， 若为0，则RNN将以更少但是更大的矩阵乘法实现，因此在CPU上运行更快，但消耗更多的内存。如果设为1，则RNN将以更多但更小的矩阵乘法实现，因此在CPU上运行更慢，在GPU上运行更快，并且消耗更少的内存。如果设为2（仅LSTM和GRU可以设为2），则RNN将把输入门、遗忘门和输出门合并为单个矩阵，以获得更加在GPU上更加高效的实现。注意，RNN dropout必须在所有门上共享，并导致正则效果性能微弱降低。</p>
</li>
<li>
<p>input_dim：输入维度，当使用该层为模型首层时，应指定该值（或等价的指定input_shape)</p>
</li>
<li>
<p>input_length：当输入序列的长度固定时，该参数为输入序列的长度。当需要在该层后连接<code>Flatten</code>层，然后又要连接<code>Dense</code>层时，需要指定该参数，否则全连接的输出无法计算出来。注意，如果循环层不是网络的第一层，你需要在网络的第一层中指定序列的长度（通过<code>input_shape</code>指定）。</p>
</li>
</ul>
<h3 id="shape">输入shape</h3>
<p>形如（samples，timesteps，input_dim）的3D张量</p>
<h3 id="shape_1">输出shape</h3>
<p>如果<code>return_sequences=True</code>：返回形如（samples，timesteps，output_dim）的3D张量</p>
<p>否则，返回形如（samples，output_dim）的2D张量</p>
<h3 id="_2">例子</h3>
<pre><code class="python"># as the first layer in a Sequential model
model = Sequential()
model.add(LSTM(32, input_shape=(10, 64)))
# now model.output_shape == (None, 32)
# note: `None` is the batch dimension.

# the following is identical:
model = Sequential()
model.add(LSTM(32, input_dim=64, input_length=10))

# for subsequent layers, no need to specify the input size:
         model.add(LSTM(16))

# to stack recurrent layers, you must use return_sequences=True
# on any recurrent layer that feeds into another recurrent layer.
# note that you only need to specify the input size on the first layer.
model = Sequential()
model.add(LSTM(64, input_dim=64, input_length=10, return_sequences=True))
model.add(LSTM(32, return_sequences=True))
model.add(LSTM(10))
</code></pre>
<h3 id="rnn">指定RNN初始状态的注意事项</h3>
<p>可以通过设置<code>initial_state</code>用符号式的方式指定RNN层的初始状态。即，<code>initial_stat</code>的值应该为一个tensor或一个tensor列表，代表RNN层的初始状态。</p>
<p>也可以通过设置<code>reset_states</code>参数用数值的方法设置RNN的初始状态，状态的值应该为numpy数组或numpy数组的列表，代表RNN层的初始状态。</p>
<h3 id="masking">屏蔽输入数据（Masking）</h3>
<p>循环层支持通过时间步变量对输入数据进行Masking，如果想将输入数据的一部分屏蔽掉，请使用<a href="../embedding_layer">Embedding</a>层并将参数<code>mask_zero</code>设为<code>True</code>。</p>
<h3 id="rnn_1">使用状态RNN的注意事项</h3>
<p>可以将RNN设置为‘stateful’，意味着由每个batch计算出的状态都会被重用于初始化下一个batch的初始状态。状态RNN假设连续的两个batch之中，相同下标的元素有一一映射关系。</p>
<p>要启用状态RNN，请在实例化层对象时指定参数<code>stateful=True</code>，并在Sequential模型使用固定大小的batch：通过在模型的第一层传入<code>batch_size=(...)</code>和<code>input_shape</code>来实现。在函数式模型中，对所有的输入都要指定相同的<code>batch_size</code>。</p>
<p>如果要将循环层的状态重置，请调用<code>.reset_states()</code>，对模型调用将重置模型中所有状态RNN的状态。对单个层调用则只重置该层的状态。</p>
<hr/>
<h2 id="simplernn">SimpleRNN层</h2>
<pre><code class="python">keras.layers.recurrent.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)
</code></pre>
<p>全连接RNN网络，RNN的输出会被回馈到输入</p>
<h3 id="_3">参数</h3>
<ul>
<li>
<p>units：输出维度</p>
</li>
<li>
<p>activation：激活函数，为预定义的激活函数名（参考<a href="../../other/activations">激活函数</a>）</p>
</li>
<li>
<p>use_bias: 布尔值，是否使用偏置项</p>
</li>
<li>
<p>kernel_initializer：权值初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考<a href="../../other/initializations">initializers</a></p>
</li>
<li>
<p>recurrent_initializer：循环核的初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考<a href="../../other/initializations">initializers</a></p>
</li>
<li>
<p>bias_initializer：权值初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考<a href="../../other/initializations">initializers</a></p>
</li>
<li>
<p>kernel_regularizer：施加在权重上的正则项，为<a href="../../other/regularizers">Regularizer</a>对象</p>
</li>
<li>
<p>bias_regularizer：施加在偏置向量上的正则项，为<a href="../../other/regularizers">Regularizer</a>对象</p>
</li>
<li>
<p>recurrent_regularizer：施加在循环核上的正则项，为<a href="../../other/regularizers">Regularizer</a>对象</p>
</li>
<li>
<p>activity_regularizer：施加在输出上的正则项，为<a href="../../other/regularizers">Regularizer</a>对象</p>
</li>
<li>
<p>kernel_constraints：施加在权重上的约束项，为<a href="../../other/constraints">Constraints</a>对象</p>
</li>
<li>
<p>recurrent_constraints：施加在循环核上的约束项，为<a href="../../other/constraints">Constraints</a>对象</p>
</li>
<li>
<p>bias_constraints：施加在偏置上的约束项，为<a href="../../other/constraints">Constraints</a>对象</p>
</li>
<li>
<p>dropout：0~1之间的浮点数，控制输入线性变换的神经元断开比例</p>
</li>
<li>
<p>recurrent_dropout：0~1之间的浮点数，控制循环状态的线性变换的神经元断开比例</p>
</li>
</ul>
<h3 id="_4">参考文献</h3>
<ul>
<li><a href="http://arxiv.org/abs/1512.05287">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</a></li>
</ul>
<hr/>
<h2 id="gru">GRU层</h2>
<pre><code class="python">keras.layers.recurrent.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)
</code></pre>
<p>门限循环单元（详见参考文献）</p>
<h3 id="_5">参数</h3>
<ul>
<li>
<p>units：输出维度</p>
</li>
<li>
<p>activation：激活函数，为预定义的激活函数名（参考<a href="../../other/activations">激活函数</a>）</p>
</li>
<li>
<p>use_bias: 布尔值，是否使用偏置项</p>
</li>
<li>
<p>kernel_initializer：权值初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考<a href="../../other/initializations">initializers</a></p>
</li>
<li>
<p>recurrent_initializer：循环核的初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考<a href="../../other/initializations">initializers</a></p>
</li>
<li>
<p>bias_initializer：权值初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考<a href="../../other/initializations">initializers</a></p>
</li>
<li>
<p>kernel_regularizer：施加在权重上的正则项，为<a href="../../other/regularizers">Regularizer</a>对象</p>
</li>
<li>
<p>bias_regularizer：施加在偏置向量上的正则项，为<a href="../../other/regularizers">Regularizer</a>对象</p>
</li>
<li>
<p>recurrent_regularizer：施加在循环核上的正则项，为<a href="../../other/regularizers">Regularizer</a>对象</p>
</li>
<li>
<p>activity_regularizer：施加在输出上的正则项，为<a href="../../other/regularizers">Regularizer</a>对象</p>
</li>
<li>
<p>kernel_constraints：施加在权重上的约束项，为<a href="../../other/constraints">Constraints</a>对象</p>
</li>
<li>
<p>recurrent_constraints：施加在循环核上的约束项，为<a href="../../other/constraints">Constraints</a>对象</p>
</li>
<li>
<p>bias_constraints：施加在偏置上的约束项，为<a href="../../other/constraints">Constraints</a>对象</p>
</li>
<li>
<p>dropout：0~1之间的浮点数，控制输入线性变换的神经元断开比例</p>
</li>
<li>
<p>recurrent_dropout：0~1之间的浮点数，控制循环状态的线性变换的神经元断开比例</p>
</li>
</ul>
<h3 id="_6">参考文献</h3>
<ul>
<li>
<p><a href="http://www.aclweb.org/anthology/W14-4012">On the Properties of Neural Machine Translation: Encoder–Decoder Approaches</a></p>
</li>
<li>
<p><a href="http://arxiv.org/pdf/1412.3555v1.pdf">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</a></p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1512.05287">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</a></p>
</li>
</ul>
<hr/>
<h2 id="lstm">LSTM层</h2>
<pre><code class="python">keras.layers.recurrent.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)
</code></pre>
<p>Keras长短期记忆模型，关于此算法的详情，请参考<a href="http://deeplearning.net/tutorial/lstm.html">本教程</a></p>
<h3 id="_7">参数</h3>
<ul>
<li>
<p>units：输出维度</p>
</li>
<li>
<p>activation：激活函数，为预定义的激活函数名（参考<a href="../../other/activations">激活函数</a>）</p>
</li>
<li>
<p>recurrent_activation: 为循环步施加的激活函数（参考<a href="../../other/activations">激活函数</a>）</p>
</li>
<li>
<p>use_bias: 布尔值，是否使用偏置项</p>
</li>
<li>
<p>kernel_initializer：权值初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考<a href="../../other/initializations">initializers</a></p>
</li>
<li>
<p>recurrent_initializer：循环核的初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考<a href="../../other/initializations">initializers</a></p>
</li>
<li>
<p>bias_initializer：权值初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考<a href="../../other/initializations">initializers</a></p>
</li>
<li>
<p>kernel_regularizer：施加在权重上的正则项，为<a href="../../other/regularizers">Regularizer</a>对象</p>
</li>
<li>
<p>bias_regularizer：施加在偏置向量上的正则项，为<a href="../../other/regularizers">Regularizer</a>对象</p>
</li>
<li>
<p>recurrent_regularizer：施加在循环核上的正则项，为<a href="../../other/regularizers">Regularizer</a>对象</p>
</li>
<li>
<p>activity_regularizer：施加在输出上的正则项，为<a href="../../other/regularizers">Regularizer</a>对象</p>
</li>
<li>
<p>kernel_constraints：施加在权重上的约束项，为<a href="../../other/constraints">Constraints</a>对象</p>
</li>
<li>
<p>recurrent_constraints：施加在循环核上的约束项，为<a href="../../other/constraints">Constraints</a>对象</p>
</li>
<li>
<p>bias_constraints：施加在偏置上的约束项，为<a href="../../other/constraints">Constraints</a>对象</p>
</li>
<li>
<p>dropout：0~1之间的浮点数，控制输入线性变换的神经元断开比例</p>
</li>
<li>
<p>recurrent_dropout：0~1之间的浮点数，控制循环状态的线性变换的神经元断开比例</p>
</li>
</ul>
<h3 id="_8">参考文献</h3>
<ul>
<li>
<p><a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf">Long short-term memory</a>（original 1997 paper）</p>
</li>
<li>
<p><a href="http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015">Learning to forget: Continual prediction with LSTM</a></p>
</li>
<li>
<p><a href="http://www.cs.toronto.edu/~graves/preprint.pdf">Supervised sequence labelling with recurrent neural networks</a></p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1512.05287">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</a></p>
</li>
</ul>
</div>
    </body></html>