<!DOCTYPE html>
    <html><head><meta charset="UTF-8">
    </head><body>
    <p><a href="http://keras-cn.readthedocs.io/en/latest/">原文链接</a></p>
        <div class="section">
<h1 id="batchnormalization">（批）规范化BatchNormalization</h1>
<h2 id="batchnormalization_1">BatchNormalization层</h2>
<pre><code class="python">keras.layers.normalization.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)
</code></pre>
<p>该层在每个batch上将前一层的激活值重新规范化，即使得其输出数据的均值接近0，其标准差接近1</p>
<h3 id="_1">参数</h3>
<ul>
<li>axis: 整数，指定要规范化的轴，通常为特征轴。例如在进行<code>data_format="channels_first</code>的2D卷积后，一般会设axis=1。</li>
<li>momentum: 动态均值的动量</li>
<li>epsilon：大于0的小浮点数，用于防止除0错误</li>
<li>center: 若设为True，将会将beta作为偏置加上去，否则忽略参数beta</li>
<li>scale: 若设为True，则会乘以gamma，否则不使用gamma。当下一层是线性的时，可以设False，因为scaling的操作将被下一层执行。</li>
<li>beta_initializer：beta权重的初始方法</li>
<li>gamma_initializer: gamma的初始化方法</li>
<li>moving_mean_initializer: 动态均值的初始化方法</li>
<li>moving_variance_initializer: 动态方差的初始化方法</li>
<li>beta_regularizer: 可选的beta正则</li>
<li>gamma_regularizer: 可选的gamma正则</li>
<li>beta_constraint: 可选的beta约束</li>
<li>gamma_constraint: 可选的gamma约束</li>
</ul>
<h3 id="shape">输入shape</h3>
<p>任意，当使用本层为模型首层时，指定<code>input_shape</code>参数时有意义。</p>
<h3 id="shape_1">输出shape</h3>
<p>与输入shape相同</p>
<h3 id="_2">参考文献</h3>
<ul>
<li><a href="http://arxiv.org/pdf/1502.03167v3.pdf">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
</ul>
<p>【Tips】BN层的作用</p>
<p>（1）加速收敛
（2）控制过拟合，可以少用或不用Dropout和正则
（3）降低网络对初始化权重不敏感
（4）允许使用较大的学习率</p>
</div>
    </body></html>