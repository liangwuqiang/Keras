<!DOCTYPE html>
    <html><head><meta charset="UTF-8">
    </head><body>
    <p><a href="http://keras-cn.readthedocs.io/en/latest/">原文链接</a></p>
        <div class="section">
<h1 id="_1">初始化方法</h1>
<p>初始化方法定义了对Keras层设置初始化权重的方法</p>
<p>不同的层可能使用不同的关键字来传递初始化方法，一般来说指定初始化方法的关键字是<code>kernel_initializer</code> 和 <code>bias_initializer</code>，例如：</p>
<pre><code class="python">model.add(Dense(64,
                kernel_initializer='random_uniform',
                bias_initializer='zeros'))
</code></pre>
<p>一个初始化器可以由字符串指定（必须是下面的预定义初始化器之一），或一个callable的函数，例如</p>
<pre><code class="python">from keras import initializers

model.add(Dense(64, kernel_initializer=initializers.random_normal(stddev=0.01)))

# also works; will use the default parameters.
model.add(Dense(64, kernel_initializer='random_normal'))
</code></pre>
<h2 id="initializer">Initializer</h2>
<p>Initializer是所有初始化方法的父类，不能直接使用，如果想要定义自己的初始化方法，请继承此类。</p>
<h2 id="_2">预定义初始化方法</h2>
<h3 id="zeros">Zeros</h3>
<pre><code class="python">keras.initializers.Zeros()
</code></pre>
<p>全零初始化</p>
<h3 id="ones">Ones</h3>
<pre><code class="python">keras.initializers.Ones()
</code></pre>
<p>全1初始化</p>
<h3 id="constant">Constant</h3>
<pre><code class="python">keras.initializers.Constant(value=0)
</code></pre>
<p>初始化为固定值value</p>
<h3 id="randomnormal">RandomNormal</h3>
<pre><code class="python">keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None))
</code></pre>
<p>正态分布初始化</p>
<ul>
<li>mean：均值</li>
<li>stddev：标准差</li>
<li>seed：随机数种子</li>
</ul>
<h3 id="randomuniform">RandomUniform</h3>
<pre><code class="python">keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)
</code></pre>
<p>均匀分布初始化
<em> minval：均匀分布下边界
</em> maxval：均匀分布上边界
* seed：随机数种子</p>
<h3 id="truncatednormal">TruncatedNormal</h3>
<pre><code class="python">keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=None)
</code></pre>
<p>截尾高斯分布初始化，该初始化方法与RandomNormal类似，但位于均值两个标准差以外的数据将会被丢弃并重新生成，形成截尾分布。该分布是神经网络权重和滤波器的推荐初始化方法。</p>
<ul>
<li>mean：均值</li>
<li>stddev：标准差</li>
<li>seed：随机数种子</li>
</ul>
<h3 id="variancescaling">VarianceScaling</h3>
<pre><code class="python">keras.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='normal', seed=None)
</code></pre>
<p>该初始化方法能够自适应目标张量的shape。</p>
<p>当<code>distribution="normal"</code>时，样本从0均值，标准差为sqrt(scale / n)的截尾正态分布中产生。其中：</p>
<pre><code>* 当```mode = "fan_in"```时，权重张量的输入单元数。
* 当```mode = "fan_out"```时，权重张量的输出单元数
* 当```mode = "fan_avg"```时，权重张量的输入输出单元数的均值
</code></pre>
<p>当<code>distribution="uniform"</code>时，权重从[-limit, limit]范围内均匀采样，其中limit = limit = sqrt(3 * scale / n)</p>
<ul>
<li>scale: 放缩因子，正浮点数</li>
<li>mode: 字符串，“fan_in”，“fan_out”或“fan_avg”fan_in", "fan_out", "fan_avg".</li>
<li>distribution: 字符串，“normal”或“uniform”.</li>
<li>seed: 随机数种子</li>
</ul>
<h3 id="orthogonal">Orthogonal</h3>
<pre><code class="python">keras.initializers.Orthogonal(gain=1.0, seed=None)
</code></pre>
<p>用随机正交矩阵初始化</p>
<ul>
<li>gain: 正交矩阵的乘性系数</li>
<li>seed：随机数种子</li>
</ul>
<p>参考文献：<a href="http://arxiv.org/abs/1312.6120">Saxe et al.</a></p>
<h3 id="identiy">Identiy</h3>
<pre><code class="python">keras.initializers.Identity(gain=1.0)
</code></pre>
<p>使用单位矩阵初始化，仅适用于2D方阵</p>
<ul>
<li>gain：单位矩阵的乘性系数</li>
</ul>
<h3 id="lecun_uniform">lecun_uniform</h3>
<pre><code class="python">lecun_uniform(seed=None)
</code></pre>
<p>LeCun均匀分布初始化方法，参数由[-limit, limit]的区间中均匀采样获得，其中limit=sqrt(3 / fan_in), fin_in是权重向量的输入单元数（扇入）</p>
<ul>
<li>seed：随机数种子</li>
</ul>
<p>参考文献：<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">LeCun 98, Efficient Backprop</a></p>
<h3 id="lecun_normal">lecun_normal</h3>
<pre><code class="python">lecun_normal(seed=None)
</code></pre>
<p>LeCun正态分布初始化方法，参数由0均值，标准差为stddev = sqrt(1 / fan_in)的正态分布产生，其中fan_in和fan_out是权重张量的扇入扇出（即输入和输出单元数目）</p>
<ul>
<li>seed：随机数种子</li>
</ul>
<p>参考文献：</p>
<p><a href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a>
<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Efficient Backprop</a></p>
<h3 id="glorot_normal">glorot_normal</h3>
<pre><code class="python">glorot_normal(seed=None)
</code></pre>
<p>Glorot正态分布初始化方法，也称作Xavier正态分布初始化，参数由0均值，标准差为sqrt(2 / (fan_in + fan_out))的正态分布产生，其中fan_in和fan_out是权重张量的扇入扇出（即输入和输出单元数目）</p>
<ul>
<li>seed：随机数种子</li>
</ul>
<p>参考文献：<a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Glorot &amp; Bengio, AISTATS 2010</a></p>
<h3 id="glorot_uniform">glorot_uniform</h3>
<pre><code class="python">glorot_uniform(seed=None)
</code></pre>
<p>Glorot均匀分布初始化方法，又成Xavier均匀初始化，参数从[-limit, limit]的均匀分布产生，其中limit为<code>sqrt(6 / (fan_in + fan_out))</code>。fan_in为权值张量的输入单元数，fan_out是权重张量的输出单元数。</p>
<ul>
<li>seed：随机数种子</li>
</ul>
<p>参考文献：<a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Glorot &amp; Bengio, AISTATS 2010</a></p>
<h3 id="he_normal">he_normal</h3>
<pre><code class="python">he_normal(seed=None)
</code></pre>
<p>He正态分布初始化方法，参数由0均值，标准差为sqrt(2 / fan_in) 的正态分布产生，其中fan_in权重张量的扇入</p>
<ul>
<li>seed：随机数种子</li>
</ul>
<p>参考文献：<a href="http://arxiv.org/abs/1502.01852">He et al</a></p>
<h3 id="he_uniform">he_uniform</h3>
<pre><code class="python">he_normal(seed=None)
</code></pre>
<p>LeCun均匀分布初始化方法，参数由[-limit, limit]的区间中均匀采样获得，其中limit=sqrt(6 / fan_in), fin_in是权重向量的输入单元数（扇入）</p>
<ul>
<li>seed：随机数种子</li>
</ul>
<p>参考文献：<a href="http://arxiv.org/abs/1502.01852">He et al</a></p>
<h2 id="_3">自定义初始化器</h2>
<p>如果需要传递自定义的初始化器，则该初始化器必须是callable的，并且接收<code>shape</code>（将被初始化的张量shape）和<code>dtype</code>（数据类型）两个参数，并返回符合<code>shape</code>和<code>dtype</code>的张量。</p>
<pre><code class="python">from keras import backend as K

def my_init(shape, dtype=None):
    return K.random_normal(shape, dtype=dtype)

model.add(Dense(64, init=my_init))
</code></pre>
</div>
    </body></html>