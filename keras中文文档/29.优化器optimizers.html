<!DOCTYPE html>
    <html><head><meta charset="UTF-8">
    </head><body>
    <p><a href="http://keras-cn.readthedocs.io/en/latest/">原文链接</a></p>
        <div class="section">
<h1 id="optimizers">优化器optimizers</h1>
<p>优化器是编译Keras模型必要的两个参数之一</p>
<pre><code class="python">from keras import optimizers

model = Sequential()
model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))
model.add(Activation('tanh'))
model.add(Activation('softmax'))

sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='mean_squared_error', optimizer=sgd)
</code></pre>
<p>可以在调用<code>model.compile()</code>之前初始化一个优化器对象，然后传入该函数（如上所示），也可以在调用<code>model.compile()</code>时传递一个预定义优化器名。在后者情形下，优化器的参数将使用默认值。</p>
<pre><code class="python"># pass optimizer by name: default parameters will be used
model.compile(loss='mean_squared_error', optimizer='sgd')
</code></pre>
<h2 id="_1">所有优化器都可用的参数</h2>
<p>参数<code>clipnorm</code>和<code>clipvalue</code>是所有优化器都可以使用的参数,用于对梯度进行裁剪.示例如下:</p>
<pre><code class="python">from keras import optimizers

# All parameter gradients will be clipped to
# a maximum norm of 1.
sgd = optimizers.SGD(lr=0.01, clipnorm=1.)
</code></pre>
<pre><code class="python">from keras import optimizers

# All parameter gradients will be clipped to
# a maximum value of 0.5 and
# a minimum value of -0.5.
sgd = optimizers.SGD(lr=0.01, clipvalue=0.5)
</code></pre>
<h2 id="sgd">SGD</h2>
<pre><code class="python">keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)
</code></pre>
<p>随机梯度下降法，支持动量参数，支持学习衰减率，支持Nesterov动量</p>
<h3 id="_2">参数</h3>
<ul>
<li>
<p>lr：大或等于0的浮点数，学习率</p>
</li>
<li>
<p>momentum：大或等于0的浮点数，动量参数</p>
</li>
<li>
<p>decay：大或等于0的浮点数，每次更新后的学习率衰减值</p>
</li>
<li>
<p>nesterov：布尔值，确定是否使用Nesterov动量</p>
</li>
</ul>
<hr/>
<h2 id="rmsprop">RMSprop</h2>
<pre><code class="python">keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)
</code></pre>
<p>除学习率可调整外，建议保持优化器的其他默认参数不变</p>
<p>该优化器通常是面对递归神经网络时的一个良好选择</p>
<h3 id="_3">参数</h3>
<ul>
<li>
<p>lr：大或等于0的浮点数，学习率</p>
</li>
<li>
<p>rho：大或等于0的浮点数</p>
</li>
<li>
<p>epsilon：大或等于0的小浮点数，防止除0错误</p>
</li>
</ul>
<hr/>
<h2 id="adagrad">Adagrad</h2>
<pre><code class="python">keras.optimizers.Adagrad(lr=0.01, epsilon=1e-06)
</code></pre>
<p>建议保持优化器的默认参数不变</p>
<h3 id="adagrad_1">Adagrad</h3>
<ul>
<li>
<p>lr：大或等于0的浮点数，学习率</p>
</li>
<li>
<p>epsilon：大或等于0的小浮点数，防止除0错误</p>
</li>
</ul>
<hr/>
<h2 id="adadelta">Adadelta</h2>
<pre><code class="python">keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-06)
</code></pre>
<p>建议保持优化器的默认参数不变</p>
<h3 id="_4">参数</h3>
<ul>
<li>
<p>lr：大或等于0的浮点数，学习率</p>
</li>
<li>
<p>rho：大或等于0的浮点数</p>
</li>
<li>
<p>epsilon：大或等于0的小浮点数，防止除0错误</p>
</li>
</ul>
<h3 id="_5">参考文献</h3>
<hr/>
<ul>
<li><a href="http://arxiv.org/abs/1212.5701">Adadelta - an adaptive learning rate method</a></li>
</ul>
<h2 id="adam">Adam</h2>
<pre><code class="python">keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)
</code></pre>
<p>该优化器的默认值来源于参考文献</p>
<h3 id="_6">参数</h3>
<ul>
<li>
<p>lr：大或等于0的浮点数，学习率</p>
</li>
<li>
<p>beta_1/beta_2：浮点数， 0&lt;beta&lt;1，通常很接近1</p>
</li>
<li>
<p>epsilon：大或等于0的小浮点数，防止除0错误</p>
</li>
</ul>
<h3 id="_7">参考文献</h3>
<ul>
<li><a href="http://arxiv.org/abs/1412.6980v8">Adam - A Method for Stochastic Optimization</a></li>
</ul>
<hr/>
<h2 id="adamax">Adamax</h2>
<pre><code class="python">keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)
</code></pre>
<p>Adamax优化器来自于Adam的论文的Section7，该方法是基于无穷范数的Adam方法的变体。</p>
<p>默认参数由论文提供</p>
<h3 id="_8">参数</h3>
<ul>
<li>
<p>lr：大或等于0的浮点数，学习率</p>
</li>
<li>
<p>beta_1/beta_2：浮点数， 0&lt;beta&lt;1，通常很接近1</p>
</li>
<li>
<p>epsilon：大或等于0的小浮点数，防止除0错误</p>
</li>
</ul>
<h3 id="_9">参考文献</h3>
<ul>
<li><a href="http://arxiv.org/abs/1412.6980v8">Adam - A Method for Stochastic Optimization</a></li>
</ul>
<hr/>
<h2 id="nadam">Nadam</h2>
<pre><code class="python">keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)
</code></pre>
<p>Nesterov Adam optimizer: Adam本质上像是带有动量项的RMSprop，Nadam就是带有Nesterov 动量的Adam RMSprop</p>
<p>默认参数来自于论文，推荐不要对默认参数进行更改。</p>
<h3 id="_10">参数</h3>
<ul>
<li>
<p>lr：大或等于0的浮点数，学习率</p>
</li>
<li>
<p>beta_1/beta_2：浮点数， 0&lt;beta&lt;1，通常很接近1</p>
</li>
<li>
<p>epsilon：大或等于0的小浮点数，防止除0错误</p>
</li>
</ul>
<h3 id="_11">参考文献</h3>
<ul>
<li>
<p><a href="http://cs229.stanford.edu/proj2015/054_report.pdf">Nadam report</a></p>
</li>
<li>
<p><a href="http://www.cs.toronto.edu/~fritz/absps/momentum.pdf">On the importance of initialization and momentum in deep learning</a></p>
</li>
</ul>
<h2 id="tfoptimizer">TFOptimizer</h2>
<pre><code class="python">keras.optimizers.TFOptimizer(optimizer)
</code></pre>
<p>TF优化器的包装器</p>
</div>
    </body></html>