<!DOCTYPE html>
    <html><head><meta charset="UTF-8">
    </head><body>
    <p><a href="http://keras-cn.readthedocs.io/en/latest/">原文链接</a></p>
        <div class="section">
<h1 id="keras">Keras后端</h1>
<h2 id="_1">什么是“后端”</h2>
<p>Keras是一个模型级的库，提供了快速构建深度学习网络的模块。Keras并不处理如张量乘法、卷积等底层操作。这些操作依赖于某种特定的、优化良好的张量操作库。Keras依赖于处理张量的库就称为“后端引擎”。Keras提供了三种后端引擎Theano/Tensorflow/CNTK，并将其函数统一封装，使得用户可以以同一个接口调用不同后端引擎的函数</p>
<ul>
<li><a href="http://deeplearning.net/software/theano/">Theano</a>是一个开源的符号主义张量操作框架，由蒙特利尔大学LISA/MILA实验室开发。</li>
<li><a href="http://www.tensorflow.org/">TensorFlow</a>是一个符号主义的张量操作框架，由Google开发。</li>
<li><a href="https://www.microsoft.com/en-us/cognitive-toolkit/">CNTK</a>是一个由微软开发的商业级工具包。</li>
</ul>
<p>在未来，我们有可能要添加更多的后端选项。</p>
<h2 id="_2">切换后端</h2>
<p>注意：Windows用户请把<code>$Home</code>改为<code>%USERPROFILE%</code></p>
<p>如果你至少运行过一次Keras，你将在下面的目录下找到Keras的配置文件：</p>
<p><code>$HOME/.keras/keras.json</code></p>
<p>如果该目录下没有该文件，你可以手动创建一个</p>
<p>文件的默认配置如下：</p>
<pre><code>{
    "image_data_format": "channels_last",
    "epsilon": 1e-07,
    "floatx": "float32",
    "backend": "tensorflow"
}
</code></pre>
<p>将<code>backend</code>字段的值改写为你需要使用的后端：<code>theano</code>或<code>tensorflow</code>或者<code>CNTK</code>，即可完成后端的切换</p>
<p>我们也可以通过定义环境变量<code>KERAS_BACKEND</code>来覆盖上面配置文件中定义的后端：</p>
<pre><code class="python">KERAS_BACKEND=tensorflow python -c "from keras import backend;"
Using TensorFlow backend.
</code></pre>
<h2 id="kerasjson">keras.json 细节</h2>
<pre><code class="python">{
    "image_data_format": "channels_last",
    "epsilon": 1e-07,
    "floatx": "float32",
    "backend": "tensorflow"
}
</code></pre>
<p>你可以更改以上<code>~/.keras/keras.json</code>中的配置</p>
<ul>
<li>
<p><code>iamge_data_format</code>：字符串，"channels_last"或"channels_first"，该选项指定了Keras将要使用的维度顺序，可通过<code>keras.backend.image_data_format()</code>来获取当前的维度顺序。对2D数据来说，"channels_last"假定维度顺序为(rows,cols,channels)而"channels_first"假定维度顺序为(channels, rows, cols)。对3D数据而言，"channels_last"假定(conv_dim1, conv_dim2, conv_dim3, channels)，"channels_first"则是(channels, conv_dim1, conv_dim2, conv_dim3)</p>
</li>
<li>
<p><code>epsilon</code>：浮点数，防止除0错误的小数字</p>
</li>
<li><code>floatx</code>：字符串，<code>"float16"</code>, <code>"float32"</code>, <code>"float64"</code>之一，为浮点数精度</li>
<li><code>backend</code>：字符串，所使用的后端，为"tensorflow"或"theano"</li>
</ul>
<h2 id="keras_1">使用抽象的Keras后端来编写代码</h2>
<p>如果你希望你编写的Keras模块能够同时在Theano和TensorFlow两个后端上使用，你可以通过Keras后端接口来编写代码，这里是一个简介：</p>
<pre><code class="python">from keras import backend as K
</code></pre>
<p>下面的代码实例化了一个输入占位符，等价于<code>tf.placeholder()</code> ，<code>T.matrix()</code>，<code>T.tensor3()</code>等</p>
<pre><code class="python">input = K.placeholder(shape=(2, 4, 5))
# also works:
input = K.placeholder(shape=(None, 4, 5))
# also works:
input = K.placeholder(ndim=3)
</code></pre>
<p>下面的代码实例化了一个共享变量（shared），等价于<code>tf.variable()</code>或 <code>theano.shared()</code></p>
<pre><code class="python">val = np.random.random((3, 4, 5))
var = K.variable(value=val)

# all-zeros variable:
var = K.zeros(shape=(3, 4, 5))
# all-ones:
var = K.ones(shape=(3, 4, 5))
</code></pre>
<p>大多数你需要的张量操作都可以通过统一的Keras后端接口完成，而不关心具体执行这些操作的是Theano还是TensorFlow</p>
<pre><code class="python">a = b + c * K.abs(d)
c = K.dot(a, K.transpose(b))
a = K.sum(b, axis=2)
a = K.softmax(b)
a = concatenate([b, c], axis=-1)
# etc...
</code></pre>
<h2 id="kera">Kera后端函数</h2>
<h3 id="backend">backend</h3>
<pre><code class="python">backend()
</code></pre>
<p>返回当前后端</p>
<h3 id="epsilon">epsilon</h3>
<pre><code class="python">epsilon()
</code></pre>
<p>以数值形式返回一个（一般来说很小的）数，用以防止除0错误</p>
<h3 id="set_epsilon">set_epsilon</h3>
<pre><code class="python">set_epsilon(e)
</code></pre>
<p>设置在数值表达式中使用的fuzz factor，用于防止除0错误，该值应该是一个较小的浮点数，示例：</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; K.epsilon()
1e-08
&gt;&gt;&gt; K.set_epsilon(1e-05)
&gt;&gt;&gt; K.epsilon()
1e-05
</code></pre>
<h3 id="floatx">floatx</h3>
<pre><code class="python">floatx()
</code></pre>
<p>返回默认的浮点数数据类型，为字符串，如 'float16', 'float32', 'float64'</p>
<h3 id="set_floatxfloatx">set_floatx(floatx)</h3>
<pre><code class="python">floatx()
</code></pre>
<p>设置默认的浮点数数据类型，为字符串，如 'float16', 'float32', 'float64',示例：</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; K.floatx()
'float32'
&gt;&gt;&gt; K.set_floatx('float16')
&gt;&gt;&gt; K.floatx()
'float16'
</code></pre>
<h3 id="cast_to_floatx">cast_to_floatx</h3>
<pre><code class="python">cast_to_floatx(x)
</code></pre>
<p>将numpy array转换为默认的Keras floatx类型，x为numpy array，返回值也为numpy array但其数据类型变为floatx。示例：</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; K.floatx()
'float32'
&gt;&gt;&gt; arr = numpy.array([1.0, 2.0], dtype='float64')
&gt;&gt;&gt; arr.dtype
dtype('float64')
&gt;&gt;&gt; new_arr = K.cast_to_floatx(arr)
&gt;&gt;&gt; new_arr
array([ 1.,  2.], dtype=float32)
&gt;&gt;&gt; new_arr.dtype
dtype('float32')
</code></pre>
<h3 id="image_data_format">image_data_format</h3>
<pre><code class="python">image_data_format()
</code></pre>
<p>返回默认的图像的维度顺序（‘channels_last’或‘channels_first’）</p>
<h3 id="set_image_data_format">set_image_data_format</h3>
<pre><code class="python">set_image_data_format(data_format)
</code></pre>
<p>设置图像的维度顺序（‘tf’或‘th’）,示例：</p>
<blockquote>
<blockquote>
<blockquote>
<p>from keras import backend as K
K.image_data_format()
'channels_first'
K.set_image_data_format('channels_last')
K.image_data_format()
'channels_last'</p>
</blockquote>
</blockquote>
</blockquote>
<pre><code>
### is_keras_tensor()
 ```python
is_keras_tensor(x)
</code></pre>
<p>判断x是否是keras tensor对象的谓词函数</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; np_var = numpy.array([1, 2])
&gt;&gt;&gt; K.is_keras_tensor(np_var)
False
&gt;&gt;&gt; keras_var = K.variable(np_var)
&gt;&gt;&gt; K.is_keras_tensor(keras_var)  # A variable is not a Tensor.
False
&gt;&gt;&gt; keras_placeholder = K.placeholder(shape=(2, 4, 5))
&gt;&gt;&gt; K.is_keras_tensor(keras_placeholder)  # A placeholder is a Tensor.
True
</code></pre>
<h3 id="get_uid">get_uid</h3>
<pre><code class="python">get_uid(prefix='')
</code></pre>
<p>获得默认计算图的uid，依据给定的前缀提供一个唯一的UID，参数为表示前缀的字符串，返回值为整数.</p>
<h3 id="reset_uids">reset_uids</h3>
<pre><code class="python">reset_uids()
</code></pre>
<p>重置图的标识符</p>
<h3 id="is_keras_tensor">is_keras_tensor</h3>
<pre><code class="python">is_keras_tensor(x)
</code></pre>
<p>判断x是否是一个Keras tensor，返回一个布尔值，示例</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; np_var = numpy.array([1, 2])
&gt;&gt;&gt; K.is_keras_tensor(np_var)
False
&gt;&gt;&gt; keras_var = K.variable(np_var)
&gt;&gt;&gt; K.is_keras_tensor(keras_var)  # A variable is not a Tensor.
False
&gt;&gt;&gt; keras_placeholder = K.placeholder(shape=(2, 4, 5))
&gt;&gt;&gt; K.is_keras_tensor(keras_placeholder)  # A placeholder is a Tensor.
True
</code></pre>
<h3 id="clear_session">clear_session</h3>
<pre><code class="python">clear_session()
</code></pre>
<p>结束当前的TF计算图，并新建一个。有效的避免模型/层的混乱</p>
<h3 id="manual_variable_initialization">manual_variable_initialization</h3>
<pre><code class="python">manual_variable_initialization(value)
</code></pre>
<p>指出变量应该以其默认值被初始化还是由用户手动初始化，参数value为布尔值，默认False代表变量由其默认值初始化</p>
<h3 id="learning_phase">learning_phase</h3>
<pre><code class="python">learning_phase()
</code></pre>
<p>返回训练模式/测试模式的flag，该flag是一个用以传入Keras模型的标记，以决定当前模型执行于训练模式下还是测试模式下</p>
<h3 id="set_learning_phase">set_learning_phase</h3>
<pre><code class="python">set_learning_phase()
</code></pre>
<p>设置训练模式/测试模式0或1</p>
<h3 id="is_sparse">is_sparse</h3>
<pre><code class="python">is_sparse(tensor)
</code></pre>
<p>判断一个tensor是不是一个稀疏的tensor(稀不稀疏由tensor的类型决定，而不是tensor实际上有多稀疏)，返回值是一个布尔值，示例：</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; a = K.placeholder((2, 2), sparse=False)
&gt;&gt;&gt; print(K.is_sparse(a))
False
&gt;&gt;&gt; b = K.placeholder((2, 2), sparse=True)
&gt;&gt;&gt; print(K.is_sparse(b))
True
</code></pre>
<h3 id="to_dense">to_dense</h3>
<pre><code class="python">to_dense(tensor)
</code></pre>
<p>将一个稀疏tensor转换一个不稀疏的tensor并返回之，示例：</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; b = K.placeholder((2, 2), sparse=True)
&gt;&gt;&gt; print(K.is_sparse(b))
True
&gt;&gt;&gt; c = K.to_dense(b)
&gt;&gt;&gt; print(K.is_sparse(c))
False
</code></pre>
<h3 id="variable">variable</h3>
<pre><code class="python">variable(value, dtype='float32', name=None)
</code></pre>
<p>实例化一个张量，返回之</p>
<p>参数：</p>
<ul>
<li>value：用来初始化张量的值</li>
<li>dtype：张量数据类型</li>
<li>name：张量的名字（可选）</li>
</ul>
<p>示例：</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; val = np.array([[1, 2], [3, 4]])
&gt;&gt;&gt; kvar = K.variable(value=val, dtype='float64', name='example_var')
&gt;&gt;&gt; K.dtype(kvar)
'float64'
&gt;&gt;&gt; print(kvar)
example_var
&gt;&gt;&gt; kvar.eval()
array([[ 1.,  2.],
   [ 3.,  4.]])
</code></pre>
<h3 id="placeholder">placeholder</h3>
<pre><code class="python">placeholder(shape=None, ndim=None, dtype='float32', name=None)
</code></pre>
<p>实例化一个占位符，返回之</p>
<p>参数：</p>
<ul>
<li>shape：占位符的shape（整数tuple，可能包含None） </li>
<li>ndim: 占位符张量的阶数，要初始化一个占位符，至少指定<code>shape</code>和<code>ndim</code>之一，如果都指定则使用<code>shape</code></li>
<li>dtype: 占位符数据类型</li>
<li>name: 占位符名称（可选）</li>
</ul>
<p>示例：</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; input_ph = K.placeholder(shape=(2, 4, 5))
&gt;&gt;&gt; input_ph._keras_shape
(2, 4, 5)
&gt;&gt;&gt; input_ph
&lt;tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32&gt;
</code></pre>
<h3 id="shape">shape</h3>
<pre><code class="python">shape(x)
</code></pre>
<p>返回一个张量的符号shape，符号shape的意思是返回值本身也是一个tensor，示例：</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; tf_session = K.get_session()
&gt;&gt;&gt; val = np.array([[1, 2], [3, 4]])
&gt;&gt;&gt; kvar = K.variable(value=val)
&gt;&gt;&gt; input = keras.backend.placeholder(shape=(2, 4, 5))
&gt;&gt;&gt; K.shape(kvar)
&lt;tf.Tensor 'Shape_8:0' shape=(2,) dtype=int32&gt;
&gt;&gt;&gt; K.shape(input)
&lt;tf.Tensor 'Shape_9:0' shape=(3,) dtype=int32&gt;
__To get integer shape (Instead, you can use K.int_shape(x))__

&gt;&gt;&gt; K.shape(kvar).eval(session=tf_session)
array([2, 2], dtype=int32)
&gt;&gt;&gt; K.shape(input).eval(session=tf_session)
array([2, 4, 5], dtype=int32)
</code></pre>
<h3 id="int_shape">int_shape</h3>
<pre><code class="python">int_shape(x)
</code></pre>
<p>以整数Tuple或None的形式返回张量shape，示例：</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; input = K.placeholder(shape=(2, 4, 5))
&gt;&gt;&gt; K.int_shape(input)
(2, 4, 5)
&gt;&gt;&gt; val = np.array([[1, 2], [3, 4]])
&gt;&gt;&gt; kvar = K.variable(value=val)
&gt;&gt;&gt; K.int_shape(kvar)
(2, 2)
</code></pre>
<h3 id="ndim">ndim</h3>
<pre><code class="python">ndim(x)
</code></pre>
<p>返回张量的阶数，为整数，示例：</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; input = K.placeholder(shape=(2, 4, 5))
&gt;&gt;&gt; val = np.array([[1, 2], [3, 4]])
&gt;&gt;&gt; kvar = K.variable(value=val)
&gt;&gt;&gt; K.ndim(input)
3
&gt;&gt;&gt; K.ndim(kvar)
2
</code></pre>
<h3 id="dtype">dtype</h3>
<pre><code class="python">dtype(x)
</code></pre>
<p>返回张量的数据类型，为字符串，示例：</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; K.dtype(K.placeholder(shape=(2,4,5)))
'float32'
&gt;&gt;&gt; K.dtype(K.placeholder(shape=(2,4,5), dtype='float32'))
'float32'
&gt;&gt;&gt; K.dtype(K.placeholder(shape=(2,4,5), dtype='float64'))
'float64'
__Keras variable__

&gt;&gt;&gt; kvar = K.variable(np.array([[1, 2], [3, 4]]))
&gt;&gt;&gt; K.dtype(kvar)
'float32_ref'
&gt;&gt;&gt; kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')
&gt;&gt;&gt; K.dtype(kvar)
'float32_ref'
</code></pre>
<h3 id="eval">eval</h3>
<pre><code class="python">eval(x)
</code></pre>
<p>求得张量的值，返回一个Numpy array，示例：</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')
&gt;&gt;&gt; K.eval(kvar)
array([[ 1.,  2.],
   [ 3.,  4.]], dtype=float32)
</code></pre>
<h3 id="zeros">zeros</h3>
<pre><code class="python">zeros(shape, dtype='float32', name=None)
</code></pre>
<p>生成一个全0张量，示例：</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; kvar = K.zeros((3,4))
&gt;&gt;&gt; K.eval(kvar)
array([[ 0.,  0.,  0.,  0.],
   [ 0.,  0.,  0.,  0.],
   [ 0.,  0.,  0.,  0.]], dtype=float32)
</code></pre>
<h3 id="ones">ones</h3>
<pre><code class="python">ones(shape, dtype='float32', name=None)
</code></pre>
<p>生成一个全1张量，示例</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; kvar = K.ones((3,4))
&gt;&gt;&gt; K.eval(kvar)
array([[ 1.,  1.,  1.,  1.],
   [ 1.,  1.,  1.,  1.],
   [ 1.,  1.,  1.,  1.]], dtype=float32)
</code></pre>
<h3 id="eye">eye</h3>
<pre><code class="python">eye(size, dtype='float32', name=None)
</code></pre>
<p>生成一个单位矩阵，示例：</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; kvar = K.eye(3)
&gt;&gt;&gt; K.eval(kvar)
array([[ 1.,  0.,  0.],
   [ 0.,  1.,  0.],
   [ 0.,  0.,  1.]], dtype=float32)
</code></pre>
<h3 id="zeros_like">zeros_like</h3>
<pre><code class="python">zeros_like(x, name=None)
</code></pre>
<p>生成与另一个张量x的shape相同的全0张量，示例：</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; kvar = K.variable(np.random.random((2,3)))
&gt;&gt;&gt; kvar_zeros = K.zeros_like(kvar)
&gt;&gt;&gt; K.eval(kvar_zeros)
array([[ 0.,  0.,  0.],
   [ 0.,  0.,  0.]], dtype=float32)
</code></pre>
<h3 id="ones_like">ones_like</h3>
<pre><code class="python">ones_like(x, name=None)
</code></pre>
<p>生成与另一个张量shape相同的全1张量，示例：</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; kvar = K.variable(np.random.random((2,3)))
&gt;&gt;&gt; kvar_ones = K.ones_like(kvar)
&gt;&gt;&gt; K.eval(kvar_ones)
array([[ 1.,  1.,  1.],
   [ 1.,  1.,  1.]], dtype=float32)
</code></pre>
<h3 id="random_uniform_variable">random_uniform_variable</h3>
<pre><code class="python">random_uniform_variable(shape, low, high, dtype=None, name=None, seed=None)
</code></pre>
<p>初始化一个Keras变量，其数值为从一个均匀分布中采样的样本，返回之。</p>
<p>参数：</p>
<ul>
<li>shape：张量shape</li>
<li>low：浮点数，均匀分布之下界</li>
<li>high：浮点数，均匀分布之上界</li>
<li>dtype：数据类型</li>
<li>name：张量名</li>
<li>seed：随机数种子</li>
</ul>
<p>示例：</p>
<pre><code class="python">&gt;&gt;&gt; kvar = K.random_uniform_variable((2,3), 0, 1)
&gt;&gt;&gt; kvar
&lt;tensorflow.python.ops.variables.Variable object at 0x10ab40b10&gt;
&gt;&gt;&gt; K.eval(kvar)
array([[ 0.10940075,  0.10047495,  0.476143  ],
   [ 0.66137183,  0.00869417,  0.89220798]], dtype=float32)
</code></pre>
<h3 id="count_params">count_params</h3>
<pre><code class="python">count_params(x)
</code></pre>
<p>返回张量中标量的个数，示例：</p>
<pre><code class="python">&gt;&gt;&gt; kvar = K.zeros((2,3))
&gt;&gt;&gt; K.count_params(kvar)
6
&gt;&gt;&gt; K.eval(kvar)
array([[ 0.,  0.,  0.],
   [ 0.,  0.,  0.]], dtype=float32)
</code></pre>
<h3 id="cast">cast</h3>
<pre><code class="python">cast(x, dtype)
</code></pre>
<p>改变张量的数据类型，dtype只能是<code>float16</code>, <code>float32</code>或<code>float64</code>之一，示例：</p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; input = K.placeholder((2, 3), dtype='float32')
&gt;&gt;&gt; input
&lt;tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32&gt;
__It doesn't work in-place as below.__

&gt;&gt;&gt; K.cast(input, dtype='float16')
&lt;tf.Tensor 'Cast_1:0' shape=(2, 3) dtype=float16&gt;
&gt;&gt;&gt; input
&lt;tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32&gt;
__you need to assign it.__

&gt;&gt;&gt; input = K.cast(input, dtype='float16')
&gt;&gt;&gt; input
&lt;tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16&gt;```
</code></pre>
<h3 id="update">update</h3>
<pre><code class="python">update(x, new_x)
</code></pre>
<p>用new_x更新x</p>
<h3 id="update_add">update_add</h3>
<pre><code class="python">update_add(x, increment)
</code></pre>
<p>通过将x增加increment更新x</p>
<h3 id="update_sub">update_sub</h3>
<pre><code class="python">update_sub(x, decrement)
</code></pre>
<p>通过将x减少decrement更新x</p>
<h3 id="moving_average_update">moving_average_update</h3>
<pre><code class="python">moving_average_update(x, value, momentum)
</code></pre>
<p>含义暂不明确</p>
<h3 id="dot">dot</h3>
<pre><code class="python">dot(x, y)
</code></pre>
<p>求两个张量的乘积。当试图计算两个N阶张量的乘积时，与Theano行为相同，如<code>(2, 3).(4, 3, 5) = (2, 4, 5))</code>，示例：</p>
<pre><code class="python">&gt;&gt;&gt; x = K.placeholder(shape=(2, 3))
&gt;&gt;&gt; y = K.placeholder(shape=(3, 4))
&gt;&gt;&gt; xy = K.dot(x, y)
&gt;&gt;&gt; xy
&lt;tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32&gt;
</code></pre>
<pre><code class="python">&gt;&gt;&gt; x = K.placeholder(shape=(32, 28, 3))
&gt;&gt;&gt; y = K.placeholder(shape=(3, 4))
&gt;&gt;&gt; xy = K.dot(x, y)
&gt;&gt;&gt; xy
&lt;tf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32&gt;
</code></pre>
<p>Theano-like的行为示例：</p>
<pre><code class="python">&gt;&gt;&gt; x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)
&gt;&gt;&gt; y = K.ones((4, 3, 5))
&gt;&gt;&gt; xy = K.dot(x, y)
&gt;&gt;&gt; K.int_shape(xy)
(2, 4, 5)
</code></pre>
<h3 id="batch_dot">batch_dot</h3>
<pre><code class="python">batch_dot(x, y, axes=None)
</code></pre>
<p>按批进行张量乘法，该函数用于计算x和y的点积，其中x和y都是成batch出现的数据。即它的数据shape形如<code>(batch_size,:)</code>。batch_dot将产生比输入张量维度低的张量，如果张量的维度被减至1，则通过<code>expand_dims</code>保证其维度至少为2
例如，假设<code>x = [[1, 2],[3,4]]</code> ， <code>y = [[5, 6],[7, 8]]</code>，则<code>batch_dot(x, y, axes=1) = [[17, 53]]</code>，即<code>x.dot(y.T)</code>的主对角元素，此过程中我们没有计算过反对角元素的值</p>
<p>参数：</p>
<ul>
<li>x,y：阶数大于等于2的张量，在tensorflow下，只支持大于等于3阶的张量</li>
<li>axes：目标结果的维度，为整数或整数列表，<code>axes[0]</code>和<code>axes[1]</code>应相同</li>
</ul>
<p>示例：
假设<code>x=[[1,2],[3,4]]</code>，<code>y=[[5,6],[7,8]]</code>，则<code>batch_dot(x, y, axes=1)</code>为<code>[[17, 53]]</code>，恰好为<code>x.dot(y.T)</code>的主对角元，整个过程没有计算反对角元的元素。</p>
<p>我们做一下shape的推导，假设x是一个shape为(100,20)的tensor，y是一个shape为(100,30,20)的tensor，假设<code>axes=(1,2)</code>，则输出tensor的shape通过循环x.shape和y.shape确定：</p>
<ul>
<li><code>x.shape[0]</code>：值为100，加入到输入shape里</li>
<li><code>x.shape[1]</code>：20，不加入输出shape里，因为该维度的值会被求和(dot_axes[0]=1)</li>
<li><code>y.shape[0]</code>：值为100，不加入到输出shape里，y的第一维总是被忽略</li>
<li><code>y.shape[1]</code>：30，加入到输出shape里</li>
<li>
<p><code>y.shape[2]</code>：20，不加到output shape里，y的第二个维度会被求和(dot_axes[1]=2)</p>
</li>
<li>
<p>结果为(100, 30)</p>
</li>
</ul>
<pre><code class="python">&gt;&gt;&gt; x_batch = K.ones(shape=(32, 20, 1))
&gt;&gt;&gt; y_batch = K.ones(shape=(32, 30, 20))
&gt;&gt;&gt; xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])
&gt;&gt;&gt; K.int_shape(xy_batch_dot)
(32, 1, 30)
</code></pre>
<h3 id="transpose">transpose</h3>
<pre><code class="python">transpose(x)
</code></pre>
<p>张量转置，返回转置后的tensor，示例：</p>
<pre><code class="python">&gt;&gt;&gt; var = K.variable([[1, 2, 3], [4, 5, 6]])
&gt;&gt;&gt; K.eval(var)
array([[ 1.,  2.,  3.],
   [ 4.,  5.,  6.]], dtype=float32)
&gt;&gt;&gt; var_transposed = K.transpose(var)
&gt;&gt;&gt; K.eval(var_transposed)
array([[ 1.,  4.],
   [ 2.,  5.],
   [ 3.,  6.]], dtype=float32)

&gt;&gt;&gt; input = K.placeholder((2, 3))
&gt;&gt;&gt; input
&lt;tf.Tensor 'Placeholder_11:0' shape=(2, 3) dtype=float32&gt;
&gt;&gt;&gt; input_transposed = K.transpose(input)
&gt;&gt;&gt; input_transposed
&lt;tf.Tensor 'transpose_4:0' shape=(3, 2) dtype=float32&gt;

</code></pre>
<h3 id="gather">gather</h3>
<pre><code class="python">gather(reference, indices)
</code></pre>
<p>在给定的张量中检索给定下标的向量</p>
<p>参数：</p>
<ul>
<li>reference：张量</li>
<li>indices：整数张量，其元素为要查询的下标</li>
</ul>
<p>返回值：一个与<code>reference</code>数据类型相同的张量</p>
<h3 id="max">max</h3>
<pre><code class="python">max(x, axis=None, keepdims=False)
</code></pre>
<p>求张量中的最大值</p>
<h3 id="min">min</h3>
<pre><code class="python">min(x, axis=None, keepdims=False)
</code></pre>
<p>求张量中的最小值</p>
<h3 id="sum">sum</h3>
<pre><code class="python">sum(x, axis=None, keepdims=False)
</code></pre>
<p>在给定轴上计算张量中元素之和</p>
<h3 id="prod">prod</h3>
<pre><code class="python">prod(x, axis=None, keepdims=False)
</code></pre>
<p>在给定轴上计算张量中元素之积</p>
<h3 id="cumsum">cumsum</h3>
<pre><code class="python">cumsum(x, axis=0)
</code></pre>
<p>在给定轴上求张量的累积和</p>
<h3 id="cumprod">cumprod</h3>
<pre><code class="python">cumprod(x, axis=0)
</code></pre>
<p>在给定轴上求张量的累积积</p>
<h3 id="var">var</h3>
<pre><code class="python">var(x, axis=None, keepdims=False)
</code></pre>
<p>在给定轴上计算张量方差</p>
<h3 id="std">std</h3>
<pre><code class="python">std(x, axis=None, keepdims=False)
</code></pre>
<p>在给定轴上求张量元素之标准差</p>
<h3 id="mean">mean</h3>
<pre><code class="python">mean(x, axis=None, keepdims=False)
</code></pre>
<p>在给定轴上求张量元素之均值</p>
<h3 id="any">any</h3>
<pre><code class="python">any(x, axis=None, keepdims=False)
</code></pre>
<p>按位或，返回数据类型为uint8的张量（元素为0或1）</p>
<h3 id="all">all</h3>
<pre><code class="python">any(x, axis=None, keepdims=False)
</code></pre>
<p>按位与，返回类型为uint8de tensor</p>
<h3 id="argmax">argmax</h3>
<pre><code class="python">argmax(x, axis=-1)
</code></pre>
<p>在给定轴上求张量之最大元素下标</p>
<h3 id="argmin">argmin</h3>
<pre><code class="python">argmin(x, axis=-1)
</code></pre>
<p>在给定轴上求张量之最小元素下标</p>
<h3 id="square">square</h3>
<pre><code class="python">square(x)
</code></pre>
<p>逐元素平方</p>
<h3 id="abs">abs</h3>
<pre><code class="python">abs(x)
</code></pre>
<p>逐元素绝对值</p>
<h3 id="sqrt">sqrt</h3>
<pre><code class="python">sqrt(x)
</code></pre>
<p>逐元素开方</p>
<h3 id="exp">exp</h3>
<pre><code class="python">exp(x)
</code></pre>
<p>逐元素求自然指数</p>
<h3 id="log">log</h3>
<pre><code class="python">log(x)
</code></pre>
<p>逐元素求自然对数</p>
<h3 id="logsumexp">logsumexp</h3>
<pre><code class="python">logsumexp(x, axis=None, keepdims=False)
</code></pre>
<p>在给定轴上计算log(sum(exp()))，该函数在数值稳定性上超过直接计算log(sum(exp()))，可以避免由exp和log导致的上溢和下溢</p>
<h3 id="round">round</h3>
<pre><code class="python">round(x)
</code></pre>
<p>逐元素四舍五入</p>
<h3 id="sign">sign</h3>
<pre><code class="python">sign(x)
</code></pre>
<p>逐元素求元素的符号（+1或-1）</p>
<h3 id="pow">pow</h3>
<pre><code class="python">pow(x, a)
</code></pre>
<p>逐元素求x的a次方</p>
<h3 id="clip">clip</h3>
<pre><code class="python">clip(x, min_value, max_value)
</code></pre>
<p>逐元素clip（将超出指定范围的数强制变为边界值）</p>
<h3 id="equal">equal</h3>
<pre><code class="python">equal(x, y)
</code></pre>
<p>逐元素判相等关系，返回布尔张量</p>
<h3 id="not_equal">not_equal</h3>
<pre><code class="python">not_equal(x, y)
</code></pre>
<p>逐元素判不等关系，返回布尔张量</p>
<h3 id="greater">greater</h3>
<pre><code class="python">greater(x,y)
</code></pre>
<p>逐元素判断x&gt;y关系，返回布尔张量</p>
<h3 id="greater_equal">greater_equal</h3>
<pre><code class="python">greater_equal(x,y)
</code></pre>
<p>逐元素判断x&gt;=y关系，返回布尔张量</p>
<h3 id="lesser">lesser</h3>
<pre><code class="python">lesser(x,y)
</code></pre>
<p>逐元素判断x&lt;y关系，返回布尔张量</p>
<h3 id="lesser_equal">lesser_equal</h3>
<pre><code class="python">lesser_equal(x,y)
</code></pre>
<p>逐元素判断x&lt;=y关系，返回布尔张量</p>
<h3 id="maximum">maximum</h3>
<pre><code class="python">maximum(x, y)
</code></pre>
<p>逐元素取两个张量的最大值</p>
<h3 id="minimum">minimum</h3>
<pre><code class="python">minimum(x, y)
</code></pre>
<p>逐元素取两个张量的最小值</p>
<h3 id="sin">sin</h3>
<pre><code class="python">sin(x)
</code></pre>
<p>逐元素求正弦值</p>
<h3 id="cos">cos</h3>
<pre><code class="python">cos(x)
</code></pre>
<p>逐元素求余弦值</p>
<h3 id="normalize_batch_in_training">normalize_batch_in_training</h3>
<pre><code class="python">normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.0001)
</code></pre>
<p>对一个batch数据先计算其均值和方差，然后再进行batch_normalization</p>
<h3 id="batch_normalization">batch_normalization</h3>
<pre><code class="python">batch_normalization(x, mean, var, beta, gamma, epsilon=0.0001)
</code></pre>
<p>对一个batch的数据进行batch_normalization，计算公式为：
output = (x-mean)/(sqrt(var)+epsilon)*gamma+beta</p>
<h3 id="concatenate">concatenate</h3>
<pre><code class="python">concatenate(tensors, axis=-1)
</code></pre>
<p>在给定轴上将一个列表中的张量串联为一个张量 specified axis</p>
<h3 id="reshape">reshape</h3>
<pre><code class="python">reshape(x, shape)
</code></pre>
<p>将张量的shape变换为指定shape</p>
<h3 id="permute_dimensions">permute_dimensions</h3>
<pre><code class="python">permute_dimensions(x, pattern)
</code></pre>
<p>按照给定的模式重排一个张量的轴</p>
<p>参数：</p>
<ul>
<li>pattern：代表维度下标的tuple如<code>(0, 2, 1)</code></li>
</ul>
<h3 id="resize_images">resize_images</h3>
<pre><code class="python">resize_images(X, height_factor, width_factor, dim_ordering)
</code></pre>
<p>依据给定的缩放因子，改变一个batch图片的shape，参数中的两个因子都为正整数，图片的排列顺序与维度的模式相关，如‘th’和‘tf’</p>
<h3 id="resize_volumes">resize_volumes</h3>
<pre><code class="python">resize_volumes(X, depth_factor, height_factor, width_factor, dim_ordering)
</code></pre>
<p>依据给定的缩放因子，改变一个5D张量数据的shape，参数中的两个因子都为正整数，图片的排列顺序与维度的模式相关，如‘th’和‘tf’。5D数据的形式是<a href="../th">batch, channels, depth, height, width</a>或<a href="../tf">batch, depth, height, width, channels</a></p>
<h3 id="repeat_elements">repeat_elements</h3>
<pre><code class="python">repeat_elements(x, rep, axis)
</code></pre>
<p>在给定轴上重复张量元素<code>rep</code>次，与<code>np.repeat</code>类似。例如，若xshape<code>(s1, s2, s3)</code>并且给定轴为<code>axis=1`，输出张量的shape为`(s1, s2 * rep, s3)</code></p>
<h3 id="repeat">repeat</h3>
<pre><code class="python">repeat(x, n)
</code></pre>
<p>重复2D张量，例如若xshape是<code>(samples, dim)</code>且n为2，则输出张量的shape是<code>(samples, 2, dim)</code></p>
<h3 id="arange">arange</h3>
<pre><code class="python">arange(start, stop=None, step=1, dtype='int32')
</code></pre>
<p>生成1D的整数序列张量，该函数的参数与Theano的arange函数含义相同，如果只有一个参数被提供了，那么它实际上就是<code>stop</code>参数的值</p>
<p>为了与tensorflow的默认保持匹配，函数返回张量的默认数据类型是<code>int32</code></p>
<h3 id="tile">tile</h3>
<pre><code class="python">tile(x, n)
</code></pre>
<p>将x在各个维度上重复n次，x为张量，n为与x维度数目相同的列表</p>
<h3 id="batch_flatten">batch_flatten</h3>
<pre><code class="python">batch_flatten(x)
</code></pre>
<p>将一个n阶张量转变为2阶张量，其第一维度保留不变</p>
<h3 id="expand_dims">expand_dims</h3>
<pre><code class="python">expand_dims(x, dim=-1)
</code></pre>
<p>在下标为<code>dim</code>的轴上增加一维</p>
<h3 id="squeeze">squeeze</h3>
<pre><code class="python">squeeze(x, axis)
</code></pre>
<p>将下标为<code>axis</code>的一维从张量中移除</p>
<h3 id="temporal_padding">temporal_padding</h3>
<pre><code class="python">temporal_padding(x, padding=1)
</code></pre>
<p>向3D张量中间的那个维度的左右两端填充<code>padding</code>个0值</p>
<h3 id="asymmetric_temporal_padding">asymmetric_temporal_padding</h3>
<pre><code class="python">asymmetric_temporal_padding(x, left_pad=1, right_pad=1)
</code></pre>
<p>向3D张量中间的那个维度的一端填充<code>padding</code>个0值</p>
<h3 id="spatial_2d_padding">spatial_2d_padding</h3>
<pre><code class="python">spatial_2d_padding(x, padding=(1, 1), dim_ordering='th')
</code></pre>
<p>向4D张量第二和第三维度的左右两端填充<code>padding[0]</code>和<code>padding[1]</code>个0值</p>
<h3 id="spatial_3d_padding">spatial_3d_padding</h3>
<pre><code class="python">spatial_3d_padding(x, padding=(1, 1, 1), dim_ordering='th')
</code></pre>
<p>向5D张量深度、高度和宽度三个维度上填充<code>padding[0]</code>，<code>padding[1]</code>和<code>padding[2]</code>个0值</p>
<h3 id="stack">stack</h3>
<pre><code class="python">stack(x, axis=0)
</code></pre>
<p>将一个列表中维度数目为R的张量堆积起来形成维度为R+1的新张量</p>
<h3 id="one-hot">one-hot</h3>
<pre><code class="python">one_hot(indices, nb_classes)
</code></pre>
<p>输入为n维的整数张量，形如(batch_size, dim1, dim2, ... dim(n-1))，输出为(n+1)维的one-hot编码，形如(batch_size, dim1, dim2, ... dim(n-1), nb_classes)</p>
<h3 id="reverse">reverse</h3>
<pre><code class="python">reverse(x, axes)
</code></pre>
<p>将一个张量在给定轴上反转</p>
<h3 id="get_value">get_value</h3>
<pre><code class="python">get_value(x)
</code></pre>
<p>以Numpy array的形式返回张量的值</p>
<h3 id="batch_get_value">batch_get_value</h3>
<pre><code class="python">batch_get_value(x)
</code></pre>
<p>以Numpy array list的形式返回多个张量的值</p>
<h3 id="set_value">set_value</h3>
<pre><code class="python">set_value(x, value)
</code></pre>
<p>从numpy array将值载入张量中</p>
<h3 id="batch_set_value">batch_set_value</h3>
<pre><code class="python">batch_set_value(tuples)
</code></pre>
<p>将多个值载入多个张量变量中</p>
<p>参数：</p>
<ul>
<li>tuples: 列表，其中的元素形如<code>(tensor, value)</code>。<code>value</code>是要载入的Numpy array数据</li>
</ul>
<h3 id="print_tensor">print_tensor</h3>
<pre><code>print_tensor(x, message='')
</code></pre>
<p>在求值时打印张量的信息，并返回原张量</p>
<h3 id="function">function</h3>
<pre><code class="python">function(inputs, outputs, updates=[])
</code></pre>
<p>实例化一个Keras函数</p>
<p>参数：</p>
<ul>
<li>inputs:：列表，其元素为占位符或张量变量</li>
<li>outputs：输出张量的列表</li>
<li>updates：列表，其元素是形如<code>(old_tensor, new_tensor)</code>的tuple.</li>
</ul>
<h3 id="gradients">gradients</h3>
<pre><code class="python">gradients(loss, variables)
</code></pre>
<p>返回loss函数关于variables的梯度，variables为张量变量的列表</p>
<h3 id="stop_gradient">stop_gradient</h3>
<pre><code class="python">stop_gradient(variables)
</code></pre>
<p>Returns <code>variables</code> but with zero gradient with respect to every other variables.</p>
<h3 id="rnn">rnn</h3>
<pre><code class="python">rnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None)
</code></pre>
<p>在张量的时间维上迭代</p>
<p>参数：</p>
<ul>
<li>inputs： 形如<code>(samples, time, ...)</code>的时域信号的张量，阶数至少为3</li>
<li>step_function：每个时间步要执行的函数
  其参数：  </li>
<li>input：形如<code>(samples, ...)</code>的张量，不含时间维，代表某个时间步时一个batch的样本    </li>
<li>states：张量列表
    其返回值：</li>
<li>output：形如<code>(samples, ...)</code>的张量<ul>
<li>new_states：张量列表，与‘states’的长度相同    </li>
</ul>
</li>
<li>initial_states：形如<code>(samples, ...)</code>的张量，包含了<code>step_function</code>状态的初始值。</li>
<li>go_backwards：布尔值，若设为True，则逆向迭代序列</li>
<li>mask：形如<code>(samples, time, 1)</code>的二值张量，需要屏蔽的数据元素上值为1</li>
<li>constants：按时间步传递给函数的常数列表</li>
<li>unroll：当使用TensorFlow时，RNN总是展开的。当使用Theano时，设置该值为<code>True</code>将展开递归网络</li>
<li>input_length：使用TensorFlow时不需要此值，在使用Theano时，如果要展开递归网络，必须指定输入序列</li>
</ul>
<p>返回值：形如<code>(last_output, outputs, new_states)</code>的tuple</p>
<ul>
<li>last_output：rnn最后的输出，形如<code>(samples, ...)</code></li>
<li>outputs：形如<code>(samples, time, ...)</code>的张量，每个在[s,t]点的输出对应于样本s在t时间的输出</li>
<li>new_states: 列表，其元素为形如<code>(samples, ...)</code>的张量，代表每个样本的最后一个状态</li>
</ul>
<h3 id="switch">switch</h3>
<pre><code class="python">switch(condition, then_expression, else_expression)
</code></pre>
<p>依据给定的条件‘condition’（整数或布尔值）在两个表达式之间切换，注意两个表达式都应该是具有同样shape的符号化张量表达式</p>
<p>参数：</p>
<ul>
<li>condition：标量张量</li>
<li>then_expression：TensorFlow表达式</li>
<li>else_expression: TensorFlow表达式</li>
</ul>
<h3 id="in_train_phase">in_train_phase</h3>
<pre><code class="python">in_train_phase(x, alt)
</code></pre>
<p>如果处于训练模式，则选择x，否则选择alt，注意alt应该与x的shape相同</p>
<h3 id="in_test_phase">in_test_phase</h3>
<pre><code class="python">in_test_phase(x, alt)
</code></pre>
<p>如果处于测试模式，则选择x，否则选择alt，注意alt应该与x的shape相同</p>
<h3 id="relu">relu</h3>
<pre><code class="python">relu(x, alpha=0.0, max_value=None)
</code></pre>
<p>修正线性单元</p>
<p>参数：</p>
<ul>
<li>alpha：负半区斜率</li>
<li>max_value: 饱和门限</li>
</ul>
<h3 id="elu">elu</h3>
<pre><code class="python">elu(x, alpha=1.0)
</code></pre>
<p>指数线性单元</p>
<p>参数：</p>
<ul>
<li>x：输入张量</li>
<li>alpha: 标量</li>
</ul>
<h3 id="softmax">softmax</h3>
<pre><code class="python">softmax(x)
</code></pre>
<p>返回张量的softmax值</p>
<h3 id="softplus">softplus</h3>
<pre><code class="python">softplus(x)
</code></pre>
<p>返回张量的softplus值</p>
<h3 id="softsign">softsign</h3>
<pre><code class="python">softsign(x)
</code></pre>
<p>返回张量的softsign值</p>
<h3 id="categorical_crossentropy">categorical_crossentropy</h3>
<pre><code class="python">categorical_crossentropy(output, target, from_logits=False)
</code></pre>
<p>计算输出张量和目标张量的Categorical crossentropy（类别交叉熵），目标张量与输出张量必须shape相同</p>
<h3 id="sparse_categorical_crossentropy">sparse_categorical_crossentropy</h3>
<pre><code class="python">sparse_categorical_crossentropy(output, target, from_logits=False)
</code></pre>
<p>计算输出张量和目标张量的Categorical crossentropy（类别交叉熵），目标张量必须是整型张量</p>
<h3 id="binary_crossentropy">binary_crossentropy</h3>
<pre><code class="python">binary_crossentropy(output, target, from_logits=False)
</code></pre>
<p>计算输出张量和目标张量的交叉熵</p>
<h3 id="sigmoid">sigmoid</h3>
<pre><code class="python">sigmoid(x)
</code></pre>
<p>逐元素计算sigmoid值</p>
<h3 id="hard_sigmoid">hard_sigmoid</h3>
<pre><code class="python">hard_sigmoid(x)
</code></pre>
<p>该函数是分段线性近似的sigmoid，计算速度更快</p>
<h3 id="tanh">tanh</h3>
<pre><code class="python">tanh(x)
</code></pre>
<p>逐元素计算sigmoid值</p>
<h3 id="dropout">dropout</h3>
<pre><code class="python">dropout(x, level, seed=None)
</code></pre>
<p>随机将x中一定比例的值设置为0，并放缩整个tensor</p>
<p>参数：</p>
<ul>
<li>x：张量</li>
<li>level：x中设置成0的元素比例</li>
<li>seed：随机数种子</li>
</ul>
<h3 id="l2_normalize">l2_normalize</h3>
<pre><code class="python">l2_normalize(x, axis)
</code></pre>
<p>在给定轴上对张量进行L2范数规范化</p>
<h3 id="in_top_k">in_top_k</h3>
<pre><code class="python">in_top_k(predictions, targets, k)
</code></pre>
<p>判断目标是否在predictions的前k大值位置</p>
<p>参数：</p>
<ul>
<li>predictions：预测值张量, shape为(batch_size, classes), 数据类型float32</li>
<li>targets：真值张量, shape为(batch_size,),数据类型为int32或int64</li>
<li>k：整数</li>
</ul>
<h3 id="conv1d">conv1d</h3>
<pre><code class="python">conv1d(x, kernel, strides=1, border_mode='valid', image_shape=None, filter_shape=None)
</code></pre>
<p>1D卷积</p>
<p>参数：</p>
<ul>
<li>kernel：卷积核张量</li>
<li>strides：步长，整型</li>
<li>border_mode：“same”，“valid”之一的字符串</li>
</ul>
<h3 id="conv2d">conv2d</h3>
<pre><code class="python">conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th', image_shape=None, filter_shape=None)
</code></pre>
<p>2D卷积</p>
<p>参数：</p>
<ul>
<li>kernel：卷积核张量</li>
<li>strides：步长，长为2的tuple</li>
<li>border_mode：“same”，“valid”之一的字符串</li>
<li>dim_ordering：“tf”和“th”之一，维度排列顺序</li>
</ul>
<h3 id="deconv2d">deconv2d</h3>
<pre><code class="python">deconv2d(x, kernel, output_shape, strides=(1, 1), border_mode='valid', dim_ordering='th', image_shape=None, filter_shape=None)
</code></pre>
<p>2D反卷积（转置卷积）</p>
<p>参数：</p>
<ul>
<li>x：输入张量</li>
<li>kernel：卷积核张量</li>
<li>output_shape: 输出shape的1D的整数张量</li>
<li>strides：步长，tuple类型</li>
<li>border_mode：“same”或“valid”</li>
<li>dim_ordering：“tf”或“th”</li>
</ul>
<h3 id="conv3d">conv3d</h3>
<pre><code class="python">conv3d(x, kernel, strides=(1, 1, 1), border_mode='valid', dim_ordering='th', volume_shape=None, filter_shape=None)
</code></pre>
<p>3D卷积</p>
<p>参数：</p>
<ul>
<li>x：输入张量</li>
<li>kernel：卷积核张量</li>
<li>strides：步长，tuple类型</li>
<li>border_mode：“same”或“valid”</li>
<li>dim_ordering：“tf”或“th”</li>
</ul>
<h3 id="pool2d">pool2d</h3>
<pre><code class="python">pool2d(x, pool_size, strides=(1, 1), border_mode='valid', dim_ordering='th', pool_mode='max')
</code></pre>
<p>2D池化</p>
<p>参数：</p>
<ul>
<li>pool_size：含有两个整数的tuple，池的大小</li>
<li>strides：含有两个整数的tuple，步长</li>
<li>border_mode：“same”，“valid”之一的字符串</li>
<li>dim_ordering：“tf”和“th”之一，维度排列顺序</li>
<li>pool_mode: “max”，“avg”之一，池化方式</li>
</ul>
<h3 id="pool3d">pool3d</h3>
<pre><code class="python">pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid', dim_ordering='th', pool_mode='max')
</code></pre>
<p>3D池化</p>
<p>参数：</p>
<ul>
<li>pool_size：含有3个整数的tuple，池的大小</li>
<li>strides：含有3个整数的tuple，步长</li>
<li>border_mode：“same”，“valid”之一的字符串</li>
<li>dim_ordering：“tf”和“th”之一，维度排列顺序</li>
<li>pool_mode: “max”，“avg”之一，池化方式</li>
</ul>
<h3 id="bias_add">bias_add</h3>
<pre><code class="python">bias_add(x, bias, data_format=None)
</code></pre>
<p>为张量增加一个偏置项</p>
<h3 id="random_normal">random_normal</h3>
<pre><code class="python">random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None)
</code></pre>
<p>返回具有正态分布值的张量，mean和stddev为均值和标准差</p>
<h3 id="random_uniform">random_uniform</h3>
<pre><code class="python">random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None)
</code></pre>
<p>返回具有均匀分布值的张量，minval和maxval是均匀分布的下上界</p>
<h3 id="random_binomial">random_binomial</h3>
<pre><code class="python">random_binomial(shape, p=0.0, dtype=None, seed=None)
</code></pre>
<p>返回具有二项分布值的张量，p是二项分布参数</p>
<h3 id="truncated_normall">truncated_normall</h3>
<pre><code class="python">truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None)
</code></pre>
<p>返回具有截尾正态分布值的张量，在距离均值两个标准差之外的数据将会被截断并重新生成</p>
<h3 id="ctc_label_dense_to_sparse">ctc_label_dense_to_sparse</h3>
<pre><code class="python">ctc_label_dense_to_sparse(labels, label_lengths)
</code></pre>
<p>将ctc标签从稠密形式转换为稀疏形式</p>
<h3 id="ctc_batch_cost">ctc_batch_cost</h3>
<pre><code class="python">ctc_batch_cost(y_true, y_pred, input_length, label_length)
</code></pre>
<p>在batch上运行CTC损失算法</p>
<p>参数：</p>
<ul>
<li>y_true：形如(samples，max_tring_length)的张量，包含标签的真值</li>
<li>y_pred：形如(samples，time_steps，num_categories)的张量，包含预测值或输出的softmax值</li>
<li>input_length：形如(samples，1)的张量，包含y_pred中每个batch的序列长</li>
<li>label_length：形如(samples，1)的张量，包含y_true中每个batch的序列长</li>
</ul>
<p>返回值：形如(samoles，1)的tensor，包含了每个元素的CTC损失</p>
<h3 id="ctc_decode">ctc_decode</h3>
<pre><code class="python">ctc_decode(y_pred, input_length, greedy=True, beam_width=None, dict_seq_lens=None, dict_values=None)
</code></pre>
<p>使用贪婪算法或带约束的字典搜索算法解码softmax的输出</p>
<p>参数：</p>
<ul>
<li>y_pred：形如(samples，time_steps，num_categories)的张量，包含预测值或输出的softmax值</li>
<li>input_length：形如(samples，1)的张量，包含y_pred中每个batch的序列长</li>
<li>greedy：设置为True使用贪婪算法，速度快</li>
<li>dict_seq_lens：dic_values列表中各元素的长度</li>
<li>dict_values：列表的列表，代表字典</li>
</ul>
<p>返回值：形如(samples，time_steps，num_catgories)的张量，包含了路径可能性（以softmax概率的形式）。注意仍然需要一个用来取出argmax和处理空白标签的函数</p>
<h3 id="map_fn">map_fn</h3>
<pre><code class="python">map_fn(fn, elems, name=None)
</code></pre>
<p>元素elems在函数fn上的映射，并返回结果</p>
<p>参数：</p>
<ul>
<li>fn：函数</li>
<li>elems：张量</li>
<li>name：节点的名字</li>
</ul>
<p>返回值：返回一个张量，该张量的第一维度等于elems，第二维度取决于fn</p>
<h3 id="foldl">foldl</h3>
<pre><code class="python">foldl(fn, elems, initializer=None, name=None)
</code></pre>
<p>减少elems，用fn从左到右连接它们</p>
<p>参数：</p>
<ul>
<li>fn：函数，例如：lambda acc, x: acc + x</li>
<li>elems：张量</li>
<li>initializer：初始化的值(elems[0])</li>
<li>name：节点名</li>
</ul>
<p>返回值：与initializer的类型和形状一致</p>
<h3 id="foldr">foldr</h3>
<pre><code class="python">foldr(fn, elems, initializer=None, name=None)
</code></pre>
<p>减少elems，用fn从右到左连接它们</p>
<p>参数：</p>
<ul>
<li>fn：函数，例如：lambda acc, x: acc + x</li>
<li>elems：张量</li>
<li>initializer：初始化的值（elems[-1]）</li>
<li>name：节点名</li>
</ul>
<p>返回值：与initializer的类型和形状一致</p>
</div>
    </body></html>