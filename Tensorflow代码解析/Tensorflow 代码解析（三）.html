<!DOCTYPE html>
<html><head><meta charset="UTF-8">
</head><body>
<p><a href="http://www.dataguru.cn/article-10827-1.html">原文链接</a></p>
<p><center><h1>Tensorflow 代码解析（三）</h1></center></p>
    <td id="article_content">
<p style="margin-bottom:10px">
<span class="newtag"><img src="/static/image/common/tag1.png"/><a href="http://www.dataguru.cn/tags/tm" target="_blank">tm</a></span>
<span class="newtag"><img src="/static/image/common/tag1.png"/><a href="http://www.dataguru.cn/tags/Python" target="_blank">Python</a></span>
<span class="newtag"><img src="/static/image/common/tag1.png"/><a href="http://www.dataguru.cn/tags/测试" target="_blank">测试</a></span>
<span class="newtag"><img src="/static/image/common/tag1.png"/><a href="http://www.dataguru.cn/tags/C++" target="_blank">C++</a></span>
<span class="newtag"><img src="/static/image/common/tag1.png"/><a href="http://www.dataguru.cn/tags/GPU" target="_blank">GPU</a></span>
<span class="newtag"><img src="/static/image/common/tag1.png"/><a href="http://www.dataguru.cn/tags/函数" target="_blank">函数</a></span>
</p>
<div>4.  TF – Kernels模块</div><div>TF中包含大量Op算子，这些算子组成Graph的节点集合。这些算子对Tensor实现相应的运算操作。图 4 1列出了TF中的Op算子的分类和举例。</div><div><p><a href="images/bd763a7bdf0e5947757f47ebaf7712e8.png" target="_blank"><img src="images/bd763a7bdf0e5947757f47ebaf7712e8.png"/></a></p></div><div>图 4 1 TensorFlow核心库中的部分运算</div><div><br/></div><div>4.1   OpKernels 简介</div><div>OpKernel类（core/framework/op_kernel.h）是所有Op类的基类。继承OpKernel还可以自定义新的Op类。用的较多的Op如（MatMul,  Conv2D,  SoftMax,  AvgPooling, Argmax等）。</div><div><br/></div><div>所有Op包含注册（Register Op）和实现（正向计算、梯度定义）两部分。</div><div><br/></div><div>所有Op类的实现需要overide抽象基函数 void Compute(OpKernelContext* context)，实现自身Op功能。用户可以根据需要自定义新的Op操作，参考[12]。</div><div><br/></div><div>TF中所有Op操作的属性定义和描述都在 ops/ops.pbtxt。如下Add操作，定义了输入参数x、y，输出参数z。</div><div><p><a href="images/c4c02778acf9ed47028f7a6c66f0c262.png" target="_blank"><img src="images/c4c02778acf9ed47028f7a6c66f0c262.png"/></a></p></div><div>4.2 UnaryOp &amp; BinaryOp</div><div>UnaryOp和BinaryOp定义了简单的一元操作和二元操作，类定义在/core/kernels/ cwise_ops.h文件，类实现在/core/kernels/cwise_op_*.cc类型的文件中，如cwise_op_sin.cc文件。</div><div><br/></div><div>一元操作全称为Coefficient-wise unary operations，一元运算有abs， sqrt， exp， sin， cos，conj（共轭）等。如abs的基本定义：</div><div><p><a href="images/9cede034fc0694941d15caa53e34aa78.png" target="_blank"><img src="images/9cede034fc0694941d15caa53e34aa78.png"/></a></p></div><div>二元操作全称为Coefficient-wise binary operations，二元运算有add，sub， div， mul，mod等。如sum的基本定义：</div><div><p><a href="images/f38c3ab2a5da9ef772d4b0f27f41bae8.png" target="_blank"><img src="images/f38c3ab2a5da9ef772d4b0f27f41bae8.png"/></a></p></div><div>4.3 MatMul</div><div><br/></div><div>4.3.1 Python相关部分</div><div>在Python脚本中定义matmul运算：</div><div><p><a href="images/7e6508f3ea13d39a781d3bcc95737ed0.png" target="_blank"><img src="images/7e6508f3ea13d39a781d3bcc95737ed0.png"/></a></p></div><div>根据Ops名称MatMul从Ops库中找出对应Ops类型</div><div><br/></div><div>创建ops节点</div><div><p><a href="images/9a224b6c948919a64ec293c32de87baa.png" target="_blank"><img src="images/9a224b6c948919a64ec293c32de87baa.png"/></a></p></div><div>创建ops节点并指定相关属性和设备分配</div><div><p><a href="images/7e3a80ae5b8deeffbf83a09e0e3f8010.png" target="_blank"><img src="images/7e3a80ae5b8deeffbf83a09e0e3f8010.png"/></a></p></div><div><br/></div><div>4.3.2 C++相关部分</div><div>Python脚本通过swig调用进入C接口API文件core/client/tensor_c_api.cc，调用TF_NewNode函数生成节点，同时还需要指定输入变量，TF_AddInput函数设置first输入变量，TF_AddInputList函数设置other输入变量。这里op_type为MatMul，first输入变量为a，other输入变量为b。</div><div><p><a href="images/989825d8c124b6272bc71fa154e642e8.png" target="_blank"><img src="images/989825d8c124b6272bc71fa154e642e8.png"/></a></p></div><div>创建节点根据节点类型从注册的Ops工厂中生成，即TF通过工厂模式把一系列Ops注册到Ops工厂中。其中MatMul的注册函数为如下</div><div><p><a href="images/52b9f40db435ea3e77172db6748d011f.png" target="_blank"><img src="images/52b9f40db435ea3e77172db6748d011f.png"/></a></p></div><div>4.3.3 MatMul正向计算</div><div>MatMul的实现部分在core/kernels/matmul_op.cc文件中，类MatMulOp继承于OpKernel，成员函数Compute完成计算操作。</div><div><p><a href="images/742235f1ea454de024c6cd9ab316f68f.png" target="_blank"><img src="images/742235f1ea454de024c6cd9ab316f68f.png"/></a></p></div><div><br/></div><div>MatMul的测试用例core/kernels/matmul_op_test.cc文件，要调试这个测试用例，可通过如下方式：</div><div><p><a href="images/1134d70ec23d8daab09ac64c88536e3e.png" target="_blank"><img src="images/1134d70ec23d8daab09ac64c88536e3e.png"/></a></p></div><div>在TF中MatMul实现了CPU和GPU两个版本，其中CPU版本使用Eigen库，GPU版本使用cuBLAS库。</div><div><br/></div><div>CPU版的MatMul使用Eigen库，调用方式如下：</div><div><p><a href="images/70ccb732c44838cdb6c0ec31813ebce6.png" target="_blank"><img src="images/70ccb732c44838cdb6c0ec31813ebce6.png"/></a></p></div><div><br/></div><div>简而言之就是调用eigen的constract函数。</div><div><br/></div><div>GPU版的MatMul使用cuBLAS库，准确而言是基于cuBLAS的stream_executor库。Stream executor是google开发的开源并行计算库，调用方式如下：</div><div><p><a href="images/59f25f142c7043552f845e21f37e0f80.png" target="_blank"><img src="images/59f25f142c7043552f845e21f37e0f80.png"/></a></p></div><div>其中stream类似于设备句柄，可以调用stream executor中的cuda模块完成运算。</div><div><br/></div><div>4.3.4 MatMul梯度计算</div><div>MatMul的梯度计算本质上也是一种kernel ops，描述为MatMulGrad。MatMulgrad操作是定义在grad_ops工厂中，类似于ops工厂。定义方式如下：</div><div><p><a href="images/4d77954f5ea7625038be681e36871b69.png" target="_blank"><img src="images/4d77954f5ea7625038be681e36871b69.png"/></a></p></div><div>MatmulGrad由FDH（Function Define Helper）完成定义，</div><div><p><a href="images/426a88f7b8b5730774c1241bbf10d034.png" target="_blank"><img src="images/426a88f7b8b5730774c1241bbf10d034.png"/></a></p></div><div>其中attr_adj_x="transpose_a" ax0=false, ax1=true, attr_adj_y= "transpose_b", ay0=true, ay1=false, *g属于FunctionDef类，包含MatMul的梯度定义。</div><div><br/></div><div>从FDH定义中可以看出MatMulGrad本质上还是MatMul操作。在矩阵求导运算中：</div><div><p><a href="images/6c1acab2fa5c02b7835402fa2bec6ce3.png" target="_blank"><img src="images/6c1acab2fa5c02b7835402fa2bec6ce3.png"/></a></p></div><div>MatMulGrad的测试用例core/ops/math_grad_test.cc文件，要调试这个测试用例，可通过如下方式：</div><div><p><a href="images/7945ab6d3002e3e2d12c446ab0ca1282.png" target="_blank"><img src="images/7945ab6d3002e3e2d12c446ab0ca1282.png"/></a></p></div><div>4.4 Conv2d</div><div>关于conv2d的python调用部分和C++创建部分可参考MatMul中的描述。</div><div><br/></div><div>4.4.1 Conv2d正向计算部分</div><div>TF中conv2d接口如下所示，简单易用：</div><div><br/></div><div>实现部分在core/kernels/conv_ops.cc文件中，类Conv2DOp继承于抽象基类OpKernel。</div><div><br/></div><div>Conv2DOp的测试用例core/kernels/eigen_spatial_convolutions_test.cc文件，要调试这个测试用例，可通过如下方式：</div><div><p><a href="images/856bf26454d00166e7ef38f0c9d1835b.png" target="_blank"><img src="images/856bf26454d00166e7ef38f0c9d1835b.png"/></a></p></div><div>Conv2DOp的成员函数Compute完成计算操作。</div><div><br/></div><div>为方便描述，假设tf.nn.conv2d中input参数的shape为[batch, in_rows, in_cols, in_depth]，filter参数的shape为[filter_rows, filter_cols, in_depth, out_depth]。</div><div><br/></div><div>首先，计算卷积运算后输出tensor的shape。</div><div><br/></div><div>Ø  若padding=VALID，output_size = (input_size - filter_size + stride) / stride;</div><div><br/></div><div>Ø  若padding=SAME，output_size = (input_size + stride - 1) / stride;</div><div><br/></div><div>其次，根据计算结果给输出tensor分配内存。</div><div><br/></div><div>然后，开始卷积计算。Conv2DOp实现了CPU和GPU两种模式下的卷积运算。同时，还需要注意input tensor的输入格式，通常有NHWC和NCHW两种格式。在TF中，Conv2d-CPU模式下目前仅支持NHWC格式，即[Number, Height, Weight, Channel]格式。Conv2d-GPU模式下以NCHW为主，但支持将NHWC转换为NCHW求解。C++中多维数组是row-major顺序存储的，而Eigen默认是col-major顺序的，则C++中[N, H, W, C]相当于Eigen中的[C, W, H, N]，即dimention order是相反的，需要特别注意。</div><div><br/></div><div>Conv2d-CPU模式下调用Eigen库函数。</div><div><p><a href="images/7a7261b64aa93e4bf6eda6c18fcd281d.png" target="_blank"><img src="images/7a7261b64aa93e4bf6eda6c18fcd281d.png"/></a></p></div><div>Eigen库中卷积函数的详细代码参见图 4 2。</div><div><p><a href="images/d3cf534362ca60962be5732180d04953.png" target="_blank"><img src="images/d3cf534362ca60962be5732180d04953.png"/></a></p></div><div>图 4 2 Eigen卷积运算的定义</div><div><br/></div><div>Ø  Tensor:: extract_image_patches () 为卷积或池化操作抽取与kernel size一致的image patches。该函数的定义在eigen3/unsupported/Eigen/CXX11/src/Tensor/ TensorBase.h中，参考该目录下ReadME.md。</div><div><br/></div><div>Ø  Tensor:: extract_image_patches () 的输出与input tensor的data layout有关。设input tensor为ColMajor格式[NHWC]，则image patches输出为[batch, filter_index, filter_rows, filter_cols, in_depth]，并reshape为[batch * filter_index, filter_rows * filter_cols * in_depth]，而kernels维度为[filter_rows * filter_cols * in_depth, out_depth]，然后kernels矩阵乘image patches得到输出矩阵[batch * filter_index, out_depth]，并reshape为[batch, out_rows, out_cols, out_depth]。</div><div><br/></div><div>Conv2d-GPU模式下调用基于cuDNN的stream_executor库。若input tensor为NHWC格式的，则先转换为NCHW格式</div><div><p><a href="images/4b51c0499e6174fd53e58f77ec859080.png" target="_blank"><img src="images/4b51c0499e6174fd53e58f77ec859080.png"/></a></p></div><div>调用cudnn库实现卷积运算：</div><div><p><a href="images/7391f1050265b616854bc8ddcc55b9e1.png" target="_blank"><img src="images/7391f1050265b616854bc8ddcc55b9e1.png"/></a></p></div><div>计算完成后再转换成HHWC格式的</div><div><p><a href="images/0d3247ccb1304b6f6ec7f69968e6fc21.png" target="_blank"><img src="images/0d3247ccb1304b6f6ec7f69968e6fc21.png"/></a></p></div><div>4.4.2 Conv2d梯度计算部分</div><div>Conv2D梯度计算公式，假设output=Conv2d(input, filter)，则</div><div><p><a href="images/f167f0ef462d22d5ecea2effacea26f0.png" target="_blank"><img src="images/f167f0ef462d22d5ecea2effacea26f0.png"/></a></p></div><div>Conv2D梯度计算的测试用例core/kernels/eigen_backward_spatial_convolutions_test.cc文件，要调试这个测试用例，可通过如下方式：</div><div><br/></div><div>Conv2d的梯度计算函数描述为Conv2DGrad。Conv2DGrad操作定义在grad_ops工厂中。注册方式如下：</div><div><p><a href="images/b416330185f4a64141e0d81725ddcec5.png" target="_blank"><img src="images/b416330185f4a64141e0d81725ddcec5.png"/></a></p></div><div>Conv2DGrad由FDH（Function Define Helper）完成定义，参见图 4 3。</div><div><p><a href="images/f4d2381e98ad0b987d6e7f57053f64b5.jpg" target="_blank"><img src="images/f4d2381e98ad0b987d6e7f57053f64b5.jpg"/></a></p></div><div>图 4 3 Conv2DGrad的函数定义</div><div><br/></div><div>Conv2DGrad梯度函数定义中依赖Conv2DBackpropInput和Conv2DBackpropFilter两种Ops，二者均定义在kernels/conv_grad_ops.cc文件中。</div><div><br/></div><div>Conv2DBackpropInputOp和Conv2DBackpropFilterOp的实现分为GPU和CPU版本。</div><div><br/></div><div>Conv2D运算的GPU版实现定义在类Conv2DSlowBackpropInputOp<gpudevice, t="">和类Conv2DSlowBackprop FilterOp <gpudevice, t="">中。</gpudevice,></gpudevice,></div><div><br/></div><div>Conv2D运算的CPU版有两种实现形式，分别为custom模式和fast模式。Custom模式基于贾扬清在caffe中的思路实现，相关类是Conv2DCustomBackpropInputOp<cpudevice, t="">和Conv2DCustomBackpropFilterOp<cpudevice, t="">。Fast模式基于Eigen计算库，由于在GPU下会出现nvcc编译超时，目前仅适用于CPU环境，相关类是Conv2DFastBackpropInputOp<cpudevice, t="">和Conv2DFastBackpropFilterOp<cpudevice, t="">。</cpudevice,></cpudevice,></cpudevice,></cpudevice,></div><div><br/></div><div>根据Conv2DGrad的函数定义，从代码分析Conv2D-GPU版的实现代码，即分析Conv2DBackpropInput和Conv2DBackpropFilter的实现方式。</div><div><p><a href="images/76712116d219321dcd18fdda80c20872.png" target="_blank"><img src="images/76712116d219321dcd18fdda80c20872.png"/></a></p></div><div><br/></div><div>Conv2DSlowBackpropInputOp的成员函数Compute完成计算操作。</div><div><br/></div><div>Compute实现部分调用stream executor的相关函数，需要先获取库的stream句柄，再调用卷积梯度函数。</div><div><p><a href="images/4e8ebcab3ecafe740c29ea678bb7e240.png" target="_blank"><img src="images/4e8ebcab3ecafe740c29ea678bb7e240.png"/></a></p></div><div>stream executor在卷积梯度运算部分仍然是借助cudnn库实现的。</div><div><p><a href="images/0872a283ec6e14958b48dcf0dea41381.png" target="_blank"><img src="images/0872a283ec6e14958b48dcf0dea41381.png"/></a></p></div><div>4.4.3 MaxPooling计算部分</div><div>在很多图像分类和识别问题中都用到了池化运算，池化操作主要有较大池化（max pooling）和均值池化（avg pooling），本章节主要介绍较大池化的实现方法。调用TF接口可以很容易实现池化操作。</div><div><p><a href="images/191ae70174abc633c4f77d6b032f2e3a.png" target="_blank"><img src="images/191ae70174abc633c4f77d6b032f2e3a.png"/></a></p></div><div>Eigen库中较大池化的详细描述如下：</div><div><p><a href="images/cfb000b2b3a1c4fc2d4d9f6007d548a8.png" target="_blank"><img src="images/cfb000b2b3a1c4fc2d4d9f6007d548a8.png"/></a></p></div><div>其中较大池化运算主要分为两步，第一步中extract_image_patch为池化操作抽取与kernel size一致的image patches，第二步计算每个image patch的较大值。</div><div><br/></div><div>4.5 SendOp &amp; RecvOp</div><div>TF所有操作都是节点形式表示的，包括计算节点和非计算节点。在跨设备通信中，发送节点（SendOp）和接收节点（RecvOp）为不同设备的两个相邻节点完成完成数据通信操作。Send和Recv通过TCP或RDMA来传输数据。</div><div><br/></div><div>TF采用Rendezvous（回合）通信机制，Rendezvous类似生产者/消费者的消息信箱。引用TF描述如下：</div><div><p><a href="images/5726f4a5d6bd66f602628a58cd4b3b2a.jpg" target="_blank"><img src="images/5726f4a5d6bd66f602628a58cd4b3b2a.jpg"/></a></p></div><div>TF的消息传递属于采用“发送不阻塞/接收阻塞”机制，实现场景有LocalRendezvous</div><div><br/></div><div>（本地消息传递）、RpcRemoteRendezvous (分布式消息传递)。除此之外还有IntraProcessRendezvous用于本地不同设备间通信。</div><div><br/></div><div>TF会在不同设备的两个相邻节点之间添加Send和Recv节点，通过Send和Recv之间进行通信来达到op之间通信的效果，如图 4 4右子图所示。图中还涉及到一个优化问题，即a-&gt;b和a-&gt;c需要建立两组send/recv连接的，但两组连接是可以共用的，所以合并成一组连接。</div><div><p><a href="images/c25e4cd775fe35599e0f55850d432c1a.jpg" target="_blank"><img src="images/c25e4cd775fe35599e0f55850d432c1a.jpg"/></a></p></div><div>图 4 4 Graph跨设备通信</div><div><br/></div><div>Send和Recv分别对应OpKernel中的SendOp和RecvOp两个类(kernels/sendrecv_ops.h)。</div><div><br/></div><div>SendOp的计算函数。</div><div><p><a href="images/00585a4e3bed5948cfac5c1cf36ca666.png" target="_blank"><img src="images/00585a4e3bed5948cfac5c1cf36ca666.png"/></a></p></div><div>SendOp作为发送方需要先获取封装ctx消息，然后借助Rendezvous模块发送给接收方。</div><div><br/></div><div>RecvOp的计算函数如下。</div><div><br/></div><div>RecvOp作为接收方借助Rendezvous模块获取ctx消息。</div><div><br/></div><div>其中parsed变量是类ParsedKey的实例。图 5‑5是Rendezvous封装的ParsedKey消息实体示例。</div><div><p><a href="images/074c03777780823f5a26a837f7baaa82.jpg" target="_blank"><img src="images/074c03777780823f5a26a837f7baaa82.jpg"/></a></p></div><div>4.6 ReaderOp &amp; QueueOp</div><div><br/></div><div>4.6.1 TF数据读取</div><div>TF系统定义了三种数据读取方式[13]：</div><div><br/></div><div>Ø  供给数据(Feeding)： 在TensorFlow程序运行的每一步， 通过feed_dict来供给数据。</div><div><br/></div><div>Ø  从文件读取数据： 在TensorFlow图的起始， 让一个输入管线（piplines）从文件中读取数据放入队列，通过QueueRunner供给数据，其中队列可以实现多线程异步计算。</div><div><br/></div><div>Ø  预加载数据： 在TensorFlow图中定义常量或变量来保存所有数据，如Mnist数据集（仅适用于数据量比较小的情况）。</div><div><br/></div><div>除了以上三种数据读取方式外，TF还支持用户自定义数据读取方式，即继承ReaderOpKernel类创建新的输入读取类[14]。本章节主要讲述通过piplines方式读取数据的方法。</div><div><br/></div><div>Piplines利用队列实现异步计算</div><div>从piplines读取数据也有两种方式：一种是读取所有样本文件路径名转换成string tensor，使用input_producer将tensor乱序（shuffle）或slice（切片）处理放入队列中；另一种是将数据转化为TF标准输入格式，即使用TFRecordWriter将样本数据写入tfrecords文件中，再使用TFRecordReader将tfrecords文件读取到队列中。</div><div><br/></div><div>图 4 6描述了piplines读取数据的第一种方式，这些流程通过节点和边串联起来，成为graph数据流的一部分。</div><div><br/></div><div>从左向右， 第一步 是载入文件列表，使用convert_to_tensor函数将文件列表转化为tensor，如cifar10数据集中的image_files_tensor和label_tensor。</div><div><br/></div><div>第二步是使用input_producer将image_files_tensor和label_tensor放入图中的文件队列中，这里的input_producer作用就是将样本放入队列节点中，有string_input_producer、range_input_producer和slice_input_producer三种，其中slice_input_producer的切片功能支持乱序，其他两种需要借助tf.train.shuffle_batch函数作乱序处理，有关三种方式的具体描述可参考tensorflow/python/training/input.py注释说明。</div><div><br/></div><div>第三步是使用tf.read_file()读取队列中的文件数据到内存中，使用解码器如tf.image.decode_jpeg()解码成[height, width, channels]格式的数据。</div><div><br/></div><div>最后就是使用batch函数将样本数据处理成一批批的样本，然后使用session执行训练。</div><div><p><a href="images/bbe78baca366eea8137d82c34fa11d75.jpg" target="_blank"><img src="images/bbe78baca366eea8137d82c34fa11d75.jpg"/></a></p></div><div>图 4 6 使用piplines读取数据</div><div><br/></div><div>4.6.2 TFRecords使用</div><div>TFRecords是TF支持的标准文件格式，这种格式允许将任意的数据转换为TFRecords支持的文件格式。TFRecords方法需要两步：第一步是使用TFRecordWriter将样本数据写入tfrecords文件中，第二步是使用TFRecordReader将tfrecords文件读取到队列中。</div><div><br/></div><div>图 4 7是TFRecords文件写入的简单示例。tf.train.Example将数据填入到Example协议内存块(protocol buffer)，将协议内存块序列化为一个字符串，通过TFRecordWriter写入到TFRecords文件，图中定义了label和image_raw两个feature。Example协议内存块的定义请参考文件core/example/example.proto。</div><div><p><a href="images/f352c4992e7ca80391ac6c021a0ed7b9.png" target="_blank"><img src="images/f352c4992e7ca80391ac6c021a0ed7b9.png"/></a></p></div><div>图 4 7 TFRecordWriter写入数据示例</div><div><br/></div><div>图 4 8是TFRecords文件读取的简单示例。tf.parse_single_example解析器将Example协议内存块解析为张量，放入example队列中，其中features命名和类型要与Example写入的一致。</div><div><p><a href="images/51e1b3c9668c101d7ee400ffd4d14266.png" target="_blank"><img src="images/51e1b3c9668c101d7ee400ffd4d14266.png"/></a></p></div><div>图 4 8 TFRecrodReader读取数据示例</div><div><br/></div><div>4.6.3 ReaderOps分析</div><div>ReaderOpsKernel类封装了数据读取的入口函数Compute，通过继承ReaderOpsKernel类可实现各种自定义的数据读取方法。图 4 9是ReaderOp相关的UML视图。</div><div><p><a href="images/c540d20a336319943587fe9de8d4410f.jpg" target="_blank"><img src="images/c540d20a336319943587fe9de8d4410f.jpg"/></a></p></div><div>图 4 9 ReaderOp相关的UML视图</div><div><br/></div><div>ReaderOpKernel子类必须重新定义成员函数SetReaderFactory实现对应的数据读取逻辑。TFRecordReaderOp的读取方法定义在TFRecordReader类中。</div><div><p><a href="images/ebc9950abf77f99eabed32d89ba1e1c2.png" target="_blank"><img src="images/ebc9950abf77f99eabed32d89ba1e1c2.png"/></a></p></div><div>其中offset的计算方式。</div><div><p><a href="images/986d0e579124c319508325535e5f0d30.png" target="_blank"><img src="images/986d0e579124c319508325535e5f0d30.png"/></a></p></div><div><br/></div><div>作者简介：</div><div><p><a href="images/aede82e65dde405c0f4c554ce3185821.jpg" target="_blank"><img src="images/aede82e65dde405c0f4c554ce3185821.jpg"/></a></p></div><div>姚健，毕业于中科院计算所网络数据实验室，毕业后就职于360天眼实验室，主要从事<a class="relatedlink" href="http://www.dataguru.cn/article-9400-1.html?union_site=innerlink" target="_blank">深度学习</a>和增强学习相关研究工作。目前就职于腾讯MIG事业部，从事神经机器翻译工作。联系方式： yao_62995@163.com</div><div><br/></div><div><div><b><font color="#f00000">欢迎加入本站公开兴趣群</font></b></div><div>商业智能与数据分析群</div><div>兴趣范围包括各种让数据产生价值的办法，实际应用案例分享与讨论，分析工具，ETL工具，数据仓库，数据挖掘工具，报表系统等全方位知识</div><div>QQ群：81035754</div></div> </td>
</body></html>