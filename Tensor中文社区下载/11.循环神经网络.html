<!DOCTYPE html>
    <html><head><meta charset="UTF-8">
    </head><body>
    <p><a href="http://www.tensorfly.cn/tfdoc/tutorials/overview.html">原文链接</a></p>
        <section class="normal" id="section-">
<h1 id="循环神经网络-">循环神经网络 <a class="md-anchor" id="AUTOGENERATED-recurrent-neural-networks"></a></h1>
<h2 id="介绍-">介绍 <a class="md-anchor" id="AUTOGENERATED-introduction"></a></h2>
<p>可以在 <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">this great article</a> 查看循环神经网络(RNN)以及 LSTM 的介绍。</p>
<h2 id="语言模型-">语言模型 <a class="md-anchor" id="AUTOGENERATED-language-modeling"></a></h2>
<p>此教程将展示如何在高难度的语言模型中训练循环神经网络。该问题的目标是获得一个能确定语句概率的概率模型。为了做到这一点，通过之前已经给出的词语来预测后面的词语。我们将使用 PTB(Penn Tree Bank) 数据集，这是一种常用来衡量模型的基准，同时它比较小而且训练起来相对快速。</p>
<p>语言模型是很多有趣难题的关键所在，比如语音识别，机器翻译，图像字幕等。它很有意思--可以参看 <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank">here</a>。</p>
<p>本教程的目的是重现 <a href="http://arxiv.org/abs/1409.2329" target="_blank">Zaremba et al., 2014</a> 的成果，他们在 PTB 数据集上得到了很棒的结果。</p>
<h2 id="教程文件-">教程文件 <a class="md-anchor" id="AUTOGENERATED-tutorial-files"></a></h2>
<p>本教程使用的下面文件的目录是 <code>models/rnn/ptb</code>:</p>
<table>
<thead>
<tr>
<th>文件</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ptb_word_lm.py</code></td>
<td>在 PTB 数据集上训练一个语言模型.</td>
</tr>
<tr>
<td><code>reader.py</code></td>
<td>读取数据集.</td>
</tr>
</tbody>
</table>
<h2 id="下载及准备数据-">下载及准备数据 <a class="md-anchor" id="AUTOGENERATED-download-and-prepare-the-data"></a></h2>
<p>本教程需要的数据在 data/ 路径下，来源于 Tomas Mikolov 网站上的 PTB 数据集<code>http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz</code>。</p>
<p>该数据集已经预先处理过并且包含了全部的 10000 个不同的词语，其中包括语句结束标记符，以及标记稀有词语的特殊符号 <code>(&lt;unk&gt;)</code> 。我们在 <code>reader.py</code> 中转换所有的词语，让他们各自有唯一的整型标识符，便于神经网络处理。</p>
<h2 id="模型-">模型 <a class="md-anchor" id="AUTOGENERATED-the-model"></a></h2>
<h3 id="lstm-">LSTM <a class="md-anchor" id="AUTOGENERATED-lstm"></a></h3>
<p>模型的核心由一个 LSTM 单元组成，其可以在某时刻处理一个词语，以及计算语句可能的延续性的概率。网络的存储状态由一个零矢量初始化并在读取每一个词语后更新。而且，由于计算上的原因，我们将以 <code>batch_size</code> 为最小批量来处理数据。</p>
<p>基础的伪代码就像下面这样：</p>
<pre><code class="lang-python">lstm = rnn_cell.BasicLSTMCell(lstm_size)
<span class="hljs-comment"># 初始化 LSTM 存储状态.</span>
state = tf.zeros([batch_size, lstm.state_size])

loss = <span class="hljs-number">0.0</span>
<span class="hljs-keyword">for</span> current_batch_of_words <span class="hljs-keyword">in</span> words_in_dataset:
    <span class="hljs-comment"># 每次处理一批词语后更新状态值.</span>
    output, state = lstm(current_batch_of_words, state)

    <span class="hljs-comment"># LSTM 输出可用于产生下一个词语的预测</span>
    logits = tf.matmul(output, softmax_w) + softmax_b
    probabilities = tf.nn.softmax(logits)
    loss += loss_function(probabilities, target_words)
</code></pre>
<h3 id="截断反向传播-">截断反向传播 <a class="md-anchor" id="AUTOGENERATED-truncated-backpropagation"></a></h3>
<p>为使学习过程易于处理，通常的做法是将反向传播的梯度在（按时间）展开的步骤上照一个固定长度(<code>num_steps</code>)截断。
通过在一次迭代中的每个时刻上提供长度为 <code>num_steps</code> 的输入和每次迭代完成之后反向传导，这会很容易实现。</p>
<p>一个简化版的用于计算图创建的截断反向传播代码：</p>
<pre><code class="lang-python"><span class="hljs-comment"># 一次给定的迭代中的输入占位符.</span>
words = tf.placeholder(tf.int32, [batch_size, num_steps])

lstm = rnn_cell.BasicLSTMCell(lstm_size)
<span class="hljs-comment"># 初始化 LSTM 存储状态.</span>
initial_state = state = tf.zeros([batch_size, lstm.state_size])

<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(num_steps)):
    <span class="hljs-comment"># 每处理一批词语后更新状态值.</span>
    output, state = lstm(words[:, i], state)

    <span class="hljs-comment"># 其余的代码.</span>
    <span class="hljs-comment"># ...</span>

final_state = state
</code></pre>
<p>下面展现如何实现迭代整个数据集：</p>
<pre><code class="lang-python"><span class="hljs-comment"># 一个 numpy 数组，保存每一批词语之后的 LSTM 状态.</span>
numpy_state = initial_state.eval()
total_loss = <span class="hljs-number">0.0</span>
<span class="hljs-keyword">for</span> current_batch_of_words <span class="hljs-keyword">in</span> words_in_dataset:
    numpy_state, current_loss = session.run([final_state, loss],
        <span class="hljs-comment"># 通过上一次迭代结果初始化 LSTM 状态.</span>
        feed_dict={initial_state: numpy_state, words: current_batch_of_words})
    total_loss += current_loss
</code></pre>
<h3 id="输入-">输入 <a class="md-anchor" id="AUTOGENERATED-inputs"></a></h3>
<p>在输入 LSTM 前，词语 ID 被嵌入到了一个密集的表示中(查看 <a href="word2vec.html">矢量表示教程</a>)。这种方式允许模型高效地表示词语，也便于写代码：</p>
<pre><code class="lang-python"><span class="hljs-comment"># embedding_matrix 张量的形状是： [vocabulary_size, embedding_size]</span>
word_embeddings = tf.nn.embedding_lookup(embedding_matrix, word_ids)
</code></pre>
<p>嵌入的矩阵会被随机地初始化，模型会学会通过数据分辨不同词语的意思。</p>
<h3 id="损失函数-">损失函数 <a class="md-anchor" id="AUTOGENERATED-loss-fuction"></a></h3>
<p>我们想使目标词语的平均负对数概率最小</p>
<p><img alt="" src="images/05332d8d11592754992393f6272816d5.png"/></p>
<p>实现起来并非很难，而且函数 <code>sequence_loss_by_example</code> 已经有了，可以直接使用。</p>
<p>论文中的典型衡量标准是每个词语的平均困惑度（perplexity），计算式为</p>
<p><img alt="" src="images/cadc043f87b9d5422bc2d48eb93f5b3a.png"/></p>
<p>同时我们会观察训练过程中的困惑度值（perplexity）。</p>
<h3 id="多个-lstm-层堆叠-">多个 LSTM 层堆叠 <a class="md-anchor" id="AUTOGENERATED-stacking-multiple-lstms"></a></h3>
<p>要想给模型更强的表达能力，可以添加多层 LSTM 来处理数据。第一层的输出作为第二层的输入，以此类推。</p>
<p>类 <code>MultiRNNCell</code> 可以无缝的将其实现：</p>
<pre><code class="lang-python">lstm = rnn_cell.BasicLSTMCell(lstm_size)
stacked_lstm = rnn_cell.MultiRNNCell([lstm] * number_of_layers)

initial_state = state = stacked_lstm.zero_state(batch_size, tf.float32)
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(num_steps)):
    <span class="hljs-comment"># 每次处理一批词语后更新状态值.</span>
    output, state = stacked_lstm(words[:, i], state)

    <span class="hljs-comment"># 其余的代码.</span>
    <span class="hljs-comment"># ...</span>

final_state = state
</code></pre>
<h2 id="编译并运行代码-">编译并运行代码 <a class="md-anchor" id="AUTOGENERATED-compile-and-run-the-code"></a></h2>
<p>首先需要构建库，在 CPU 上编译：</p>
<pre><code>bazel build -c opt tensorflow/models/rnn/ptb:ptb_word_lm
</code></pre><p>如果你有一个强大的 GPU，可以运行：</p>
<pre><code>bazel build -c opt --config=cuda tensorflow/models/rnn/ptb:ptb_word_lm
</code></pre><p>运行模型：</p>
<pre><code>bazel-bin/tensorflow/models/rnn/ptb/ptb_word_lm \
  --data_path=/tmp/simple-examples/data/ --alsologtostderr --model small
</code></pre><p>教程代码中有 3 个支持的模型配置参数："small"，
"medium" 和 "large"。它们指的是 LSTM 的大小，以及用于训练的超参数集。</p>
<p>模型越大，得到的结果应该更好。在测试集中 <code>small</code> 模型应该可以达到低于 120 的困惑度（perplexity），<code>large</code> 模型则是低于 80，但它可能花费数小时来训练。</p>
<h2 id="除此之外？-">除此之外？ <a class="md-anchor" id="AUTOGENERATED-what-next-"></a></h2>
<p>还有几个优化模型的技巧没有提到，包括：</p>
<ul>
<li>随时间降低学习率,</li>
<li>LSTM 层间 dropout.</li>
</ul>
<p>继续学习和更改代码以进一步改善模型吧。</p>
<p>原文：<a href="http://tensorflow.org/tutorials/recurrent/index.md" target="_blank">Recurrent Neural Networks</a>
翻译：<a href="https://github.com/Warln" target="_blank">Warln</a>
校对：<a href="https://github.com/wanghong-yang" target="_blank">HongyangWang</a></p>
</section>
    </body></html>