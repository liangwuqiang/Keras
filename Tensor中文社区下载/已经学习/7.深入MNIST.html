<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8"/>
	<title></title>
	<meta name="generator" content="LibreOffice 5.1.4.2 (Linux)"/>
	<meta name="created" content="00:00:00"/>
	<meta name="changed" content="2017-09-21T11:09:21.310766477"/>
	<meta name="" content=""/>
	<style type="text/css">
		pre.ctl { font-family: "Liberation Mono", monospace }
		code.ctl { font-family: "Liberation Mono", monospace }
	</style>
</head>
<body lang="zh-CN" dir="ltr">
<p><a href="http://www.tensorfly.cn/tfdoc/tutorials/overview.html"><span style="background: #ffff00">原文链接</span></a></p>
<h1><a name="深入mnist-"></a><a name="AUTOGENERATED-deep-mnist-for-experts"></a>
<span style="background: #ffff00">深入<font face="Thorndale, serif"><font size="6" style="font-size: 24pt"><span lang="en-US">MNIST
</span></span></font></font>
</h1>
<p><font face="Liberation Serif, serif"><span lang="en-US"><span style="background: #ffff00">TensorFlow</span></font>是一个非常强大的用来做大规模数值计算的库。其所擅长的任务之一就是实现以及训练深度神经网络。</span></p>
<p><span style="background: #ffff00">在本教程中，我们将学到构建一个<font face="Liberation Serif, serif"><span lang="en-US">TensorFlow</span></font>模型的基本步骤，并将通过这些步骤为<font face="Liberation Serif, serif"><span lang="en-US">MNIST</span></font>构建一个深度卷积神经网络。</span></p>
<p><em><span style="background: #ffff00">这个教程假设你已经熟悉神经网络和<font face="Liberation Serif, serif"><span lang="en-US">MNIST</span></font>数据集。如果你尚未了解，请查看<a href="mnist_beginners.html">新手指南</a><font face="Liberation Serif, serif"><span lang="en-US">.</span></em></span></font></p>
<h2><a name="安装-"></a><a name="AUTOGENERATED-setup"></a><span style="background: #ffff00">安装
</span>
</h2>
<p><span style="background: #ffff00">在创建模型之前，我们会先加载<font face="Liberation Serif, serif"><span lang="en-US">MNIST</span></font>数据集，然后启动一个<font face="Liberation Serif, serif"><span lang="en-US">TensorFlow</span></font>的<font face="Liberation Serif, serif"><span lang="en-US">session</span></font>。</span></p>
<h3><a name="加载mnist数据-"></a><a name="AUTOGENERATED-load-mnist-data"></a>
<span style="background: #ffff00">加载<font face="Liberation Serif, serif"><span lang="en-US">MNIST</span></font>数据
</span>
</h3>
<p><span style="background: #ffff00">为了方便起见，我们已经准备了<a href="https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/mnist/input_data.py" target="_blank">一个脚本</a>来自动下载和导入<font face="Liberation Serif, serif"><span lang="en-US">MNIST</span></font>数据集。它会自动创建一个</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">&amp;apos;MNIST_data&amp;apos;</span></code></span></font><span style="background: #ffff00">的目录来存储数据。</span></p>
<pre class="cjk"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">import input_data</span></code></span></font>
<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">mnist = input_data.read_data_sets(&amp;apos;MNIST_data&amp;apos;, one_hot=True)</span></code></span></font></pre><p>
<span style="background: #ffff00">这里，</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">mnist</span></code></span></font><span style="background: #ffff00">是一个轻量级的类。它以<font face="Liberation Serif, serif"><span lang="en-US">Numpy</span></font>数组的形式存储着训练、校验和测试数据集。同时提供了一个函数，用于在迭代中获得<font face="Liberation Serif, serif"><span lang="en-US">minibatch</span></font>，后面我们将会用到。</span></p>
<h3><a name="运行tensorflow的interactivesession-"></a><a name="AUTOGENERATED-start-tensorflow-interactivesession"></a>
<span style="background: #ffff00">运行<font face="Liberation Serif, serif"><span lang="en-US">TensorFlow</span></font>的<font face="Liberation Serif, serif"><span lang="en-US">InteractiveSession
</span></span></font>
</h3>
<p><font face="Liberation Serif, serif"><span lang="en-US"><span style="background: #ffff00">Tensorflow</span></font>依赖于一个高效的<font face="Liberation Serif, serif"><span lang="en-US">C++</span></font>后端来进行计算。与后端的这个连接叫做<font face="Liberation Serif, serif"><span lang="en-US">session</span></font>。一般而言，使用<font face="Liberation Serif, serif"><span lang="en-US">TensorFlow</span></font>程序的流程是先创建一个图，然后在<font face="Liberation Serif, serif"><span lang="en-US">session</span></font>中启动它。</span></p>
<p><span style="background: #ffff00">这里，我们使用更加方便的</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">InteractiveSession</span></code></span></font><span style="background: #ffff00">类。通过它，你可以更加灵活地构建你的代码。它能让你在运行图的时候，插入一些<a href="../get_started/basic_usage.html#the-computation-graph">计算图</a>，这些计算图是由某些操作<font face="Liberation Serif, serif"><span lang="en-US">(operations)</span></font>构成的。这对于工作在交互式环境中的人们来说非常便利，比如使用<font face="Liberation Serif, serif"><span lang="en-US">IPython</span></font>。如果你没有使用</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">InteractiveSession</span></code></span></font><span style="background: #ffff00">，那么你需要在启动<font face="Liberation Serif, serif"><span lang="en-US">session</span></font>之前构建整个计算图，然后<a href="../get_started/basic_usage.html#launching-the-graph-in-a-session">启动该计算图</a>。</span></p>
<pre class="cjk"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">import tensorflow as tf</span></code></span></font>
<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">sess = tf.InteractiveSession()</span></code></span></font></pre><h4>
<a name="计算图-"></a><a name="AUTOGENERATED-computation-graph"></a>
<span style="background: #ffff00">计算图 </span>
</h4>
<p><span style="background: #ffff00">为了在<font face="Liberation Serif, serif"><span lang="en-US">Python</span></font>中进行高效的数值计算，我们通常会使用像<font face="Liberation Serif, serif"><span lang="en-US">NumPy</span></font>一类的库，将一些诸如矩阵乘法的耗时操作在<font face="Liberation Serif, serif"><span lang="en-US">Python</span></font>环境的外部来计算，这些计算通常会通过其它语言并用更为高效的代码来实现。</span></p>
<p><span style="background: #ffff00">但遗憾的是，每一个操作切换回<font face="Liberation Serif, serif"><span lang="en-US">Python</span></font>环境时仍需要不小的开销。如果你想在<font face="Liberation Serif, serif"><span lang="en-US">GPU</span></font>或者分布式环境中计算时，这一开销更加可怖，这一开销主要可能是用来进行数据迁移。</span></p>
<p><font face="Liberation Serif, serif"><span lang="en-US"><span style="background: #ffff00">TensorFlow</span></font>也是在<font face="Liberation Serif, serif"><span lang="en-US">Python</span></font>外部完成其主要工作，但是进行了改进以避免这种开销。其并没有采用在<font face="Liberation Serif, serif"><span lang="en-US">Python</span></font>外部独立运行某个耗时操作的方式，而是先让我们描述一个交互操作图，然后完全将其运行在<font face="Liberation Serif, serif"><span lang="en-US">Python</span></font>外部。这与<font face="Liberation Serif, serif"><span lang="en-US">Theano</span></font>或<font face="Liberation Serif, serif"><span lang="en-US">Torch</span></font>的做法类似。</span></p>
<p><span style="background: #ffff00">因此<font face="Liberation Serif, serif"><span lang="en-US">Python</span></font>代码的目的是用来构建这个可以在外部运行的计算图，以及安排计算图的哪一部分应该被运行。详情请查看<a href="../get_started/basic_usage.html">基本用法</a>中的<a href="../get_started/basic_usage.html#the-computation-graph">计算图表</a>一节。</span></p>
<h2><a name="构建softmax-回归模型-"></a><a name="AUTOGENERATED-build-a-softmax-regression-model"></a>
<span style="background: #ffff00">构建<font face="Liberation Serif, serif"><span lang="en-US">Softmax
</span></font>回归模型 </span>
</h2>
<p><span style="background: #ffff00">在这一节中我们将建立一个拥有一个线性层的<font face="Liberation Serif, serif"><span lang="en-US">softmax</span></font>回归模型。在下一节，我们会将其扩展为一个拥有多层卷积网络的<font face="Liberation Serif, serif"><span lang="en-US">softmax</span></font>回归模型。</span></p>
<h3><a name="占位符-"></a><a name="AUTOGENERATED-placeholders"></a>
<span style="background: #ffff00">占位符 </span>
</h3>
<p><span style="background: #ffff00">我们通过为输入图像和目标输出类别创建节点，来开始构建计算图。</span></p>
<pre class="cjk"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">x = tf.placeholder(&quot;float&quot;, shape=[None, 784])</span></code></span></font>
<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">y_ = tf.placeholder(&quot;float&quot;, shape=[None, 10])</span></code></span></font></pre><p>
<span style="background: #ffff00">这里的</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">x</span></code></span></font><span style="background: #ffff00">和</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">y</span></code></span></font><span style="background: #ffff00">并不是特定的值，相反，他们都只是一个</span><code class="cjk"><span style="background: #ffff00">占位符</span></code><span style="background: #ffff00">，可以在<font face="Liberation Serif, serif"><span lang="en-US">TensorFlow</span></font>运行某一计算时根据该占位符输入具体的值。</span></p>
<p><span style="background: #ffff00">输入图片</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">x</span></code></span></font><span style="background: #ffff00">是一个<font face="Liberation Serif, serif"><span lang="en-US">2</span></font>维的浮点数张量。这里，分配给它的</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">shape</span></code></span></font><span style="background: #ffff00">为</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">[None,
784]</span></code></span></font><span style="background: #ffff00">，其中</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">784</span></code></span></font><span style="background: #ffff00">是一张展平的<font face="Liberation Serif, serif"><span lang="en-US">MNIST</span></font>图片的维度。</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">None</span></code></span></font><span style="background: #ffff00">表示其值大小不定，在这里作为第一个维度值，用以指代<font face="Liberation Serif, serif"><span lang="en-US">batch</span></font>的大小，意即</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">x</span></code></span></font><span style="background: #ffff00">的数量不定。输出类别值</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">y_</span></code></span></font><span style="background: #ffff00">也是一个<font face="Liberation Serif, serif"><span lang="en-US">2</span></font>维张量，其中每一行为一个<font face="Liberation Serif, serif"><span lang="en-US">10</span></font>维的<font face="Liberation Serif, serif"><span lang="en-US">one-hot</span></font>向量<font face="Liberation Serif, serif"><span lang="en-US">,</span></font>用于代表对应某一<font face="Liberation Serif, serif"><span lang="en-US">MNIST</span></font>图片的类别。</span></p>
<p><span style="background: #ffff00">虽然</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">placeholder</span></code></span></font><span style="background: #ffff00">的</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">shape</span></code></span></font><span style="background: #ffff00">参数是可选的，但有了它，<font face="Liberation Serif, serif"><span lang="en-US">TensorFlow</span></font>能够自动捕捉因数据维度不一致导致的错误。</span></p>
<h3><a name="变量-"></a><a name="AUTOGENERATED-variables"></a><span style="background: #ffff00">变量
</span>
</h3>
<p><span style="background: #ffff00">我们现在为模型定义权重</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">W</span></code></span></font><span style="background: #ffff00">和偏置</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">b</span></code></span></font><span style="background: #ffff00">。可以将它们当作额外的输入量，但是<font face="Liberation Serif, serif"><span lang="en-US">TensorFlow</span></font>有一个更好的处理方式：</span><code class="cjk"><span style="background: #ffff00">变量</span></code><span style="background: #ffff00">。一个</span><code class="cjk"><span style="background: #ffff00">变量</span></code><span style="background: #ffff00">代表着<font face="Liberation Serif, serif"><span lang="en-US">TensorFlow</span></font>计算图中的一个值，能够在计算过程中使用，甚至进行修改。在机器学习的应用过程中，模型参数一般用</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">Variable</span></code></span></font><span style="background: #ffff00">来表示。</span></p>
<pre class="cjk"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">W = tf.Variable(tf.zeros([784,10]))</span></code></span></font>
<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">b = tf.Variable(tf.zeros([10]))</span></code></span></font></pre><p>
<span style="background: #ffff00">我们在调用</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">tf.Variable</span></code></span></font><span style="background: #ffff00">的时候传入初始值。在这个例子里，我们把</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">W</span></code></span></font><span style="background: #ffff00">和</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">b</span></code></span></font><span style="background: #ffff00">都初始化为零向量。</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">W</span></code></span></font><span style="background: #ffff00">是一个<font face="Liberation Serif, serif"><span lang="en-US">784x10</span></font>的矩阵（因为我们有<font face="Liberation Serif, serif"><span lang="en-US">784</span></font>个特征和<font face="Liberation Serif, serif"><span lang="en-US">10</span></font>个输出值）。</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">b</span></code></span></font><span style="background: #ffff00">是一个<font face="Liberation Serif, serif"><span lang="en-US">10</span></font>维的向量（因为我们有<font face="Liberation Serif, serif"><span lang="en-US">10</span></font>个分类）。</span></p>
<p><font face="Liberation Serif, serif"><span lang="en-US">Before
<code class="western">Variable</code>s can be used within a session,
they must be initialized using that session. This step takes the
initial values (in this case tensors full of zeros) that have already
been specified, and assigns them to each <code class="western">Variable</code>.
This can be done for all <code class="western">Variables</code> at
once.</span></font></p>
<p><code class="cjk"><span style="background: #ffff00">变量</span></code><span style="background: #ffff00">需要通过<font face="Liberation Serif, serif"><span lang="en-US">seesion</span></font>初始化后，才能在<font face="Liberation Serif, serif"><span lang="en-US">session</span></font>中使用。这一初始化步骤为，为初始值指定具体值（本例当中是全为零），并将其分配给每个</span><code class="cjk"><span style="background: #ffff00">变量</span></code><font face="Liberation Serif, serif"><span lang="en-US"><span style="background: #ffff00">,</span></font>可以一次性为所有</span><code class="cjk"><span style="background: #ffff00">变量</span></code><span style="background: #ffff00">完成此操作。</span></p>
<pre class="cjk" style="margin-bottom: 0.5cm"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">sess.run(tf.initialize_all_variables())</span></code></span></font></pre><h3>
<a name="类别预测与损失函数-"></a><a name="AUTOGENERATED-predicted-class-and-cost-function"></a>
<span style="background: #ffff00">类别预测与损失函数 </span>
</h3>
<p><span style="background: #ffff00">现在我们可以实现我们的回归模型了。这只需要一行！我们把向量化后的图片</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">x</span></code></span></font><span style="background: #ffff00">和权重矩阵</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">W</span></code></span></font><span style="background: #ffff00">相乘，加上偏置</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">b</span></code></span></font><span style="background: #ffff00">，然后计算每个分类的<font face="Liberation Serif, serif"><span lang="en-US">softmax</span></font>概率值。</span></p>
<pre class="cjk" style="margin-bottom: 0.5cm"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">y = tf.nn.softmax(tf.matmul(x,W) + b)</span></code></span></font></pre><p>
<span style="background: #ffff00">可以很容易的为训练过程指定最小化误差用的损失函数，我们的损失函数是目标类别和预测类别之间的交叉熵。</span></p>
<pre class="cjk" style="margin-bottom: 0.5cm"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">cross_entropy = -tf.reduce_sum(y_*tf.log(y))</span></code></span></font></pre><p>
<span style="background: #ffff00">注意，</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">tf.reduce_sum</span></code></span></font><span style="background: #ffff00">把<font face="Liberation Serif, serif"><span lang="en-US">minibatch</span></font>里的每张图片的交叉熵值都加起来了。我们计算的交叉熵是指整个<font face="Liberation Serif, serif"><span lang="en-US">minibatch</span></font>的。</span></p>
<h2><a name="训练模型-"></a><a name="AUTOGENERATED-train-the-model"></a>
<span style="background: #ffff00">训练模型 </span>
</h2>
<p><span style="background: #ffff00">我们已经定义好模型和训练用的损失函数，那么用<font face="Liberation Serif, serif"><span lang="en-US">TensorFlow</span></font>进行训练就很简单了。因为<font face="Liberation Serif, serif"><span lang="en-US">TensorFlow</span></font>知道整个计算图，它可以使用自动微分法找到对于各个变量的损失的梯度值。<font face="Liberation Serif, serif"><span lang="en-US">TensorFlow</span></font>有<a href="../api_docs/python/train.html#optimizers">大量内置的优化算法</a>
这个例子中，我们用最速下降法让交叉熵下降，步长为<font face="Liberation Serif, serif"><span lang="en-US">0.01.</span></span></font></p>
<pre class="cjk" style="margin-bottom: 0.5cm"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)</span></code></span></font></pre><p>
<span style="background: #ffff00">这一行代码实际上是用来往计算图上添加一个新操作，其中包括计算梯度，计算每个参数的步长变化，并且计算出新的参数值。</span></p>
<p><span style="background: #ffff00">返回的</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">train_step</span></code></span></font><span style="background: #ffff00">操作对象，在运行时会使用梯度下降来更新参数。因此，整个模型的训练可以通过反复地运行</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">train_step</span></code></span></font><span style="background: #ffff00">来完成。</span></p>
<pre class="cjk"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">for i in range(1000):</span></code></span></font>
<code class="cjk"><span style="background: #ffff00">  </code><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">batch = mnist.train.next_batch(50)</span></code></span></font>
<code class="cjk"><span style="background: #ffff00">  </code><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">train_step.run(feed_dict={x: batch[0], y_: batch[1]})</span></code></span></font></pre><p>
<span style="background: #ffff00">每一步迭代，我们都会加载<font face="Liberation Serif, serif"><span lang="en-US">50</span></font>个训练样本，然后执行一次</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">train_step</span></code></span></font><span style="background: #ffff00">，并通过</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">feed_dict</span></code></span></font><span style="background: #ffff00">将</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">x</span></code><span style="background: #ffff00">
</span></span></font><span style="background: #ffff00">和
</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">y_</span></code></span></font><span style="background: #ffff00">张量</span><code class="cjk"><span style="background: #ffff00">占位符</span></code><span style="background: #ffff00">用训练训练数据替代。</span></p>
<p><span style="background: #ffff00">注意，在计算图中，你可以用</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">feed_dict</span></code></span></font><span style="background: #ffff00">来替代任何张量，并不仅限于替换</span><code class="cjk"><span style="background: #ffff00">占位符</span></code><span style="background: #ffff00">。</span></p>
<h3><a name="评估模型-"></a><a name="AUTOGENERATED-evaluate-the-model"></a>
<span style="background: #ffff00">评估模型 </span>
</h3>
<p><span style="background: #ffff00">那么我们的模型性能如何呢？</span></p>
<p><span style="background: #ffff00">首先让我们找出那些预测正确的标签。</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">tf.argmax</span></code><span style="background: #ffff00">
</span></span></font><span style="background: #ffff00">是一个非常有用的函数，它能给出某个<font face="Liberation Serif, serif"><span lang="en-US">tensor</span></font>对象在某一维上的其数据最大值所在的索引值。由于标签向量是由<font face="Liberation Serif, serif"><span lang="en-US">0,1</span></font>组成，因此最大值<font face="Liberation Serif, serif"><span lang="en-US">1</span></font>所在的索引位置就是类别标签，比如</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">tf.argmax(y,1)</span></code></span></font><span style="background: #ffff00">返回的是模型对于任一输入<font face="Liberation Serif, serif"><span lang="en-US">x</span></font>预测到的标签值，而
</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">tf.argmax(y_,1)</span></code><span style="background: #ffff00">
</span></span></font><span style="background: #ffff00">代表正确的标签，我们可以用
</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">tf.equal</span></code><span style="background: #ffff00">
</span></span></font><span style="background: #ffff00">来检测我们的预测是否真实标签匹配<font face="Liberation Serif, serif"><span lang="en-US">(</span></font>索引位置一样表示匹配<font face="Liberation Serif, serif"><span lang="en-US">)</span></font>。</span></p>
<pre class="cjk" style="margin-bottom: 0.5cm"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))</span></code></span></font></pre><p>
<span style="background: #ffff00">这里返回一个布尔数组。为了计算我们分类的准确率，我们将布尔值转换为浮点数来代表对、错，然后取平均值。例如：</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">[True,
False, True, True]</span></code></span></font><span style="background: #ffff00">变为</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">[1,0,1,1]</span></code></span></font><span style="background: #ffff00">，计算出平均值为</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">0.75</span></code></span></font><span style="background: #ffff00">。</span></p>
<pre class="cjk" style="margin-bottom: 0.5cm"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))</span></code></span></font></pre><p>
<span style="background: #ffff00">最后，我们可以计算出在测试数据上的准确率，大概是<font face="Liberation Serif, serif"><span lang="en-US">91%</span></font>。</span></p>
<pre class="cjk" style="margin-bottom: 0.5cm"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">print accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels})</span></code></span></font></pre><h2>
<a name="构建一个多层卷积网络-"></a><a name="AUTOGENERATED-build-a-multilayer-convolutional-network"></a>
<span style="background: #ffff00">构建一个多层卷积网络 </span>
</h2>
<p><span style="background: #ffff00">在<font face="Liberation Serif, serif"><span lang="en-US">MNIST</span></font>上只有<font face="Liberation Serif, serif"><span lang="en-US">91%</span></font>正确率，实在太糟糕。在这个小节里，我们用一个稍微复杂的模型：卷积神经网络来改善效果。这会达到大概<font face="Liberation Serif, serif"><span lang="en-US">99.2%</span></font>的准确率。虽然不是最高，但是还是比较让人满意。</span></p>
<h3><a name="权重初始化-"></a><a name="AUTOGENERATED-weight-initialization"></a>
<span style="background: #ffff00">权重初始化 </span>
</h3>
<p><span style="background: #ffff00">为了创建这个模型，我们需要创建大量的权重和偏置项。这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免<font face="Liberation Serif, serif"><span lang="en-US">0</span></font>梯度。由于我们使用的是<font face="Liberation Serif, serif"><span lang="en-US">ReLU</span></font>神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为<font face="Liberation Serif, serif"><span lang="en-US">0</span></font>的问题（<font face="Liberation Serif, serif"><span lang="en-US">dead
neurons</span></font>）。为了不在建立模型的时候反复做初始化操作，我们定义两个函数用于初始化。</span></p>
<pre class="cjk"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">def weight_variable(shape):</span></code></span></font>
<code class="cjk"><span style="background: #ffff00">  </code><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">initial = tf.truncated_normal(shape, stddev=0.1)</span></code></span></font>
<code class="cjk"><span style="background: #ffff00">  </code><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">return tf.Variable(initial)</span></code></span></font>

<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">def bias_variable(shape):</span></code></span></font>
<code class="cjk"><span style="background: #ffff00">  </code><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">initial = tf.constant(0.1, shape=shape)</span></code></span></font>
<code class="cjk"><span style="background: #ffff00">  </code><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">return tf.Variable(initial)</span></code></span></font></pre><h3>
<a name="卷积和池化-"></a><a name="AUTOGENERATED-convolution-and-pooling"></a>
<span style="background: #ffff00">卷积和池化 </span>
</h3>
<p><font face="Liberation Serif, serif"><span lang="en-US"><span style="background: #ffff00">TensorFlow</span></font>在卷积和池化上有很强的灵活性。我们怎么处理边界？步长应该设多大？在这个实例里，我们会一直使用<font face="Liberation Serif, serif"><span lang="en-US">vanilla</span></font>版本。我们的卷积使用<font face="Liberation Serif, serif"><span lang="en-US">1</span></font>步长（<font face="Liberation Serif, serif"><span lang="en-US">stride
size</span></font>），<font face="Liberation Serif, serif"><span lang="en-US">0</span></font>边距（<font face="Liberation Serif, serif"><span lang="en-US">padding
size</span></font>）的模板，保证输出和输入是同一个大小。我们的池化用简单传统的<font face="Liberation Serif, serif"><span lang="en-US">2x2</span></font>大小的模板做<font face="Liberation Serif, serif"><span lang="en-US">max</span>
<span style="background: #ffff00">pooling</span></span></font><span style="background: #ffff00">。为了代码更简洁，我们把这部分抽象成一个函数。</span></p>
<pre class="cjk"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">def conv2d(x, W):</span></code></span></font>
<code class="cjk"><span style="background: #ffff00">  </code><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&amp;apos;SAME&amp;apos;)</span></code></span></font>

<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">def max_pool_2x2(x):</span></code></span></font>
<code class="cjk"><span style="background: #ffff00">  </code><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],</span></code></span></font>
<code class="cjk"><span style="background: #ffff00">                        </code><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">strides=[1, 2, 2, 1], padding=&amp;apos;SAME&amp;apos;)</span></code></span></font></pre><h3>
<a name="第一层卷积-"></a><a name="AUTOGENERATED-first-convolutional-layer"></a>
<span style="background: #ffff00">第一层卷积 </span>
</h3>
<p><span style="background: #ffff00">现在我们可以开始实现第一层了。它由一个卷积接一个<font face="Liberation Serif, serif"><span lang="en-US">max
pooling</span></font>完成。卷积在每个<font face="Liberation Serif, serif"><span lang="en-US">5x5</span></font>的<font face="Liberation Serif, serif"><span lang="en-US">patch</span></font>中算出<font face="Liberation Serif, serif"><span lang="en-US">32</span></font>个特征。卷积的权重张量形状是</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">[5,
5, 1, 32]</span></code></span></font><span style="background: #ffff00">，前两个维度是<font face="Liberation Serif, serif"><span lang="en-US">patch</span></font>的大小，接着是输入的通道数目，最后是输出的通道数目。
而对于每一个输出通道都有一个对应的偏置量。</span></p>
<pre class="cjk"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">W_conv1 = weight_variable([5, 5, 1, 32])</span></code></span></font>
<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">b_conv1 = bias_variable([32])</span></code></span></font></pre><p>
<span style="background: #ffff00">为了用这一层，我们把</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">x</span></code></span></font><span style="background: #ffff00">变成一个<font face="Liberation Serif, serif"><span lang="en-US">4d</span></font>向量，其第<font face="Liberation Serif, serif"><span lang="en-US">2</span></font>、第<font face="Liberation Serif, serif"><span lang="en-US">3</span></font>维对应图片的宽、高，最后一维代表图片的颜色通道数<font face="Liberation Serif, serif"><span lang="en-US">(</span></font>因为是灰度图所以这里的通道数为<font face="Liberation Serif, serif"><span lang="en-US">1</span></font>，如果是<font face="Liberation Serif, serif"><span lang="en-US">rgb</span></font>彩色图，则为<font face="Liberation Serif, serif"><span lang="en-US">3)</span></font>。</span></p>
<pre class="cjk" style="margin-bottom: 0.5cm"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western"><span style="background: #ffff00">x_image = tf.reshape(x, [-1,28,28,1])</span></code></span></font></pre><p>
<font face="Liberation Serif, serif"><span lang="en-US">We then
convolve <code class="western">x_image</code> with the weight tensor,
add the bias, apply the ReLU function, and finally <span style="background: #ffff00">max
pool. </span></span></font><span style="background: #ffff00">我们把</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">x_image</span></code></span></font><span style="background: #ffff00">和权值向量进行卷积，加上偏置项，然后应用<font face="Liberation Serif, serif"><span lang="en-US">ReLU</span></font>激活函数，最后进行<font face="Liberation Serif, serif"><span lang="en-US">max
pooling</span></font>。 </span>
</p>
<pre class="cjk"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</code></span></font>
<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">h_pool1 = max_pool_2x2(h_conv1)</code></span></font></pre><h3>
<a name="第二层卷积-"></a><a name="AUTOGENERATED-second-convolutional-layer"></a>
<span style="background: #ffff00">第二层卷积 </span>
</h3>
<p><span style="background: #ffff00">为了构建一个更深的网络，我们会把几个类似的层堆叠起来。第二层中，每个<font face="Liberation Serif, serif"><span lang="en-US">5x5</span></font>的<font face="Liberation Serif, serif"><span lang="en-US">patch</span></font>会得到<font face="Liberation Serif, serif"><span lang="en-US">64</span></font>个特征。</span></p>
<pre class="cjk"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">W_conv2 = weight_variable([5, 5, 32, 64])</code></span></font>
<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">b_conv2 = bias_variable([64])</code></span></font>

<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</code></span></font>
<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">h_pool2 = max_pool_2x2(h_conv2)</code></span></font></pre><h3>
<a name="密集连接层-"></a><a name="AUTOGENERATED-densely-connected-layer"></a>
<span style="background: #ffff00">密集连接层 </span>
</h3>
<p><span style="background: #ffff00">现在，图片尺寸减小到<font face="Liberation Serif, serif"><span lang="en-US">7x7</span></font>，我们加入一个有<font face="Liberation Serif, serif"><span lang="en-US">1024</span></font>个神经元的全连接层，用于处理整个图片。我们把池化层输出的张量<font face="Liberation Serif, serif"><span lang="en-US">reshape</span></font>成一些向量，乘上权重矩阵，加上偏置，然后对其使用<font face="Liberation Serif, serif"><span lang="en-US">ReLU</span></font>。</span></p>
<pre class="cjk"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">W_fc1 = weight_variable([7 * 7 * 64, 1024])</code></span></font>
<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">b_fc1 = bias_variable([1024])</code></span></font>

<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])</code></span></font>
<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</code></span></font></pre><h4>
<a name="dropout-"></a><a name="AUTOGENERATED-dropout"></a><font face="Liberation Serif, serif"><span lang="en-US"><span style="background: #ffff00">Dropout
</span></span></font>
</h4>
<p><span style="background: #ffff00">为了减少过拟合，我们在输出层之前加入<font face="Liberation Serif, serif"><span lang="en-US">dropout</span></font>。我们用一个</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">placeholder</span></code></span></font><span style="background: #ffff00">来代表一个神经元的输出在<font face="Liberation Serif, serif"><span lang="en-US">dropout</span></font>中保持不变的概率。这样我们可以在训练过程中启用<font face="Liberation Serif, serif"><span lang="en-US">dropout</span></font>，在测试过程中关闭<font face="Liberation Serif, serif"><span lang="en-US">dropout</span></font>。
<font face="Liberation Serif, serif"><span lang="en-US">TensorFlow</span></font>的</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">tf.nn.dropout</span></code></span></font><span style="background: #ffff00">操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的<font face="Liberation Serif, serif"><span lang="en-US">scale</span></font>。所以用<font face="Liberation Serif, serif"><span lang="en-US">dropout</span></font>的时候可以不用考虑<font face="Liberation Serif, serif"><span lang="en-US">scale</span></font>。</span></p>
<pre class="cjk"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">keep_prob = tf.placeholder(&quot;float&quot;)</code></span></font>
<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</code></span></font></pre><h3>
<a name="输出层-"></a><a name="AUTOGENERATED-readout-layer"></a><span style="background: #ffff00">输出层
</span>
</h3>
<p><span style="background: #ffff00">最后，我们添加一个<font face="Liberation Serif, serif"><span lang="en-US">softmax</span></font>层，就像前面的单层<font face="Liberation Serif, serif"><span lang="en-US">softmax
regression</span></font>一样。</span></p>
<pre class="cjk"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">W_fc2 = weight_variable([1024, 10])</code></span></font>
<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">b_fc2 = bias_variable([10])</code></span></font>

<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</code></span></font></pre><h3>
<a name="训练和评估模型-"></a><a name="AUTOGENERATED-train-and-evaluate-the-model"></a>
<span style="background: #ffff00">训练和评估模型 </span>
</h3>
<p><span style="background: #ffff00">这个模型的效果如何呢？</span></p>
<p><span style="background: #ffff00">为了进行训练和评估，我们使用与之前简单的单层<font face="Liberation Serif, serif"><span lang="en-US">SoftMax</span></font>神经网络模型几乎相同的一套代码，只是我们会用更加复杂的<font face="Liberation Serif, serif"><span lang="en-US">ADAM</span></font>优化器来做梯度最速下降，在</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">feed_dict</span></code></span></font><span style="background: #ffff00">中加入额外的参数</span><font face="Liberation Serif, serif"><span lang="en-US"><code class="western"><span style="background: #ffff00">keep_prob</span></code></span></font><span style="background: #ffff00">来控制<font face="Liberation Serif, serif"><span lang="en-US">dropout</span></font>比例。然后每<font face="Liberation Serif, serif"><span lang="en-US">100</span></font>次迭代输出一次日志。</span></p>
<pre class="cjk"><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))</code></span></font>
<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</code></span></font>
<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))</code></span></font>
<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))</code></span></font>
<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">sess.run(tf.initialize_all_variables())</code></span></font>
<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">for i in range(20000):</code></span></font>
<code class="cjk">  </code><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">batch = mnist.train.next_batch(50)</code></span></font>
<code class="cjk">  </code><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">if i%100 == 0:</code></span></font>
<code class="cjk">    </code><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">train_accuracy = accuracy.eval(feed_dict={</code></span></font>
<code class="cjk">        </code><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">x:batch[0], y_: batch[1], keep_prob: 1.0})</code></span></font>
<code class="cjk">    </code><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">print &quot;step %d, training accuracy %g&quot;%(i, train_accuracy)</code></span></font>
<code class="cjk">  </code><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})</code></span></font>

<font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">print &quot;test accuracy %g&quot;%accuracy.eval(feed_dict={</code></span></font>
<code class="cjk">    </code><font face="Liberation Mono, monospace"><span lang="en-US"><code class="western">x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})</code></span></font></pre><p>
<span style="background: #ffff00">以上代码，在最终测试集上的准确率大概是<font face="Liberation Serif, serif"><span lang="en-US">99.2%</span></font>。</span></p>
<p><span style="background: #ffff00">目前为止，我们已经学会了用<font face="Liberation Serif, serif"><span lang="en-US">TensorFlow</span></font>快捷地搭建、训练和评估一个复杂一点儿的深度学习模型。</span></p>
<p><span style="background: #ffff00">原文地址：<font face="Liberation Serif, serif"><span lang="en-US"><a href="http://tensorflow.org/tutorials/mnist/pros/index.html" target="_blank">Deep
MNIST for Experts</a> </span></font>翻译：<font face="Liberation Serif, serif"><span lang="en-US"><a href="https://github.com/chenweican" target="_blank">chenweican</a>
</span></font>校对：<font face="Liberation Serif, serif"><span lang="en-US"><a href="https://github.com/wanghong-yang" target="_blank">HongyangWang</a></span></span></font></p>
</body>
</html>