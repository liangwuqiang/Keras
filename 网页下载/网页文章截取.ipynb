{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从网页中截取文章内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤1：参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.cnblogs.com/neopenx/p/4453161.html\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    'http://www.cnblogs.com/qw12/p/6294430.html'\n",
    "    ,'http://www.cnblogs.com/neopenx/p/4453161.html'\n",
    "]\n",
    "url = urls[1]\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤2：查找相应的截取关键字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "博客园 http://www.cnblogs.com\n"
     ]
    }
   ],
   "source": [
    "if 'blog.jobbole.com' in url:  # 伯乐在线\n",
    "    title_key = '.entry-header'\n",
    "    content_key = '.entry'\n",
    "    print('伯乐在线 http://blog.jobbole.com')\n",
    "elif 'blog.csdn.net' in url:  # csdn\n",
    "    title_key = '.link_title'\n",
    "    content_key = '#article_content'\n",
    "    print('csdn http://blog.csdn.net')\n",
    "elif 'www.codingpy.com' in url:  # 编程派网址\n",
    "    title_key = '.header h1'\n",
    "    content_key = '.article-content'\n",
    "    print('编程派网址 http://www.codingpy.com')\n",
    "elif 'www.cnblogs.com' in url:  # 博客园\n",
    "    title_key = '#cb_post_title_url'\n",
    "    content_key = '#cnblogs_post_body'\n",
    "    print('博客园 http://www.cnblogs.com')\n",
    "else:\n",
    "    title_key = '#keraspython'\n",
    "    content_key = '.section'\n",
    "    print('其它')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤3：发出网页请求，接收响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "<!DOCTYPE html>\r\n",
      "<html lang=\"zh-cn\">\r\n",
      "<head>\r\n",
      "<meta charset=\"utf-8\"/>\r\n",
      "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\r\n",
      "<title>ReLu(Rectified Linear Units)激活函数 - Physcal - 博客园</title>\r\n",
      "<link type=\"text/css\" rel=\"stylesheet\" href=\"/bundles/blog-common.css?v=ChDk9h03-S75WEqNhGvXkWireJ5cCWdK1xRM9NIXfnM1\"/>\n",
      "<link id=\"MainCss\" type=\"text/css\" rel=\"stylesheet\" href=\"/skins/iMetro/bundle-iMetro.css?v=2TeLnpOZe7KA__aXpZGlfXgoc-73meGsojRtEQ8QCJA1\"/>\n",
      "<link type=\"text/css\" rel=\"stylesheet\" href=\"/blog/customcss/199930.css?v=0GV7QRXOw1y0HIAInolzRwfOQy0%3d\"/>\n",
      "<link id=\"mobile-style\" media=\"only screen and (max-width: 768px)\" type=\"text/css\" rel=\"stylesheet\" href=\"/skins/iMetro/bundle-iMetro-mobile.css?v=GdPsEX6D7age8xvCjF--7m7O4Z1PVL_D4zGOS2mXsDg1\"/>\r\n",
      "<link title=\"RSS\" type=\"application/rss+xml\" rel=\"alternate\" href=\"http://www.cnblogs.com/neopenx/rss\"/>\r\n",
      "<link title=\"RSD\" type=\"application/rsd+xml\" rel=\"EditURI\" href=\"http://www.cnblogs.com/neopenx/rsd.xml\"/>\n",
      "<link type=\"application/wlwmanifest+xml\" rel=\"wlwmanifest\" href=\"http://www.cnblogs.com/neopenx/wlwmanifest.xml\"/>\r\n",
      "<script src=\"//common.cnblogs.com/script/jquery.js\" type=\"text/javascript\"></script>  \r\n",
      "<script type=\"text/javascript\">var currentBlogApp = 'neopenx', cb_enable_mathjax=true;var isLogined=false;</script>\r\n",
      "<script src=\"/bundles/blog-common.js?v=zLAewHCaOmEYGOn3iyM1yprOSNEWA2FkU2TsbgxrSSg1\" type=\"text/javascript\"></script>\r\n",
      "</head>\r\n",
      "<body>\r\n",
      "<a name=\"top\"></a>\n",
      "<div id=\"page_begin_html\"></div><script>load_page_begin_html();</script>\r\n",
      "\r\n",
      "<!--done-->\r\n",
      "<div id=\"home\">\r\n",
      "<div id=\"header\">\r\n",
      "\t<div id=\"blogTitle\">\r\n",
      "\t<a id=\"lnkBlogLogo\" href=\"http://www.cnblogs.com/neopenx/\"><img id=\"blogLogo\" src=\"/Skins/custom/images/logo.gif\" alt=\"返回主页\" /></a>\t\t\t\r\n",
      "\t\t\r\n",
      "<!--done-->\r\n",
      "<h1><a id=\"Header1_HeaderTitle\" class=\"headermaintitle\" href=\"http://www.cnblogs.com/neopenx/\">Physcalの大魔導書</a></h1>\r\n",
      "<h2>某HFUT的蒟蒻，ICT/VIPL的直博狗，SeetaTech的码农，还是当大魔导师好了(=￣ω￣=)。</h2>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\t\t\r\n",
      "\t</div><!--end: blogTitle 博客的标题和副标题 -->\r\n",
      "\t<div id=\"navigator\">\r\n",
      "\t\t\r\n",
      "<ul id=\"navList\">\r\n",
      "<li></li>\r\n",
      "<li><a id=\"blog_nav_myhome\" class=\"menu\" href=\"http://www.cnblogs.com/neopenx/\">首页</a></li>\r\n",
      "<li><a id=\"blog_nav_newpost\" class=\"menu\" rel=\"nofollow\" href=\"https://i.cnblogs.com/EditPosts.aspx?opt=1\">新随笔</a></li>\r\n",
      "<li></li>\r\n",
      "<li>\r\n",
      "<!----></li>\r\n",
      "<li><a id=\"blog_nav_admin\" class=\"menu\" rel=\"nofollow\" href=\"https://i.cnblogs.com/\">管理</a></li>\r\n",
      "</ul>\r\n",
      "\t\t<div class=\"blogStats\">\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t</div><!--end: blogStats -->\r\n",
      "\t</div><!--end: navigator 博客导航栏 -->\r\n",
      "</div><!--end: header 头部 -->\r\n",
      "\r\n",
      "<div id=\"main\">\r\n",
      "\t<div id=\"mainContent\">\r\n",
      "\t<div class=\"forFlow\">\r\n",
      "\t\t\r\n",
      "<div id=\"post_detail\">\r\n",
      "<!--done-->\r\n",
      "<div id=\"topics\">\r\n",
      "\t<div class = \"post\">\r\n",
      "\t\t<h1 class = \"postTitle\">\r\n",
      "\t\t\t<a id=\"cb_post_title_url\" class=\"postTitle2\" href=\"http://www.cnblogs.com/neopenx/p/4453161.html\">ReLu(Rectified Linear Units)激活函数</a>\r\n",
      "\t\t</h1>\r\n",
      "\t\t<div class=\"clear\"></div>\r\n",
      "\t\t<div class=\"postBody\">\r\n",
      "\t\t\t<div id=\"cnblogs_post_body\"><h3>论文参考：<a href=\"http://wenku.baidu.com/link?url=GtIz68_egY1kBAFgt3LY0f2v89H6doReu6pcNB34yHiG_PEiSekhZVtSszGB28dtRmEfAKrYOQtu5T0EVLeTzKwpZAeTJEfo00XOgbj9l4u\" target=\"_blank\">Deep Sparse Rectifier Neural Networks</a>&nbsp;(很有趣的一篇paper）</h3>\r\n",
      "<h2>起源：传统激活函数、脑神经元激活频率研究、稀疏激活性</h2>\r\n",
      "<h3>传统Sigmoid系激活函数</h3>\r\n",
      "<p><img src=\"http://images.cnitblog.com/blog2015/678029/201504/241221240623467.png\" alt=\"\" width=\"438\" height=\"329\" /></p>\r\n",
      "<p>传统神经网络中最常用的两个激活函数，Sigmoid系（Logistic-Sigmoid、Tanh-Sigmoid）被视为神经网络的核心所在。</p>\r\n",
      "<p>从数学上来看，非线性的Sigmoid函数对中央区的信号增益较大，对两侧区的信号增益小，在信号的特征空间映射上，有很好的效果。</p>\r\n",
      "<p>从神经科学上来看，中央区酷似神经元的兴奋态，两侧区酷似神经元的抑制态，因而在神经网络学习方面，可以将重点特征推向中央区，将非重点特征推向两侧区。</p>\r\n",
      "<p>无论是哪种解释，看起来都比早期的线性激活函数(y=x),阶跃激活函数(-1/1,0/1)高明了不少。</p>\r\n",
      "<h3>近似生物神经激活函数：Softplus&amp;ReLu&nbsp;</h3>\r\n",
      "<p>2001年，神经科学家Dayan、Abott从生物学角度，模拟出了脑神经元接受信号更精确的激活模型，该模型如左图所示：</p>\r\n",
      "<div><img src=\"file:///C:\\Users\\Administrator\\AppData\\Roaming\\Tencent\\Users\\2876734969\\QQ\\WinTemp\\RichOle\\5{`L6GE1KSSOWDQ`GUE3183.png\" alt=\"\" /></div>\r\n",
      "<div><img src=\"file:///C:\\Users\\Administrator\\AppData\\Roaming\\Tencent\\Users\\2876734969\\QQ\\WinTemp\\RichOle\\5{`L6GE1KSSOWDQ`GUE3183.png\" alt=\"\" /></div>\r\n",
      "<p><img src=\"http://images.cnitblog.com/blog2015/678029/201504/241258145158893.png\" alt=\"\" width=\"414\" height=\"308\" /><img src=\"http://images.cnitblog.com/blog2015/678029/201504/241900156879853.png\" alt=\"\" width=\"391\" height=\"299\" /></p>\r\n",
      "<p><span style=\"font-size: 14px;\">这个模型对比Sigmoid系主要变化有三点：<strong>①单侧抑制 ②相对宽阔的兴奋边界 ③稀疏激活性</strong>（重点，可以看到红框里前端状态完全没有激活）</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">同年，Charles Dugas等人在做<strong>正数</strong>回归预测<a href=\"http://95.173.210.46/url?q=http://papers.nips.cc/paper/1920-incorporating-second-order-functional-knowledge-for-better-option-pricing.pdf&amp;sa=U&amp;ei=4SM6VefqBIbaPL-HgPAN&amp;ved=0CAUQFjAA&amp;usg=AFQjCNHY_71xurXxTe9sgH1iHqjZHwg2rA\" target=\"_blank\">论文</a>中偶然使用了Softplus函数，Softplus函数是Logistic-Sigmoid函数原函数。</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">&nbsp;$Softplus(x)=log(1+e^{x})$</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">按照论文的说法，一开始想要使用一个指数函数（天然正数）作为激活函数来回归，但是到后期梯度实在太大，难以训练，于是加了一个log来减缓上升趋势。</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">加了1是为了保证非负性。同年，Charles Dugas等人在NIPS会议<a href=\"http://papers.nips.cc/paper/2062-estimating-car-insurance-premia-a-case-study-in-high-dimensional-data-inference\" target=\"_blank\">论文</a>中又调侃了一句，Softplus可以看作是强制非负校正函数$\\max(0,x)$平滑版本。</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">偶然的是，同是2001年，ML领域的Softplus/Rectifier激活函数与神经科学领域的提出脑神经元激活频率函数有神似的地方，这促成了新的激活函数的研究。</span></p>\r\n",
      "<h3>生物神经的稀疏激活性</h3>\r\n",
      "<p><span style=\"font-size: 14px;\">在神经科学方面，除了新的激活频率函数之外，神经科学家还发现了神经元的稀疏激活性。</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">还是2001年，Attwell等人基于大脑能量消耗的观察学习上，推测神经元编码工作方式具有稀疏性和分布性。</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">2003年Lennie等人估测大脑同时被激活的神经元只有1~4%，进一步表明神经元工作的稀疏性。</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">从这个角度来看，在经验规则的初始化W之后，传统的Sigmoid系函数同时近乎有一半的神经元被激活，这不符合神经科学的研究，而且会给深度网络训练带来巨大问题。</span></p>\r\n",
      "<p>Softplus照顾到了新模型的前两点，却没有稀疏激活性。因而，校正函数$\\max(0,x)$成了近似符合该模型的最大赢家。</p>\r\n",
      "<p>&nbsp;</p>\r\n",
      "<h2>Part I：关于稀疏性的观点</h2>\r\n",
      "<p><span style=\"font-size: 14px;\">Machine Learning中的颠覆性研究是稀疏特征，基于数据的稀疏特征研究上，派生了Deep Learning这一分支。</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">稀疏性概念最早由Olshausen、Field在1997年对信号数据稀疏编码的研究中引入，并最早在卷积神经网络中得以大施拳脚。<br /></span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">近年来，稀疏性研究不仅在计算神经科学、机器学习领域活跃，甚至信号处理、统计学也在借鉴。</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">总结起来稀疏性大概有以下三方面的贡献：</span></p>\r\n",
      "<h3><strong>1.1 信息解离</strong></h3>\r\n",
      "<p><span style=\"font-size: 14px;\">当前，深度学习一个明确的目标是从数据变量中解离出关键因子。原始数据（以自然数据为主）中通常缠绕着高度密集的特征。原因</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">是这些特征向量是相互关联的，一个小小的关键因子可能牵扰着一堆特征，有点像蝴蝶效应，牵一发而动全身。</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">基于数学原理的传统机器学习手段在解离这些关联特征方面具有致命弱点。</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">然而，如果能够解开特征间缠绕的复杂关系，转换为稀疏特征，那么特征就有了鲁棒性（去掉了无关的噪声）。</span></p>\r\n",
      "<h3><strong>1.2 线性可分性</strong></h3>\r\n",
      "<p><span style=\"font-size: 14px;\">稀疏特征有更大可能线性可分，或者对非线性映射机制有更小的依赖。因为稀疏特征处于高维的特征空间上（被自动映射了）</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">从流形学习观点来看（参见降噪自动编码器），稀疏特征被移到了一个较为纯净的低维流形面上。</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">线性可分性亦可参照天然稀疏的文本型数据，即便没有隐层结构，仍然可以被分离的很好。</span></p>\r\n",
      "<h3><strong>1.3 稠密分布但是稀疏</strong></h3>\r\n",
      "<p><span style=\"font-size: 14px;\">稠密缠绕分布着的特征是信息最富集的特征，从潜在性角度，往往比局部少数点携带的特征成倍的有效。</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">而稀疏特征，正是从稠密缠绕区解离出来的，潜在价值巨大。</span></p>\r\n",
      "<h3>1.4 稀疏性激活函数的贡献的作用：</h3>\r\n",
      "<p>不同的输入可能包含着大小不同关键特征，使用大小可变的数据结构去做容器，则更加灵活。</p>\r\n",
      "<p>假如神经元激活具有稀疏性，那么不同激活路径上：不同数量（选择性不激活）、不同功能（分布式激活），</p>\r\n",
      "<p>两种可优化的结构生成的激活路径，可以更好地从有效的数据的维度上，学习到相对稀疏的特征，起到自动化解离效果。</p>\r\n",
      "<p>&nbsp;</p>\r\n",
      "<h2>Part II：基于稀疏性的校正激活函数</h2>\r\n",
      "<h3>2.1 非饱和线性端</h3>\r\n",
      "<p>撇开稀疏激活不谈，校正激活函数$\\max(0,x)$，与Softplus函数在兴奋端的差异较大(线性和非线性)。</p>\r\n",
      "<p>几十年的机器学习发展中，我们形成了这样一个概念：非线性激活函数要比线性激活函数更加先进。</p>\r\n",
      "<p>尤其是在布满Sigmoid函数的BP神经网络，布满径向基函数的SVM神经网络中，往往有这样的幻觉，非线性函数对非线性网络贡献巨大。</p>\r\n",
      "<p>该幻觉在SVM中更加严重。核函数的形式并非完全是SVM能够处理非线性数据的主力功臣（支持向量充当着隐层角色）。</p>\r\n",
      "<p>那么在深度网络中，对非线性的依赖程度就可以缩一缩。另外，在上一部分提到，稀疏特征并不需要网络具有很强的处理线性不可分机制。</p>\r\n",
      "<p>综合以上两点，在深度学习模型中，使用简单、速度快的线性激活函数可能更为合适。</p>\r\n",
      "<p><img src=\"http://images.cnitblog.com/blog2015/678029/201504/250236342652777.png\" alt=\"\" width=\"356\" height=\"271\" /></p>\r\n",
      "<p>如图，一旦神经元与神经元之间改为线性激活，网络的非线性部分仅仅来自于神经元部分选择性激活。</p>\r\n",
      "<h3>2.2 Vanishing Gradient Problem</h3>\r\n",
      "<p>更倾向于使用线性神经激活函数的另外一个原因是，减轻梯度法训练深度网络时的Vanishing Gradient Problem。</p>\r\n",
      "<p>看过BP推导的人都知道，误差从输出层反向传播算梯度时，在各层都要乘当前层的输入神经元值，激活函数的一阶导数。</p>\r\n",
      "<p>即$Grad=Error\\cdot Sigmoid'(x)\\cdot x$。使用双端饱和(即值域被限制)Sigmoid系函数会有两个问题：</p>\r\n",
      "<p>①Sigmoid'(x)&isin;(0,1) &nbsp;导数缩放</p>\r\n",
      "<p>②x&isin;(0,1)或x&isin;(-1,1) &nbsp;饱和值缩放</p>\r\n",
      "<p>这样，经过每一层时，Error都是成倍的衰减，一旦进行递推式的多层的反向传播，梯度就会不停的衰减，消失，使得网络学习变慢。</p>\r\n",
      "<p>而校正激活函数的梯度是1，且只有一端饱和，梯度很好的在反向传播中流动，训练速度得到了很大的提高。</p>\r\n",
      "<p>Softplus函数则稍微慢点，Softplus'(x)=Sigmoid(x)&isin;(0,1) ，但是也是单端饱和，因而速度仍然会比Sigmoid系函数快。</p>\r\n",
      "<p>&nbsp;</p>\r\n",
      "<h2>Part III 潜在问题</h2>\r\n",
      "<h3>强制引入稀疏零的合理性？</h3>\r\n",
      "<p>诚然，稀疏性有很多优势。但是，过分的强制稀疏处理，会减少模型的有效容量。即特征屏蔽太多，导致模型无法学习到有效特征。</p>\r\n",
      "<p>论文中对稀疏性的引入度做了实验，理想稀疏性（强制置0）比率是70%~85%。超过85%，网络就容量就成了问题，导致错误率极高。</p>\r\n",
      "<p><img src=\"http://images.cnitblog.com/blog2015/678029/201504/280013037554268.png\" alt=\"\" width=\"472\" height=\"359\" /></p>\r\n",
      "<p>&nbsp;</p>\r\n",
      "<p>对比大脑工作的95%稀疏性来看，现有的计算神经网络和生物神经网络还是有很大差距的。</p>\r\n",
      "<p>庆幸的是，ReLu只有负值才会被稀疏掉，即引入的稀疏性是可以训练调节的，是动态变化的。</p>\r\n",
      "<p>只要进行梯度训练，网络可以向误差减少的方向，自动调控稀疏比率，保证激活链上存在着合理数量的非零值。</p>\r\n",
      "<p>&nbsp;</p>\r\n",
      "<h2><strong>Part IV ReLu的贡献</strong></h2>\r\n",
      "<h3><strong>4.1 缩小做和不做非监督预训练的代沟</strong></h3>\r\n",
      "<p><span style=\"font-size: 14px;\">ReLu的使用，使得网络可以自行引入稀疏性。这一做法，等效于无监督学习的预训练。</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\"><img src=\"http://images.cnitblog.com/blog2015/678029/201504/280055431153635.png\" alt=\"\" width=\"523\" height=\"242\" /></span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">当然，效果肯定没预训练好。论文中给出的数据显示，没做预训练情况下，ReLu激活网络遥遥领先其它激活函数。</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">甚至出现了比普通激活函数预训练后更好的奇葩情况。</span><span style=\"font-size: 14px;\">当然，在预训练后，ReLu仍然有提升空间。</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">从这一层面来说，ReLu缩小了非监督学习和监督学习之间的代沟。当然，还有更快的训练速度。</span></p>\r\n",
      "<h3><strong>4.2 更快的特征学习</strong></h3>\r\n",
      "<p><span style=\"font-size: 14px;\">在MNIST+LeNet4中，ReLu+Tanh的组合在epoch 50左右就能把验证集错误率降到1.05%</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">但是，全Tanh在epoch 150时，还是1.37%，这个结果ReLu+Tanh在epoch 17时就能达到了。</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\"><img src=\"http://images.cnitblog.com/blog2015/678029/201504/280120355838911.png\" alt=\"\" width=\"415\" height=\"336\" /></span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">该图来自AlexNet的<a href=\"http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf\" target=\"_blank\">论文</a>对ReLu和普通Sigmoid系函数做的对比测试，可以看到，ReLu的使用，使得学习周期</span></p>\r\n",
      "<p><span style=\"font-size: 14px;\">大大缩短。综合速率和效率，DL中大部分激活函数应该选择ReLu。</span></p>\r\n",
      "<p>&nbsp;</p>\r\n",
      "<h2>Part V &nbsp;Theano中ReLu的实现</h2>\r\n",
      "<p>ReLu可以直接用T.maximum(0,x)实现，用T.max(0,x)不能求导.</p>\r\n",
      "<p>&nbsp;</p>\r\n",
      "<h2>Part VI &nbsp;ReLu训练技巧</h2>\r\n",
      "<p>见<a href=\"http://www.cnblogs.com/neopenx/p/4480701.html\" target=\"_blank\">Cifar-10训练技巧</a></p></div><div id=\"MySignature\"></div>\r\n",
      "<div class=\"clear\"></div>\r\n",
      "<div id=\"blog_post_info_block\">\r\n",
      "<div id=\"BlogPostCategory\"></div>\r\n",
      "<div id=\"EntryTag\"></div>\r\n",
      "<div id=\"blog_post_info\">\r\n",
      "</div>\r\n",
      "<div class=\"clear\"></div>\r\n",
      "<div id=\"post_next_prev\"></div>\r\n",
      "</div>\r\n",
      "\r\n",
      "\r\n",
      "\t\t</div>\r\n",
      "\t\t<div class = \"postDesc\">posted @ <span id=\"post-date\">2015-04-24 12:57</span> <a href='http://www.cnblogs.com/neopenx/'>Physcal</a> 阅读(<span id=\"post_view_count\">...</span>) 评论(<span id=\"post_comment_count\">...</span>)  <a href =\"https://i.cnblogs.com/EditPosts.aspx?postid=4453161\" rel=\"nofollow\">编辑</a> <a href=\"#\" onclick=\"AddToWz(4453161);return false;\">收藏</a></div>\r\n",
      "\t</div>\r\n",
      "\t<script type=\"text/javascript\">var allowComments=true,cb_blogId=199930,cb_entryId=4453161,cb_blogApp=currentBlogApp,cb_blogUserGuid='c9576319-0a4a-e411-b908-9dcfd8948a71',cb_entryCreatedDate='2015/4/24 12:57:00';loadViewCount(cb_entryId);</script>\r\n",
      "\t\r\n",
      "</div><!--end: topics 文章、评论容器-->\r\n",
      "</div><a name=\"!comments\"></a><div id=\"blog-comments-placeholder\"></div><script type=\"text/javascript\">var commentManager = new blogCommentManager();commentManager.renderComments(0);</script>\r\n",
      "<div id='comment_form' class='commentform'>\r\n",
      "<a name='commentform'></a>\r\n",
      "<div id='divCommentShow'></div>\r\n",
      "<div id='comment_nav'><span id='span_refresh_tips'></span><a href='javascript:void(0);' onclick='return RefreshCommentList();' id='lnk_RefreshComments' runat='server' clientidmode='Static'>刷新评论</a><a href='#' onclick='return RefreshPage();'>刷新页面</a><a href='#top'>返回顶部</a></div>\r\n",
      "<div id='comment_form_container'></div>\r\n",
      "<div class='ad_text_commentbox' id='ad_text_under_commentbox'></div>\r\n",
      "<div id='ad_t2'></div>\r\n",
      "<div id='opt_under_post'></div>\r\n",
      "<div id='cnblogs_c1' class='c_ad_block'></div>\r\n",
      "<div id='under_post_news'></div>\r\n",
      "<div id='cnblogs_c2' class='c_ad_block'></div>\r\n",
      "<div id='under_post_kb'></div>\r\n",
      "<div id='HistoryToday' class='c_ad_block'></div>\r\n",
      "<script type='text/javascript'>\r\n",
      "    fixPostBody();\r\n",
      "    setTimeout(function () { incrementViewCount(cb_entryId); }, 50);\r\n",
      "    deliverAdT2();\r\n",
      "    deliverAdC1();\r\n",
      "    deliverAdC2();    \r\n",
      "    loadNewsAndKb();\r\n",
      "    loadBlogSignature();\r\n",
      "    LoadPostInfoBlock(cb_blogId, cb_entryId, cb_blogApp, cb_blogUserGuid);\r\n",
      "    GetPrevNextPost(cb_entryId, cb_blogId, cb_entryCreatedDate);\r\n",
      "    loadOptUnderPost();\r\n",
      "    GetHistoryToday(cb_blogId, cb_blogApp, cb_entryCreatedDate);   \r\n",
      "</script>\r\n",
      "</div>\r\n",
      "\r\n",
      "\r\n",
      "\t</div><!--end: forFlow -->\r\n",
      "\t</div><!--end: mainContent 主体内容容器-->\r\n",
      "\r\n",
      "\t<div id=\"sideBar\">\r\n",
      "\t\t<div id=\"sideBarMain\">\r\n",
      "\t\t\t\r\n",
      "<!--done-->\r\n",
      "<div class=\"newsItem\">\r\n",
      "<h3 class=\"catListTitle\">公告</h3>\r\n",
      "\t<div id=\"blog-news\"></div><script type=\"text/javascript\">loadBlogNews();</script>\r\n",
      "</div>\r\n",
      "\r\n",
      "\t\t\t<div id=\"blog-calendar\" style=\"display:none\"></div><script type=\"text/javascript\">loadBlogDefaultCalendar();</script>\r\n",
      "\t\t\t\r\n",
      "\t\t\t<div id=\"leftcontentcontainer\">\r\n",
      "\t\t\t\t<div id=\"blog-sidecolumn\"></div><script type=\"text/javascript\">loadBlogSideColumn();</script>\r\n",
      "\t\t\t</div>\r\n",
      "\t\t\t\r\n",
      "\t\t</div><!--end: sideBarMain -->\r\n",
      "\t</div><!--end: sideBar 侧边栏容器 -->\r\n",
      "\t<div class=\"clear\"></div>\r\n",
      "\t</div><!--end: main -->\r\n",
      "\t<div class=\"clear\"></div>\r\n",
      "\t<div id=\"footer\">\r\n",
      "\t\t\r\n",
      "<!--done-->\r\n",
      "Copyright &copy;2017 Physcal\r\n",
      "\t</div><!--end: footer -->\r\n",
      "</div><!--end: home 自定义的最大容器 -->\r\n",
      "</body>\r\n",
      "</html>\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "req = request.Request(url)\n",
    "res = request.urlopen(req)\n",
    "html = res.read().decode('utf-8')\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤4：从网页中提取文章标题和内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文章标题： ReLu(Rectified Linear Units)激活函数\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "title = soup.select(title_key)[0].text.strip()  # 文章标题\n",
    "print('文章标题：', title)\n",
    "content = soup.select(content_key)[0]  # 文章内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤5：下载文章中的图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 哈希码生成函数，用于给图片重新命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def md5(name):\n",
    "    \"\"\" 将字符串转成哈希码 \"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        name = str(name)\n",
    "    md5 = hashlib.md5()\n",
    "    md5.update(name.encode('utf-8'))\n",
    "    return md5.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 下载图片，并修改文章图片的超链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下载完成 images/d6a14abc8b3b7b79dec5e77ae3f44026.png\n",
      "图片出错 <urlopen error [Errno 2] No such file or directory: '/C:\\\\Users\\\\Administrator\\\\AppData\\\\Roaming\\\\Tencent\\\\Users\\\\2876734969\\\\QQ\\\\WinTemp\\\\RichOle\\\\5{`L6GE1KSSOWDQ`GUE3183.png'>\n",
      "图片出错 <urlopen error [Errno 2] No such file or directory: '/C:\\\\Users\\\\Administrator\\\\AppData\\\\Roaming\\\\Tencent\\\\Users\\\\2876734969\\\\QQ\\\\WinTemp\\\\RichOle\\\\5{`L6GE1KSSOWDQ`GUE3183.png'>\n",
      "下载完成 images/fceb82b99c19947b59c37cd0cc41c4de.png\n",
      "下载完成 images/a3e1d58e1b709a03dcf55d784c0784ea.png\n",
      "下载完成 images/2bb487cf0996227967d675cccbd398e7.png\n",
      "下载完成 images/9c47fbf71b3d04638a9de2348fc23439.png\n",
      "下载完成 images/bb9aae5c1ed830733990036685344cc9.png\n",
      "下载完成 images/4f41f81fdba33863cda1688406035d3b.png\n",
      "已完成--------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "content = str(content)\n",
    "pattern = '<img .*?src=\\\"(.*?)\\\"'\n",
    "re_image = re.compile(pattern)\n",
    "for image_link in re_image.findall(content):\n",
    "    if not os.path.exists('images'):\n",
    "        os.mkdir('images')\n",
    "    filename = 'images/' + md5(image_link) + os.path.splitext(image_link)[-1]\n",
    "    try:\n",
    "        request.urlretrieve(image_link, filename)\n",
    "        print('下载完成', filename)\n",
    "    except Exception as e:\n",
    "        print('图片出错', e)\n",
    "    else:\n",
    "        content = content.replace(image_link, filename)\n",
    "print('已完成--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤6：将截取的文章标题和内容重新组合成新的网页文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html><head><meta charset=\"UTF-8\">\n",
      "</head><body>\n",
      "<p><a href=\"http://www.cnblogs.com/neopenx/p/4453161.html\">原文链接</a></p>\n",
      "<p><center><h1>ReLu(Rectified Linear Units)激活函数</h1></center></p>\n",
      "    <div id=\"cnblogs_post_body\"><h3>论文参考：<a href=\"http://wenku.baidu.com/link?url=GtIz68_egY1kBAFgt3LY0f2v89H6doReu6pcNB34yHiG_PEiSekhZVtSszGB28dtRmEfAKrYOQtu5T0EVLeTzKwpZAeTJEfo00XOgbj9l4u\" target=\"_blank\">Deep Sparse Rectifier Neural Networks</a> (很有趣的一篇paper）</h3>\n",
      "<h2>起源：传统激活函数、脑神经元激活频率研究、稀疏激活性</h2>\n",
      "<h3>传统Sigmoid系激活函数</h3>\n",
      "<p><img alt=\"\" height=\"329\" src=\"images/d6a14abc8b3b7b79dec5e77ae3f44026.png\" width=\"438\"/></p>\n",
      "<p>传统神经网络中最常用的两个激活函数，Sigmoid系（Logistic-Sigmoid、Tanh-Sigmoid）被视为神经网络的核心所在。</p>\n",
      "<p>从数学上来看，非线性的Sigmoid函数对中央区的信号增益较大，对两侧区的信号增益小，在信号的特征空间映射上，有很好的效果。</p>\n",
      "<p>从神经科学上来看，中央区酷似神经元的兴奋态，两侧区酷似神经元的抑制态，因而在神经网络学习方面，可以将重点特征推向中央区，将非重点特征推向两侧区。</p>\n",
      "<p>无论是哪种解释，看起来都比早期的线性激活函数(y=x),阶跃激活函数(-1/1,0/1)高明了不少。</p>\n",
      "<h3>近似生物神经激活函数：Softplus&amp;ReLu </h3>\n",
      "<p>2001年，神经科学家Dayan、Abott从生物学角度，模拟出了脑神经元接受信号更精确的激活模型，该模型如左图所示：</p>\n",
      "<div><img alt=\"\" src=\"file:///C:\\Users\\Administrator\\AppData\\Roaming\\Tencent\\Users\\2876734969\\QQ\\WinTemp\\RichOle\\5{`L6GE1KSSOWDQ`GUE3183.png\"/></div>\n",
      "<div><img alt=\"\" src=\"file:///C:\\Users\\Administrator\\AppData\\Roaming\\Tencent\\Users\\2876734969\\QQ\\WinTemp\\RichOle\\5{`L6GE1KSSOWDQ`GUE3183.png\"/></div>\n",
      "<p><img alt=\"\" height=\"308\" src=\"images/fceb82b99c19947b59c37cd0cc41c4de.png\" width=\"414\"/><img alt=\"\" height=\"299\" src=\"images/a3e1d58e1b709a03dcf55d784c0784ea.png\" width=\"391\"/></p>\n",
      "<p><span style=\"font-size: 14px;\">这个模型对比Sigmoid系主要变化有三点：<strong>①单侧抑制 ②相对宽阔的兴奋边界 ③稀疏激活性</strong>（重点，可以看到红框里前端状态完全没有激活）</span></p>\n",
      "<p><span style=\"font-size: 14px;\">同年，Charles Dugas等人在做<strong>正数</strong>回归预测<a href=\"http://95.173.210.46/url?q=http://papers.nips.cc/paper/1920-incorporating-second-order-functional-knowledge-for-better-option-pricing.pdf&amp;sa=U&amp;ei=4SM6VefqBIbaPL-HgPAN&amp;ved=0CAUQFjAA&amp;usg=AFQjCNHY_71xurXxTe9sgH1iHqjZHwg2rA\" target=\"_blank\">论文</a>中偶然使用了Softplus函数，Softplus函数是Logistic-Sigmoid函数原函数。</span></p>\n",
      "<p><span style=\"font-size: 14px;\"> $Softplus(x)=log(1+e^{x})$</span></p>\n",
      "<p><span style=\"font-size: 14px;\">按照论文的说法，一开始想要使用一个指数函数（天然正数）作为激活函数来回归，但是到后期梯度实在太大，难以训练，于是加了一个log来减缓上升趋势。</span></p>\n",
      "<p><span style=\"font-size: 14px;\">加了1是为了保证非负性。同年，Charles Dugas等人在NIPS会议<a href=\"http://papers.nips.cc/paper/2062-estimating-car-insurance-premia-a-case-study-in-high-dimensional-data-inference\" target=\"_blank\">论文</a>中又调侃了一句，Softplus可以看作是强制非负校正函数$\\max(0,x)$平滑版本。</span></p>\n",
      "<p><span style=\"font-size: 14px;\">偶然的是，同是2001年，ML领域的Softplus/Rectifier激活函数与神经科学领域的提出脑神经元激活频率函数有神似的地方，这促成了新的激活函数的研究。</span></p>\n",
      "<h3>生物神经的稀疏激活性</h3>\n",
      "<p><span style=\"font-size: 14px;\">在神经科学方面，除了新的激活频率函数之外，神经科学家还发现了神经元的稀疏激活性。</span></p>\n",
      "<p><span style=\"font-size: 14px;\">还是2001年，Attwell等人基于大脑能量消耗的观察学习上，推测神经元编码工作方式具有稀疏性和分布性。</span></p>\n",
      "<p><span style=\"font-size: 14px;\">2003年Lennie等人估测大脑同时被激活的神经元只有1~4%，进一步表明神经元工作的稀疏性。</span></p>\n",
      "<p><span style=\"font-size: 14px;\">从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。</span></p>\n",
      "<p><span style=\"font-size: 14px;\">从这个角度来看，在经验规则的初始化W之后，传统的Sigmoid系函数同时近乎有一半的神经元被激活，这不符合神经科学的研究，而且会给深度网络训练带来巨大问题。</span></p>\n",
      "<p>Softplus照顾到了新模型的前两点，却没有稀疏激活性。因而，校正函数$\\max(0,x)$成了近似符合该模型的最大赢家。</p>\n",
      "<p> </p>\n",
      "<h2>Part I：关于稀疏性的观点</h2>\n",
      "<p><span style=\"font-size: 14px;\">Machine Learning中的颠覆性研究是稀疏特征，基于数据的稀疏特征研究上，派生了Deep Learning这一分支。</span></p>\n",
      "<p><span style=\"font-size: 14px;\">稀疏性概念最早由Olshausen、Field在1997年对信号数据稀疏编码的研究中引入，并最早在卷积神经网络中得以大施拳脚。<br/></span></p>\n",
      "<p><span style=\"font-size: 14px;\">近年来，稀疏性研究不仅在计算神经科学、机器学习领域活跃，甚至信号处理、统计学也在借鉴。</span></p>\n",
      "<p><span style=\"font-size: 14px;\">总结起来稀疏性大概有以下三方面的贡献：</span></p>\n",
      "<h3><strong>1.1 信息解离</strong></h3>\n",
      "<p><span style=\"font-size: 14px;\">当前，深度学习一个明确的目标是从数据变量中解离出关键因子。原始数据（以自然数据为主）中通常缠绕着高度密集的特征。原因</span></p>\n",
      "<p><span style=\"font-size: 14px;\">是这些特征向量是相互关联的，一个小小的关键因子可能牵扰着一堆特征，有点像蝴蝶效应，牵一发而动全身。</span></p>\n",
      "<p><span style=\"font-size: 14px;\">基于数学原理的传统机器学习手段在解离这些关联特征方面具有致命弱点。</span></p>\n",
      "<p><span style=\"font-size: 14px;\">然而，如果能够解开特征间缠绕的复杂关系，转换为稀疏特征，那么特征就有了鲁棒性（去掉了无关的噪声）。</span></p>\n",
      "<h3><strong>1.2 线性可分性</strong></h3>\n",
      "<p><span style=\"font-size: 14px;\">稀疏特征有更大可能线性可分，或者对非线性映射机制有更小的依赖。因为稀疏特征处于高维的特征空间上（被自动映射了）</span></p>\n",
      "<p><span style=\"font-size: 14px;\">从流形学习观点来看（参见降噪自动编码器），稀疏特征被移到了一个较为纯净的低维流形面上。</span></p>\n",
      "<p><span style=\"font-size: 14px;\">线性可分性亦可参照天然稀疏的文本型数据，即便没有隐层结构，仍然可以被分离的很好。</span></p>\n",
      "<h3><strong>1.3 稠密分布但是稀疏</strong></h3>\n",
      "<p><span style=\"font-size: 14px;\">稠密缠绕分布着的特征是信息最富集的特征，从潜在性角度，往往比局部少数点携带的特征成倍的有效。</span></p>\n",
      "<p><span style=\"font-size: 14px;\">而稀疏特征，正是从稠密缠绕区解离出来的，潜在价值巨大。</span></p>\n",
      "<h3>1.4 稀疏性激活函数的贡献的作用：</h3>\n",
      "<p>不同的输入可能包含着大小不同关键特征，使用大小可变的数据结构去做容器，则更加灵活。</p>\n",
      "<p>假如神经元激活具有稀疏性，那么不同激活路径上：不同数量（选择性不激活）、不同功能（分布式激活），</p>\n",
      "<p>两种可优化的结构生成的激活路径，可以更好地从有效的数据的维度上，学习到相对稀疏的特征，起到自动化解离效果。</p>\n",
      "<p> </p>\n",
      "<h2>Part II：基于稀疏性的校正激活函数</h2>\n",
      "<h3>2.1 非饱和线性端</h3>\n",
      "<p>撇开稀疏激活不谈，校正激活函数$\\max(0,x)$，与Softplus函数在兴奋端的差异较大(线性和非线性)。</p>\n",
      "<p>几十年的机器学习发展中，我们形成了这样一个概念：非线性激活函数要比线性激活函数更加先进。</p>\n",
      "<p>尤其是在布满Sigmoid函数的BP神经网络，布满径向基函数的SVM神经网络中，往往有这样的幻觉，非线性函数对非线性网络贡献巨大。</p>\n",
      "<p>该幻觉在SVM中更加严重。核函数的形式并非完全是SVM能够处理非线性数据的主力功臣（支持向量充当着隐层角色）。</p>\n",
      "<p>那么在深度网络中，对非线性的依赖程度就可以缩一缩。另外，在上一部分提到，稀疏特征并不需要网络具有很强的处理线性不可分机制。</p>\n",
      "<p>综合以上两点，在深度学习模型中，使用简单、速度快的线性激活函数可能更为合适。</p>\n",
      "<p><img alt=\"\" height=\"271\" src=\"images/2bb487cf0996227967d675cccbd398e7.png\" width=\"356\"/></p>\n",
      "<p>如图，一旦神经元与神经元之间改为线性激活，网络的非线性部分仅仅来自于神经元部分选择性激活。</p>\n",
      "<h3>2.2 Vanishing Gradient Problem</h3>\n",
      "<p>更倾向于使用线性神经激活函数的另外一个原因是，减轻梯度法训练深度网络时的Vanishing Gradient Problem。</p>\n",
      "<p>看过BP推导的人都知道，误差从输出层反向传播算梯度时，在各层都要乘当前层的输入神经元值，激活函数的一阶导数。</p>\n",
      "<p>即$Grad=Error\\cdot Sigmoid'(x)\\cdot x$。使用双端饱和(即值域被限制)Sigmoid系函数会有两个问题：</p>\n",
      "<p>①Sigmoid'(x)∈(0,1)  导数缩放</p>\n",
      "<p>②x∈(0,1)或x∈(-1,1)  饱和值缩放</p>\n",
      "<p>这样，经过每一层时，Error都是成倍的衰减，一旦进行递推式的多层的反向传播，梯度就会不停的衰减，消失，使得网络学习变慢。</p>\n",
      "<p>而校正激活函数的梯度是1，且只有一端饱和，梯度很好的在反向传播中流动，训练速度得到了很大的提高。</p>\n",
      "<p>Softplus函数则稍微慢点，Softplus'(x)=Sigmoid(x)∈(0,1) ，但是也是单端饱和，因而速度仍然会比Sigmoid系函数快。</p>\n",
      "<p> </p>\n",
      "<h2>Part III 潜在问题</h2>\n",
      "<h3>强制引入稀疏零的合理性？</h3>\n",
      "<p>诚然，稀疏性有很多优势。但是，过分的强制稀疏处理，会减少模型的有效容量。即特征屏蔽太多，导致模型无法学习到有效特征。</p>\n",
      "<p>论文中对稀疏性的引入度做了实验，理想稀疏性（强制置0）比率是70%~85%。超过85%，网络就容量就成了问题，导致错误率极高。</p>\n",
      "<p><img alt=\"\" height=\"359\" src=\"images/9c47fbf71b3d04638a9de2348fc23439.png\" width=\"472\"/></p>\n",
      "<p> </p>\n",
      "<p>对比大脑工作的95%稀疏性来看，现有的计算神经网络和生物神经网络还是有很大差距的。</p>\n",
      "<p>庆幸的是，ReLu只有负值才会被稀疏掉，即引入的稀疏性是可以训练调节的，是动态变化的。</p>\n",
      "<p>只要进行梯度训练，网络可以向误差减少的方向，自动调控稀疏比率，保证激活链上存在着合理数量的非零值。</p>\n",
      "<p> </p>\n",
      "<h2><strong>Part IV ReLu的贡献</strong></h2>\n",
      "<h3><strong>4.1 缩小做和不做非监督预训练的代沟</strong></h3>\n",
      "<p><span style=\"font-size: 14px;\">ReLu的使用，使得网络可以自行引入稀疏性。这一做法，等效于无监督学习的预训练。</span></p>\n",
      "<p><span style=\"font-size: 14px;\"><img alt=\"\" height=\"242\" src=\"images/bb9aae5c1ed830733990036685344cc9.png\" width=\"523\"/></span></p>\n",
      "<p><span style=\"font-size: 14px;\">当然，效果肯定没预训练好。论文中给出的数据显示，没做预训练情况下，ReLu激活网络遥遥领先其它激活函数。</span></p>\n",
      "<p><span style=\"font-size: 14px;\">甚至出现了比普通激活函数预训练后更好的奇葩情况。</span><span style=\"font-size: 14px;\">当然，在预训练后，ReLu仍然有提升空间。</span></p>\n",
      "<p><span style=\"font-size: 14px;\">从这一层面来说，ReLu缩小了非监督学习和监督学习之间的代沟。当然，还有更快的训练速度。</span></p>\n",
      "<h3><strong>4.2 更快的特征学习</strong></h3>\n",
      "<p><span style=\"font-size: 14px;\">在MNIST+LeNet4中，ReLu+Tanh的组合在epoch 50左右就能把验证集错误率降到1.05%</span></p>\n",
      "<p><span style=\"font-size: 14px;\">但是，全Tanh在epoch 150时，还是1.37%，这个结果ReLu+Tanh在epoch 17时就能达到了。</span></p>\n",
      "<p><span style=\"font-size: 14px;\"><img alt=\"\" height=\"336\" src=\"images/4f41f81fdba33863cda1688406035d3b.png\" width=\"415\"/></span></p>\n",
      "<p><span style=\"font-size: 14px;\">该图来自AlexNet的<a href=\"http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf\" target=\"_blank\">论文</a>对ReLu和普通Sigmoid系函数做的对比测试，可以看到，ReLu的使用，使得学习周期</span></p>\n",
      "<p><span style=\"font-size: 14px;\">大大缩短。综合速率和效率，DL中大部分激活函数应该选择ReLu。</span></p>\n",
      "<p> </p>\n",
      "<h2>Part V  Theano中ReLu的实现</h2>\n",
      "<p>ReLu可以直接用T.maximum(0,x)实现，用T.max(0,x)不能求导.</p>\n",
      "<p> </p>\n",
      "<h2>Part VI  ReLu训练技巧</h2>\n",
      "<p>见<a href=\"http://www.cnblogs.com/neopenx/p/4480701.html\" target=\"_blank\">Cifar-10训练技巧</a></p></div>\n",
      "</body></html>\n"
     ]
    }
   ],
   "source": [
    "html_template = \"\"\"<!DOCTYPE html>\n",
    "<html><head><meta charset=\"UTF-8\">\n",
    "</head><body>\n",
    "<p><a href=\"{origin}\">原文链接</a></p>\n",
    "<p><center><h1>{title}</h1></center></p>\n",
    "    {content}\n",
    "</body></html>\"\"\"\n",
    "html = html_template.format(origin=url, title=title, content=content)\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤7：将文件写入磁盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = title + \".html\"\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(html.replace(u'\\xa0', u' ').replace(u'\\U0001f60a', u' '))  # 在windows中出错，所以这里进行了字符串替换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (完)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
