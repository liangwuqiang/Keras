<!DOCTYPE html>
    <html><head><meta charset="UTF-8">
    </head><body>
    <p><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/">原文链接</a></p>
        <div class="tut-main-content-pad">
<hr>
<h1>Dropout 解决 overfitting</h1>
<p style="text-align: center;">
        
          作者: <strong>Mark JingNB</strong>   
        
        编辑: <strong>莫烦</strong>   
        
          2016-11-03
        
      </p>
<p>学习资料:</p>
<ul>
<li><a href="https://github.com/MorvanZhou/tutorials/tree/master/tensorflowTUT/tf17_dropout">相关代码</a></li>
<li>为 TF 2017 打造的<a href="https://github.com/MorvanZhou/Tensorflow-Tutorial">新版可视化教学代码</a></li>
<li>机器学习-简介系列 <a href="/tutorials/machine-learning/ML-intro/3-05-overfitting/">过拟合</a></li>
</ul>
<h4 class="tut-h4-pad" id="要定">要定</h4>
<p>Overfitting 也被称为过度学习，过度拟合。 它是机器学习中常见的问题。
举个Classification（分类）的例子。</p>
<p><img class="course-image" src="images/22dab5afbb857ac2f1f3b40227c3eaf6.png"/></p>
<p>图中黑色曲线是正常模型，绿色曲线就是overfitting模型。尽管绿色曲线很精确的区分了所有的训练数据，但是并没有描述数据的整体特征，对新测试数据的适应性较差。</p>
<p>举个Regression (回归)的例子，</p>
<p><img class="course-image" src="images/b50e56673ee35f308a74b283b1f615d9.png"/></p>
<p>第三条曲线存在overfitting问题，尽管它经过了所有的训练点，但是不能很好的反应数据的趋势，预测能力严重不足。
TensorFlow提供了强大的dropout方法来解决overfitting问题。</p>
<h4 class="tut-h4-pad" id="建立 dropout 层">建立 dropout 层</h4>
<p>本次内容需要使用一下 sklearn 数据库当中的数据, 没有安装 sklearn 
的同学可以参考一下<a href="/tutorials/machine-learning/sklearn/1-2-install/">这个教程</a>
安装一下. 然后 <code class="highlighter-rouge">import</code> 以下模块.</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelBinarizer</span>
</code></pre>
</div>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="n">Wx_plus_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">Wx_plus_b</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
</code></pre>
</div>
<p>这里的<code class="highlighter-rouge">keep_prob</code>是保留概率，即我们要保留的结果所占比例，它作为一个<code class="highlighter-rouge">placeholder</code>，在<code class="highlighter-rouge">run</code>时传入，
当<code class="highlighter-rouge">keep_prob=1</code>的时候，相当于100%保留，也就是dropout没有起作用。
下面我们分析一下程序结构，首先准备数据，</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">3</span><span class="p">)</span>
</code></pre>
</div>
<p>其中<code class="highlighter-rouge">X_train</code>是训练数据, <code class="highlighter-rouge">X_test</code>是测试数据。
然后添加隐含层和输出层</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># add output layer</span>
<span class="n">l1</span> <span class="o">=</span> <span class="n">add_layer</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="s">'l1'</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">)</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">add_layer</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="s">'l2'</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>
</code></pre>
</div>
<p>loss函数（即最优化目标函数）选用交叉熵函数。交叉熵用来衡量预测值和真实值的相似程度，如果完全相同，交叉熵就等于零。</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">ys</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prediction</span><span class="p">),</span>
                                              <span class="n">reduction_indices</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>  <span class="c"># loss</span>
</code></pre>
</div>
<p>train方法（最优化算法）采用梯度下降法。</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>
</code></pre>
</div>
<h4 class="tut-h4-pad" id="训练">训练</h4>
<p>最后开始train，总共训练500次。</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">xs</span><span class="p">:</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">ys</span><span class="p">:</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">})</span>
<span class="c">#sess.run(train_step, feed_dict={xs: X_train, ys: y_train, keep_prob: 1})</span>
</code></pre>
</div>
<h4 class="tut-h4-pad" id="可视化结果">可视化结果</h4>
<p>训练中<code class="highlighter-rouge">keep_prob=1</code>时，就可以暴露出overfitting问题。<code class="highlighter-rouge">keep_prob=0.5</code>时，<code class="highlighter-rouge">dropout</code>就发挥了作用。
我们可以两种参数分别运行程序，对比一下结果。</p>
<p>当<code class="highlighter-rouge">keep_prob=1</code>时，模型对训练数据的适应性优于测试数据，存在overfitting，输出如下：
红线是 <code class="highlighter-rouge">train</code> 的误差, 蓝线是 <code class="highlighter-rouge">test</code> 的误差.</p>
<p><img class="course-image" src="images/400a842daa67a0ff9158a95f0c486ff5.png"/></p>
<p>当<code class="highlighter-rouge">keep_prob=0.5</code>时效果好了很多，输出如下：</p>
<p><img class="course-image" src="images/e44246e084c9d470be565ddc3b135699.png"/></p>
<p>程序中用到了Tensorboard输出结果，可以参考前面教程:</p>
<ul>
<li><a href="/tutorials/machine-learning/tensorflow/4-1-tensorboard1/">可视化好助手 Tensorboard 1</a></li>
<li><a href="/tutorials/machine-learning/tensorflow/4-2-tensorboard2/">可视化好助手 Tensorboard 2</a></li>
</ul>
<p style="font-size: 0.8em; padding:4em 1em 0.5em 1em; margin: 0 auto;">
        如果你觉得这篇文章或视频对你的学习很有帮助, 请你也分享它, 让它能再次帮助到更多的需要学习的人.

        莫烦没有正式的经济来源, 如果你也想支持 <strong>莫烦Python</strong> 并看到更好的教学内容, 赞助他一点点, 作为鼓励他继续开源的动力.
      </p>
<!-- donation -->
<div id="bottom-donation-section">
<h3 id="bottom-donation-title">支持 让教学变得更优秀</h3>
<br>
<div>
<a href="/support/" id="bottom-donation-button"><strong>点我 赞助 莫烦</strong></a>
</div>
<br>
</br></br></div>
<hr>
</hr></hr></div>
    </body></html>