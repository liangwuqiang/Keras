<!DOCTYPE html>
    <html><head><meta charset="UTF-8">
    </head><body>
    <p><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/">原文链接</a></p>
        <div class="tut-main-content-pad">
<br>
<h1>Batch Normalization 批标准化</h1>
<p style="text-align: center;">
        
          作者: <strong>莫烦</strong>   
        
        编辑: <strong>莫烦</strong>   
        
          2016-12-05
        
      </p>
<p>学习资料:</p>
<ul>
<li><a href="https://arxiv.org/abs/1502.03167">Batch normalization paper</a></li>
<li><a href="https://github.com/MorvanZhou/tutorials/blob/master/tensorflowTUT/tf23_BN/tf23_BN.py">本节相关代码</a></li>
<li>为 TF 2017 打造的<a href="https://github.com/MorvanZhou/Tensorflow-Tutorial">新版可视化教学代码</a></li>
<li>我制作的 <a href="/tutorials/machine-learning/ML-intro/3-08-batch-normalization/">Batch normalization 简介视频</a></li>
</ul>
<h4 class="tut-h4-pad" id="什么是 Batch Normalization">什么是 Batch Normalization</h4>
<p>请参考我制作的 <a href="/tutorials/machine-learning/ML-intro/3-08-batch-normalization/">Batch normalization 简介视频</a>
Batch normalization 是一种解决深度神经网络层数太多, 而没办法有效前向传递(forward propagate)的问题. 
因为每一层的输出值都会有不同的 均值(mean) 和 方差(deviation), 所以输出数据的分布也不一样, 如下图, 从左到右是每一层的输入数据分布, 
上排的没有 Batch normalization, 下排的有 Batch normalization.</p>
<p><img alt="Batch Normalization 批标准化829" class="course-image" src="images/c1849496c2cd68d3510f50bfa1821610.png"/></p>
<p>我们以前说过, 为了更有效的学习数据, 我们会对数据预处理, 进行 normalization (请参考我制作的 <a href="/tutorials/machine-learning/ML-intro/3-02-normalization/">为什么要特征标准化</a>).
而现在请想象, 我们可以把 “每层输出的值” 都看成 “后面一层所接收的数据”. 对每层都进行一次 normalization 会不会更好呢?
这就是 Batch normalization 方法的由来.</p>
<h4 class="tut-h4-pad" id="搭建网络">搭建网络</h4>
<p>输入需要的模块和定义网络的结构</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>


<span class="n">ACTIVATION</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span> <span class="c"># 每一层都使用 relu </span>
<span class="n">N_LAYERS</span> <span class="o">=</span> <span class="mi">7</span>            <span class="c"># 一共7层隐藏层</span>
<span class="n">N_HIDDEN_UNITS</span> <span class="o">=</span> <span class="mi">30</span>     <span class="c"># 每个层隐藏层有 30 个神经元</span>
</code></pre>
</div>
<p>使用 <code class="highlighter-rouge">build_net()</code> 功能搭建神经网络:</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">built_net</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">norm</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">add_layer</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c"># 添加层功能</span>
        <span class="n">Weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">],</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">1.</span><span class="p">))</span>
        <span class="n">biases</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_size</span><span class="p">])</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">)</span>
        <span class="n">Wx_plus_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">Weights</span><span class="p">)</span> <span class="o">+</span> <span class="n">biases</span>
        <span class="k">if</span> <span class="n">activation_function</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">Wx_plus_b</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">activation_function</span><span class="p">(</span><span class="n">Wx_plus_b</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span>

    <span class="n">fix_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">layers_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">xs</span><span class="p">]</span>    <span class="c"># 记录每层的 input</span>

    <span class="c"># loop 建立所有层</span>
    <span class="k">for</span> <span class="n">l_n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_LAYERS</span><span class="p">):</span>
        <span class="n">layer_input</span> <span class="o">=</span> <span class="n">layers_inputs</span><span class="p">[</span><span class="n">l_n</span><span class="p">]</span>
        <span class="n">in_size</span> <span class="o">=</span> <span class="n">layers_inputs</span><span class="p">[</span><span class="n">l_n</span><span class="p">]</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">add_layer</span><span class="p">(</span>
            <span class="n">layer_input</span><span class="p">,</span>    <span class="c"># input</span>
            <span class="n">in_size</span><span class="p">,</span>        <span class="c"># input size</span>
            <span class="n">N_HIDDEN_UNITS</span><span class="p">,</span> <span class="c"># output size</span>
            <span class="n">ACTIVATION</span><span class="p">,</span>     <span class="c"># activation function</span>
        <span class="p">)</span>
        <span class="n">layers_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>    <span class="c"># 把 output 加入记录</span>

    <span class="c"># 建立 output layer</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">add_layer</span><span class="p">(</span><span class="n">layers_inputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

    <span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">ys</span> <span class="o">-</span> <span class="n">prediction</span><span class="p">),</span> <span class="n">reduction_indices</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">train_op</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">layers_inputs</span><span class="p">]</span>
</code></pre>
</div>
<h4 class="tut-h4-pad" id="创建数据">创建数据</h4>
<p>创造数据并可视化数据:</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">500</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">x_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span> <span class="o">-</span> <span class="mi">5</span> <span class="o">+</span> <span class="n">noise</span>

<span class="c"># 可视化 input data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>
<p><img alt="Batch Normalization 批标准化830" class="course-image" src="images/45531fb8767caa267b6686aa0d938a69.png"/></p>
<h4 class="tut-h4-pad" id="Batch Normalization 代码">Batch Normalization 代码</h4>
<p>为了实现 Batch Normalization, 我们要对每一层的代码进行修改, 给 <code class="highlighter-rouge">built_net</code> 和 <code class="highlighter-rouge">add_layer</code> 都加上 
<code class="highlighter-rouge">norm</code> 参数, 表示是否是 Batch Normalization 层:</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">built_net</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">norm</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">add_layer</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
</code></pre>
</div>
<p>然后每层的 <code class="highlighter-rouge">Wx_plus_b</code> 需要进行一次 batch normalize 的步骤, 这样输出到 <code class="highlighter-rouge">activation</code> 的 <code class="highlighter-rouge">Wx_plus_b</code> 就已经被 <code class="highlighter-rouge">normalize</code> 过了:</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">if</span> <span class="n">norm</span><span class="p">:</span>    <span class="c"># 判断书否是 BN 层</span>
    <span class="n">fc_mean</span><span class="p">,</span> <span class="n">fc_var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">moments</span><span class="p">(</span>
        <span class="n">Wx_plus_b</span><span class="p">,</span>
        <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>   <span class="c"># 想要 normalize 的维度, [0] 代表 batch 维度</span>
                    <span class="c"># 如果是图像数据, 可以传入 [0, 1, 2], 相当于求[batch, height, width] 的均值/方差, 注意不要加入 channel 维度</span>
    <span class="p">)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">out_size</span><span class="p">]))</span>
    <span class="n">shift</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">out_size</span><span class="p">]))</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.001</span>
    <span class="n">Wx_plus_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">Wx_plus_b</span><span class="p">,</span> <span class="n">fc_mean</span><span class="p">,</span> <span class="n">fc_var</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="c"># 上面那一步, 在做如下事情:</span>
    <span class="c"># Wx_plus_b = (Wx_plus_b - fc_mean) / tf.sqrt(fc_var + 0.001)</span>
    <span class="c"># Wx_plus_b = Wx_plus_b * scale + shift</span>
    <span class="c"># 如果你已经看不懂了, 请去我最上面学习资料里的链接 (我制作的 Batch normalization 简介视频)</span>
</code></pre>
</div>
<p>如果你是使用 batch 进行每次的更新, 那每个 batch 的 mean/var 都会不同, 所以我们可以使用 moving average 的方法记录并慢慢改进 mean/var 的值.
然后将修改提升后的 mean/var 放入 <code class="highlighter-rouge">tf.nn.batch_normalization()</code>.
而且在 test 阶段, 我们就可以直接调用最后一次修改的 mean/var 值进行测试, 而不是采用 test 时的 fc_mean/fc_var.</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># 对这句进行扩充, 修改前:</span>
<span class="n">Wx_plus_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">Wx_plus_b</span><span class="p">,</span> <span class="n">fc_mean</span><span class="p">,</span> <span class="n">fc_var</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>

<span class="c"># 修改后:</span>
<span class="n">ema</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">ExponentialMovingAverage</span><span class="p">(</span><span class="n">decay</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c"># exponential moving average 的 decay 度</span>
<span class="k">def</span> <span class="nf">mean_var_with_update</span><span class="p">():</span>
    <span class="n">ema_apply_op</span> <span class="o">=</span> <span class="n">ema</span><span class="o">.</span><span class="nb">apply</span><span class="p">([</span><span class="n">fc_mean</span><span class="p">,</span> <span class="n">fc_var</span><span class="p">])</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">ema_apply_op</span><span class="p">]):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">fc_mean</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">fc_var</span><span class="p">)</span>
<span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">mean_var_with_update</span><span class="p">()</span>      <span class="c"># 根据新的 batch 数据, 记录并稍微修改之前的 mean/var</span>

<span class="c"># 将修改后的 mean / var 放入下面的公式</span>
<span class="n">Wx_plus_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">Wx_plus_b</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</code></pre>
</div>
<p>那如何确定我们是在 train 阶段还是在 test 阶段呢, 我们可以修改上面的算法, 想办法传入 <code class="highlighter-rouge">on_train</code> 参数, 
你也可以把 <code class="highlighter-rouge">on_train</code> 定义成全局变量. (<strong>注意: github 的代码中没有这一段, 想做 test 的同学们需要自己修改</strong>)</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># 修改前:</span>
<span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">mean_var_with_update</span><span class="p">()</span> 

<span class="c"># 修改后:</span>
<span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">on_train</span><span class="p">,</span>    <span class="c"># on_train 的值是 True/False</span>
                    <span class="n">mean_var_with_update</span><span class="p">,</span>   <span class="c"># 如果是 True, 更新 mean/var</span>
                    <span class="k">lambda</span><span class="p">:</span> <span class="p">(</span>               <span class="c"># 如果是 False, 返回之前 fc_mean/fc_var 的Moving Average</span>
                        <span class="n">ema</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">fc_mean</span><span class="p">),</span> 
                        <span class="n">ema</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">fc_var</span><span class="p">)</span>
                        <span class="p">)</span>    
                    <span class="p">)</span>
</code></pre>
</div>
<p>同样, 我们也可以在输入数据 <code class="highlighter-rouge">xs</code> 时, 给它做一个 normalization, <strong>同样, 如果是最 batch data 来训练的话, 要重复上述的记录修改 mean/var 的步骤</strong>:</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code>    <span class="k">if</span> <span class="n">norm</span><span class="p">:</span>
        <span class="c"># BN for the first input</span>
        <span class="n">fc_mean</span><span class="p">,</span> <span class="n">fc_var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">moments</span><span class="p">(</span>
            <span class="n">xs</span><span class="p">,</span>
            <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">shift</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.001</span>
        <span class="n">xs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">fc_mean</span><span class="p">,</span> <span class="n">fc_var</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</code></pre>
</div>
<p>然后我们把在建立网络的循环中的这一步加入 <code class="highlighter-rouge">norm</code> 这个参数:</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">output</span> <span class="o">=</span> <span class="n">add_layer</span><span class="p">(</span>
            <span class="n">layer_input</span><span class="p">,</span>    <span class="c"># input</span>
            <span class="n">in_size</span><span class="p">,</span>        <span class="c"># input size</span>
            <span class="n">N_HIDDEN_UNITS</span><span class="p">,</span> <span class="c"># output size</span>
            <span class="n">ACTIVATION</span><span class="p">,</span>     <span class="c"># activation function</span>
            <span class="n">norm</span><span class="p">,</span>           <span class="c"># normalize before activation</span>
        <span class="p">)</span>
</code></pre>
</div>
<div>
<script async="" src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle" data-ad-client="ca-pub-4601203457616636" data-ad-format="fluid" data-ad-layout="in-article" data-ad-slot="3397817325" style="display:block; text-align:center;"></ins>
<script>
       (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>
<h4 class="tut-h4-pad" id="对比有无 BN">对比有无 BN</h4>
<p>搭建两个神经网络, 一个没有 BN, 一个有 BN:</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">xs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c"># [num_samples, num_features]</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">train_op</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">layers_inputs</span> <span class="o">=</span> <span class="n">built_net</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>   <span class="c"># without BN</span>
<span class="n">train_op_norm</span><span class="p">,</span> <span class="n">cost_norm</span><span class="p">,</span> <span class="n">layers_inputs_norm</span> <span class="o">=</span> <span class="n">built_net</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c"># with BN</span>
</code></pre>
</div>
<p>训练神经网络:</p>
<p>代码中的 <code class="highlighter-rouge">plot_his()</code> 不会在这里讲解, 请自己在<a href="https://github.com/MorvanZhou/tutorials/blob/master/tensorflowTUT/tf23_BN/tf23_BN.py">全套代码中查看</a>.</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

<span class="c"># 记录两种网络的 cost 变化</span>
<span class="n">cost_his</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">cost_his_norm</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">record_step</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ion</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">251</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c"># 每层在 activation 之前计算结果值的分布</span>
        <span class="n">all_inputs</span><span class="p">,</span> <span class="n">all_inputs_norm</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">layers_inputs</span><span class="p">,</span> <span class="n">layers_inputs_norm</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">xs</span><span class="p">:</span> <span class="n">x_data</span><span class="p">,</span> <span class="n">ys</span><span class="p">:</span> <span class="n">y_data</span><span class="p">})</span>
        <span class="n">plot_his</span><span class="p">(</span><span class="n">all_inputs</span><span class="p">,</span> <span class="n">all_inputs_norm</span><span class="p">)</span>

    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">xs</span><span class="p">:</span> <span class="n">x_data</span><span class="p">,</span> <span class="n">ys</span><span class="p">:</span> <span class="n">y_data</span><span class="p">})</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_op_norm</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">xs</span><span class="p">:</span> <span class="n">x_data</span><span class="p">,</span> <span class="n">ys</span><span class="p">:</span> <span class="n">y_data</span><span class="p">})</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">record_step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c"># 记录 cost</span>
        <span class="n">cost_his</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">xs</span><span class="p">:</span> <span class="n">x_data</span><span class="p">,</span> <span class="n">ys</span><span class="p">:</span> <span class="n">y_data</span><span class="p">}))</span>
        <span class="n">cost_his_norm</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cost_norm</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">xs</span><span class="p">:</span> <span class="n">x_data</span><span class="p">,</span> <span class="n">ys</span><span class="p">:</span> <span class="n">y_data</span><span class="p">}))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ioff</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cost_his</span><span class="p">))</span><span class="o">*</span><span class="n">record_step</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cost_his</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'no BN'</span><span class="p">)</span>     <span class="c"># no norm</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cost_his</span><span class="p">))</span><span class="o">*</span><span class="n">record_step</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cost_his_norm</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'BN'</span><span class="p">)</span>   <span class="c"># norm</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>
<p><img alt="Batch Normalization 批标准化831" class="course-image" src="images/81cf8e748b0738295d8fdba15c32b744.gif"/></p>
<p>可以看出, 没有用 BN 的时候, 每层的值迅速全部都变为 0, 也可以说, 所有的神经元都已经死了. 
而有 BN, <code class="highlighter-rouge">relu</code> 过后, 每层的值都能有一个比较好的分布效果, 大部分神经元都还活着.
(看不懂了? 没问题, 再去看一遍<a href="/tutorials/machine-learning/ML-intro/3-08-batch-normalization/">我制作的 Batch normalization 简介视频</a>).</p>
<p>Relu 激励函数的图在这里:</p>
<p><img alt="Batch Normalization 批标准化832" class="course-image" src="images/351f31298d5ac3e04653d85d8a66987e.png"/></p>
<p>我们也看看使用 relu cost 的对比:</p>
<p><img alt="Batch Normalization 批标准化833" class="course-image" src="images/098358620bd7976b3e3bd7c650c6b8e2.png"/></p>
<p>因为没有使用 NB 的网络, 大部分神经元都死了, 所以连误差曲线都没了.</p>
<p>如果使用不同的 <code class="highlighter-rouge">ACTIVATION</code> 会怎么样呢? 不如把 <code class="highlighter-rouge">relu</code> 换成 <code class="highlighter-rouge">tanh</code>:</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">ACTIVATION</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span>
</code></pre>
</div>
<p><img alt="Batch Normalization 批标准化834" class="course-image" src="images/6b4b4d907e5f0b67d1c1cfe206e5d5c6.gif"/></p>
<p>可以看出, 没有 NB, 每层的值迅速全部都饱和, 都跑去了 -1/1 这个饱和区间, 
有 NB, 即使前一层因变得相对饱和, 
但是后面几层的值都被 normalize 到有效的不饱和区间内计算. 确保了一个活的神经网络.</p>
<p>tanh 激励函数的图在这里:</p>
<p><img alt="Batch Normalization 批标准化835" class="course-image" src="images/f7c74dd26c04e8aca49a81823801e78f.gif"/></p>
<p>最后我们看一下使用 tanh 的误差对比:</p>
<p><img alt="Batch Normalization 批标准化836" class="course-image" src="images/50121f8654dae8eba4fbda2413fbcc48.png"/></p>
<p style="font-size: 0.8em; padding:4em 1em 0.5em 1em; margin: 0 auto;">
        如果你觉得这篇文章或视频对你的学习很有帮助, 请你也分享它, 让它能再次帮助到更多的需要学习的人.

        莫烦没有正式的经济来源, 如果你也想支持 <strong>莫烦Python</strong> 并看到更好的教学内容, 赞助他一点点, 作为鼓励他继续开源的动力.
      </p>
<!-- donation -->
<div id="bottom-donation-section">
<h3 id="bottom-donation-title">支持 让教学变得更优秀</h3>
<br>
<div>
<a href="/support/" id="bottom-donation-button"><strong>点我 赞助 莫烦</strong></a>
</div>
<br>
</br></br></div>
<hr>
</hr></br></div>
    </body></html>