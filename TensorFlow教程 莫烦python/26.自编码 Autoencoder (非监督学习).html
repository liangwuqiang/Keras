<!DOCTYPE html>
    <html><head><meta charset="UTF-8">
    </head><body>
    <p><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/">原文链接</a></p>
        <div class="tut-main-content-pad">
<hr>
<h1>自编码 Autoencoder (非监督学习)</h1>
<p style="text-align: center;">
        
          作者: <strong>Hao</strong>   
        
        编辑: <strong>莫烦</strong>   
        
          2016-11-03
        
      </p>
<p>学习资料:</p>
<ul>
<li><a href="https://github.com/MorvanZhou/tutorials/tree/master/tensorflowTUT/tf21_autoencoder">相关代码</a></li>
<li>为 TF 2017 打造的<a href="https://github.com/MorvanZhou/Tensorflow-Tutorial">新版可视化教学代码</a></li>
<li>机器学习-简介系列 <a href="/tutorials/machine-learning/ML-intro/2-5-autoencoder/">Autoencoder</a></li>
</ul>
<h4 class="tut-h4-pad" id="要点">要点</h4>
<p><strong>Autoencoder</strong> 简单来说就是将有很多Feature的数据进行压缩，之后再进行解压的过程。
本质上来说，它也是一个对数据的非监督学习，如果大家知道 <strong>PCA</strong> (Principal component analysis)，
与 Autoencoder 相类似，它的主要功能即对数据进行非监督学习，并将压缩之后得到的“特征值”，这一中间结果正类似于PCA的结果。
之后再将压缩过的“特征值”进行解压，得到的最终结果与原始数据进行比较，对此进行非监督学习。如果大家还不是非常了解，请观<em>看机器学习简介</em>系列里的 
<a href="/tutorials/machine-learning/ML-intro/2-5-autoencoder/">Autoencoder</a> 那一集；
如果对它已经有了一定的了解，那么便可以进行代码阶段的学习了。大概过程如下图所示：</p>
<p><img class="course-image" src="images/768bad8e9765d504b9a35b0b8363bc62.png"/></p>
<p>今天的代码，我们会运用两个类型：</p>
<ul>
<li>第一，是通过Feature的压缩并解压，并将结果与原始数据进行对比，观察处理过后的数据是不是如预期跟原始数据很相像。（这里会用到MNIST数据）</li>
<li>第二，我们只看 <code class="highlighter-rouge">encoder</code> 压缩的过程，使用它将一个数据集压缩到只有两个Feature时，将数据放入一个二维坐标系内，特征压缩的效果如下：</li>
</ul>
<p><img class="course-image" src="images/f6ee95f5e487f205e0471d2ba08387fd.png"/></p>
<p>同样颜色的点，代表分到同一类的数据。（Lebel相同）</p>
<h4 class="tut-h4-pad" id="Autoencoder">Autoencoder</h4>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Parameter</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">training_epochs</span> <span class="o">=</span> <span class="mi">5</span> <span class="c"># 五组训练</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">display_step</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">examples_to_show</span> <span class="o">=</span> <span class="mi">10</span>
</code></pre>
</div>
<p>我们的MNIST数据，每张图片大小是 28x28 pix，即 784 Features：</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Network Parameters</span>
<span class="n">n_input</span> <span class="o">=</span> <span class="mi">784</span>  <span class="c"># MNIST data input (img shape: 28*28)</span>
</code></pre>
</div>
<ul>
<li>在压缩环节：我们要把这个Features不断压缩，经过第一个隐藏层压缩至256个 Features，再经过第二个隐藏层压缩至128个。</li>
<li>在解压环节：我们将128个Features还原至256个，再经过一步还原至784个。</li>
<li>在对比环节：比较原始数据与还原后的拥有 784 Features 的数据进行 cost 的对比，根据 cost 来提升我的 Autoencoder 的准确率，下图是两个隐藏层的 weights 和 biases 的定义：</li>
</ul>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># hidden layer settings</span>
<span class="n">n_hidden_1</span> <span class="o">=</span> <span class="mi">256</span> <span class="c"># 1st layer num features</span>
<span class="n">n_hidden_2</span> <span class="o">=</span> <span class="mi">128</span> <span class="c"># 2nd layer num features</span>
<span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
	<span class="s">'encoder_h1'</span><span class="p">:</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_input</span><span class="p">,</span><span class="n">n_hidden_1</span><span class="p">])),</span>
	<span class="s">'encoder_h2'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden_1</span><span class="p">,</span><span class="n">n_hidden_2</span><span class="p">])),</span>
	<span class="s">'decoder_h1'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden_2</span><span class="p">,</span><span class="n">n_hidden_1</span><span class="p">])),</span>
	<span class="s">'decoder_h2'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden_1</span><span class="p">,</span> <span class="n">n_input</span><span class="p">])),</span>
	<span class="p">}</span>
<span class="n">biases</span> <span class="o">=</span> <span class="p">{</span>
	<span class="s">'encoder_b1'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden_1</span><span class="p">])),</span>
	<span class="s">'encoder_b2'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden_2</span><span class="p">])),</span>
	<span class="s">'decoder_b1'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden_1</span><span class="p">])),</span>
	<span class="s">'decoder_b2'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_input</span><span class="p">])),</span>
	<span class="p">}</span>
</code></pre>
</div>
<p>下面来定义 <strong>Encoder</strong> 和 <strong>Decoder</strong> ，使用的 Activation function 是 <code class="highlighter-rouge">sigmoid</code>，
压缩之后的值应该在 [0,1] 这个范围内。在 <code class="highlighter-rouge">decoder</code> 过程中，通常使用对应于 <code class="highlighter-rouge">encoder</code> 的 Activation function：</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Building the encoder</span>
<span class="k">def</span> <span class="nf">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c"># Encoder Hidden layer with sigmoid activation #1</span>
    <span class="n">layer_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'encoder_h1'</span><span class="p">]),</span>
                                   <span class="n">biases</span><span class="p">[</span><span class="s">'encoder_b1'</span><span class="p">]))</span>
    <span class="c"># Decoder Hidden layer with sigmoid activation #2</span>
    <span class="n">layer_2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">layer_1</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'encoder_h2'</span><span class="p">]),</span>
                                   <span class="n">biases</span><span class="p">[</span><span class="s">'encoder_b2'</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">layer_2</span>
    
<span class="c"># Building the decoder</span>
<span class="k">def</span> <span class="nf">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c"># Encoder Hidden layer with sigmoid activation #1</span>
    <span class="n">layer_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'decoder_h1'</span><span class="p">]),</span>
                                   <span class="n">biases</span><span class="p">[</span><span class="s">'decoder_b1'</span><span class="p">]))</span>
    <span class="c"># Decoder Hidden layer with sigmoid activation #2</span>
    <span class="n">layer_2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">layer_1</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'decoder_h2'</span><span class="p">]),</span>
                                   <span class="n">biases</span><span class="p">[</span><span class="s">'decoder_b2'</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">layer_2</span>
</code></pre>
</div>
<p>来实现 <strong>Encoder</strong> 和 <strong>Decoder</strong> 输出的结果：</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Construct model</span>
<span class="n">encoder_op</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 			<span class="c"># 128 Features</span>
<span class="n">decoder_op</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">encoder_op</span><span class="p">)</span>	<span class="c"># 784 Features</span>

<span class="c"># Prediction</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">decoder_op</span>	<span class="c"># After </span>
<span class="c"># Targets (Labels) are the input data.</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">X</span>			<span class="c"># Before</span>
</code></pre>
</div>
<p>再通过我们非监督学习进行对照，即对 “原始的有 784 Features 的数据集” 和 “通过 ‘Prediction’ 
得出的有 784 Features 的数据集” 进行最小二乘法的计算，并且使 cost 最小化:</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Define loss and optimizer, minimize the squared error</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
</code></pre>
</div>
<p>最后，通过 <code class="highlighter-rouge">Matplotlib</code> 的 <code class="highlighter-rouge">pyplot</code> 模块将结果显示出来， 注意在输出时MNIST数据集经过压缩之后 x 的最大值是1，而非255：</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Launch the graph</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="c"># tf 马上就要废弃tf.initialize_all_variables()这种写法</span>
    <span class="c"># 替换成下面:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
    <span class="n">total_batch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">num_examples</span><span class="o">/</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="c"># Training cycle</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">training_epochs</span><span class="p">):</span>
        <span class="c"># Loop over all batches</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_batch</span><span class="p">):</span>
            <span class="n">batch_xs</span><span class="p">,</span> <span class="n">batch_ys</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>  <span class="c"># max(x) = 1, min(x) = 0</span>
            <span class="c"># Run optimization op (backprop) and cost op (to get loss value)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">cost</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">batch_xs</span><span class="p">})</span>
        <span class="c"># Display logs per epoch step</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">display_step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Epoch:"</span><span class="p">,</span> <span class="s">'</span><span class="si">%04</span><span class="s">d'</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span>
                  <span class="s">"cost="</span><span class="p">,</span> <span class="s">"{:.9f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"Optimization Finished!"</span><span class="p">)</span>

    <span class="c"># # Applying encode and decode over test set</span>
    <span class="n">encode_decode</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="n">y_pred</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">images</span><span class="p">[:</span><span class="n">examples_to_show</span><span class="p">]})</span>
    <span class="c"># Compare original images with their reconstructions</span>
    <span class="n">f</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">examples_to_show</span><span class="p">):</span>
        <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)))</span>
        <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">encode_decode</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>
<p>通过5个 Epoch 的训练，（通常情况下，想要得到好的的效果，我们应进行10 ~ 20个 Epoch 的训练）我们的结果如下：</p>
<p><img class="course-image" src="images/6e2c525ec9d7fb381b6c24f5cad3bf1c.png"/></p>
<p>上面一行是真实数据，下面一行是经过 <code class="highlighter-rouge">encoder</code> 和 <code class="highlighter-rouge">decoder</code> 之后的数据，如果继续进行训练，效果会更好。</p>
<h4 class="tut-h4-pad" id="Encoder">Encoder</h4>
<p>在类型二中，我们只显示 <code class="highlighter-rouge">encoder</code> 之后的数据， 并画在一个二维直角坐标系内。做法很简单，我们将原有 784 Features 的数据压缩成仅剩 2 Features 的数据：</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Parameters</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>    <span class="c"># 0.01 this learning rate will be better! Tested</span>
<span class="n">training_epochs</span> <span class="o">=</span> <span class="mi">10</span>	<span class="c"># 10 Epoch 训练</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">display_step</span> <span class="o">=</span> <span class="mi">1</span>
</code></pre>
</div>
<p>通过四层 Hidden Layers 实现将 784 Features 压缩至 2 Features：</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># hidden layer settings</span>
<span class="n">n_hidden_1</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">n_hidden_2</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">n_hidden_3</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n_hidden_4</span> <span class="o">=</span> <span class="mi">2</span>
</code></pre>
</div>
<p>Weights 和 biases 也要做相应的变化：</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'encoder_h1'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">n_input</span><span class="p">,</span> <span class="n">n_hidden_1</span><span class="p">],)),</span>
    <span class="s">'encoder_h2'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">n_hidden_1</span><span class="p">,</span> <span class="n">n_hidden_2</span><span class="p">],)),</span>
    <span class="s">'encoder_h3'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">n_hidden_2</span><span class="p">,</span> <span class="n">n_hidden_3</span><span class="p">],)),</span>
    <span class="s">'encoder_h4'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">n_hidden_3</span><span class="p">,</span> <span class="n">n_hidden_4</span><span class="p">],)),</span>

    <span class="s">'decoder_h1'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">n_hidden_4</span><span class="p">,</span> <span class="n">n_hidden_3</span><span class="p">],)),</span>
    <span class="s">'decoder_h2'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">n_hidden_3</span><span class="p">,</span> <span class="n">n_hidden_2</span><span class="p">],)),</span>
    <span class="s">'decoder_h3'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">n_hidden_2</span><span class="p">,</span> <span class="n">n_hidden_1</span><span class="p">],)),</span>
    <span class="s">'decoder_h4'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">n_hidden_1</span><span class="p">,</span> <span class="n">n_input</span><span class="p">],)),</span>
	<span class="p">}</span>
<span class="n">biases</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'encoder_b1'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden_1</span><span class="p">])),</span>
    <span class="s">'encoder_b2'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden_2</span><span class="p">])),</span>
    <span class="s">'encoder_b3'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden_3</span><span class="p">])),</span>
    <span class="s">'encoder_b4'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden_4</span><span class="p">])),</span>

    <span class="s">'decoder_b1'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden_3</span><span class="p">])),</span>
    <span class="s">'decoder_b2'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden_2</span><span class="p">])),</span>
    <span class="s">'decoder_b3'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden_1</span><span class="p">])),</span>
    <span class="s">'decoder_b4'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_input</span><span class="p">])),</span>
	<span class="p">}</span>
</code></pre>
</div>
<p>与类型一类似，创建四层神经网络。（注意：在第四层时，输出量不再是 [0,1] 范围内的数，而是将数据通过默认的 Linear activation function 调整为 (-∞,∞) ：</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">layer_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'encoder_h1'</span><span class="p">]),</span>
                                   <span class="n">biases</span><span class="p">[</span><span class="s">'encoder_b1'</span><span class="p">]))</span>
    <span class="n">layer_2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">layer_1</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'encoder_h2'</span><span class="p">]),</span>
                                   <span class="n">biases</span><span class="p">[</span><span class="s">'encoder_b2'</span><span class="p">]))</span>
    <span class="n">layer_3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">layer_2</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'encoder_h3'</span><span class="p">]),</span>
                                   <span class="n">biases</span><span class="p">[</span><span class="s">'encoder_b3'</span><span class="p">]))</span>
    <span class="n">layer_4</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">layer_3</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'encoder_h4'</span><span class="p">]),</span>
                                    <span class="n">biases</span><span class="p">[</span><span class="s">'encoder_b4'</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">layer_4</span>


<span class="k">def</span> <span class="nf">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">layer_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'decoder_h1'</span><span class="p">]),</span>
                                   <span class="n">biases</span><span class="p">[</span><span class="s">'decoder_b1'</span><span class="p">]))</span>
    <span class="n">layer_2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">layer_1</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'decoder_h2'</span><span class="p">]),</span>
                                   <span class="n">biases</span><span class="p">[</span><span class="s">'decoder_b2'</span><span class="p">]))</span>
    <span class="n">layer_3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">layer_2</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'decoder_h3'</span><span class="p">]),</span>
                                <span class="n">biases</span><span class="p">[</span><span class="s">'decoder_b3'</span><span class="p">]))</span>
    <span class="n">layer_4</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">layer_3</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'decoder_h4'</span><span class="p">]),</span>
                                <span class="n">biases</span><span class="p">[</span><span class="s">'decoder_b4'</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">layer_4</span>
</code></pre>
</div>
<p>在输出图像时，我们只关心 <code class="highlighter-rouge">encoder</code> 压缩之后，即 <code class="highlighter-rouge">decoder</code> 解压之前的结果：</p>
<p><img class="course-image" src="images/b521896d2b9434a77e0586179af8f3b8.png"/></p>
<p style="font-size: 0.8em; padding:4em 1em 0.5em 1em; margin: 0 auto;">
        如果你觉得这篇文章或视频对你的学习很有帮助, 请你也分享它, 让它能再次帮助到更多的需要学习的人.

        莫烦没有正式的经济来源, 如果你也想支持 <strong>莫烦Python</strong> 并看到更好的教学内容, 赞助他一点点, 作为鼓励他继续开源的动力.
      </p>
<!-- donation -->
<div id="bottom-donation-section">
<h3 id="bottom-donation-title">支持 让教学变得更优秀</h3>
<br>
<div>
<a href="/support/" id="bottom-donation-button"><strong>点我 赞助 莫烦</strong></a>
</div>
<br>
</br></br></div>
<hr>
</hr></hr></div>
    </body></html>