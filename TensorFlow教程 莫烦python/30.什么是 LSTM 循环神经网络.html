<!DOCTYPE html>
    <html><head><meta charset="UTF-8">
    </head><body>
    <p><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/">原文链接</a></p>
        <div class="tut-main-content-pad">
<br>
<h1>什么是 LSTM 循环神经网络</h1>
<p style="text-align: center;">
        
          作者: <strong>莫烦</strong>   
        
        编辑: <strong>莫烦</strong>   
        
      </p>
<p>学习资料:</p>
<ul>
<li>Tensorflow RNN <a href="/tutorials/machine-learning/tensorflow/5-07-RNN1/">例子1</a></li>
<li>Tensorflow RNN <a href="/tutorials/machine-learning/tensorflow/5-08-RNN2/">例子2</a></li>
<li>Tensorflow RNN <a href="/tutorials/machine-learning/tensorflow/5-09-RNN3/">例子3</a></li>
<li>PyTorch RNN <a href="/tutorials/machine-learning/torch/4-02-RNN-classification/">例子1</a></li>
<li>PyTorch RNN <a href="/tutorials/machine-learning/torch/4-03-RNN-regression/">例子2</a></li>
<li>Keras <a href="/tutorials/machine-learning/keras/2-4-RNN-classifier/">快速搭建 RNN 1</a></li>
<li>Keras <a href="/tutorials/machine-learning/keras/2-5-RNN-LSTM-Regressor/">快速搭建 RNN 2</a></li>
</ul>
<p>今天我们会来聊聊在普通RNN的弊端和为了解决这个弊端而提出的 LSTM 技术. LSTM 是 long-short term memory 的简称, 中文叫做 长短期记忆. 是当下最流行的 RNN 形式之一.</p>
<p><strong>注: 本文不会涉及数学推导. 大家可以在很多其他地方找到优秀的数学推导文章.</strong></p>
<h4 class="tut-h4-pad" id="RNN 的弊端">RNN 的弊端</h4>
<p><img alt="什么是 LSTM 循环神经网络795" class="course-image" src="images/3761c2d0b7f58c21c16887445a7d16ac.png"/></p>
<p>之前我们说过, RNN 是在有顺序的数据上进行学习的. 为了记住这些数据, RNN 会像人一样产生对先前发生事件的记忆. 不过一般形式的 RNN 就像一个老爷爷, 有时候比较健忘. 为什么会这样呢?</p>
<p><img alt="什么是 LSTM 循环神经网络796" class="course-image" src="images/938582ccb82da4609206426b4a351f06.png"/></p>
<p>想像现在有这样一个 RNN, 他的输入值是一句话: ‘我今天要做红烧排骨, 首先要准备排骨, 然后…., 最后美味的一道菜就出锅了’, shua ~ 说着说着就流口水了. 现在请 RNN 来分析, 我今天做的到底是什么菜呢. RNN可能会给出“辣子鸡”这个答案. 由于判断失误, RNN就要开始学习 这个长序列 X 和 ‘红烧排骨’ 的关系 , 而RNN需要的关键信息 ”红烧排骨”却出现在句子开头,</p>
<p><img alt="什么是 LSTM 循环神经网络797" class="course-image" src="images/c3a7461f8396b5f901636df7819e960a.png"/></p>
<p><img alt="什么是 LSTM 循环神经网络798" class="course-image" src="images/c244cb1ffbbba9d668cdee06d15c4827.png"/></p>
<p>再来看看 RNN是怎样学习的吧. 红烧排骨这个信息原的记忆要进过长途跋涉才能抵达最后一个时间点. 然后我们得到误差, 而且在 反向传递 得到的误差的时候, 他在每一步都会 乘以一个自己的参数 W. 如果这个 W 是一个小于1 的数, 比如0.9. 这个0.9 不断乘以误差, 误差传到初始时间点也会是一个接近于零的数, 所以对于初始时刻, 误差相当于就消失了. 我们把这个问题叫做梯度消失或者梯度弥散 Gradient vanishing. 反之如果 W 是一个大于1 的数, 比如1.1 不断累乘, 则到最后变成了无穷大的数, RNN被这无穷大的数撑死了, 这种情况我们叫做剃度爆炸, Gradient exploding. 这就是普通 RNN 没有办法回忆起久远记忆的原因.</p>
<div>
<script async="" src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle" data-ad-client="ca-pub-4601203457616636" data-ad-format="fluid" data-ad-layout="in-article" data-ad-slot="3397817325" style="display:block; text-align:center;"></ins>
<script>
       (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>
<h4 class="tut-h4-pad" id="LSTM">LSTM</h4>
<p><img alt="什么是 LSTM 循环神经网络799" class="course-image" src="images/ec7cabc99973965337a1c71c6ecb2dec.png"/></p>
<p>LSTM 就是为了解决这个问题而诞生的. LSTM 和普通 RNN 相比, 多出了三个控制器. (输入控制, 输出控制, 忘记控制). 现在, LSTM RNN 内部的情况是这样.</p>
<p>他多了一个 控制全局的记忆, 我们用粗线代替. 为了方便理解, 我们把粗线想象成电影或游戏当中的 主线剧情. 而原本的 RNN 体系就是 分线剧情. 三个控制器都是在原始的 RNN 体系上, 我们先看 输入方面 , 如果此时的分线剧情对于剧终结果十分重要, 输入控制就会将这个分线剧情按重要程度 写入主线剧情 进行分析. 再看 忘记方面, 如果此时的分线剧情更改了我们对之前剧情的想法, 那么忘记控制就会将之前的某些主线剧情忘记, 按比例替换成现在的新剧情. 所以 主线剧情的更新就取决于输入 和忘记 控制. 最后的输出方面, 输出控制会基于目前的主线剧情和分线剧情判断要输出的到底是什么.基于这些控制机制, LSTM 就像延缓记忆衰退的良药, 可以带来更好的结果.</p>
<p style="font-size: 0.8em; padding:4em 1em 0.5em 1em; margin: 0 auto;">
        如果你觉得这篇文章或视频对你的学习很有帮助, 请你也分享它, 让它能再次帮助到更多的需要学习的人.

        莫烦没有正式的经济来源, 如果你也想支持 <strong>莫烦Python</strong> 并看到更好的教学内容, 赞助他一点点, 作为鼓励他继续开源的动力.
      </p>
<!-- donation -->
<div id="bottom-donation-section">
<h3 id="bottom-donation-title">支持 让教学变得更优秀</h3>
<br>
<div>
<a href="/support/" id="bottom-donation-button"><strong>点我 赞助 莫烦</strong></a>
</div>
<br>
</br></br></div>
<hr>
</hr></br></div>
    </body></html>