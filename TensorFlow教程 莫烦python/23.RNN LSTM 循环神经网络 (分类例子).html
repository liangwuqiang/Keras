<!DOCTYPE html>
    <html><head><meta charset="UTF-8">
    </head><body>
    <p><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/">原文链接</a></p>
        <div class="tut-main-content-pad">
<hr>
<h1>RNN LSTM 循环神经网络 (分类例子)</h1>
<p style="text-align: center;">
        
          作者: <strong>莫烦</strong>   
        
        编辑: <strong>莫烦</strong>   
        
          2016-11-03
        
      </p>
<p>学习资料:</p>
<ul>
<li><a href="https://github.com/MorvanZhou/tutorials/tree/master/tensorflowTUT/tf20_RNN2">相关代码</a></li>
<li>为 TF 2017 打造的<a href="https://github.com/MorvanZhou/Tensorflow-Tutorial">新版可视化教学代码</a></li>
<li>机器学习-简介系列 <a href="/tutorials/machine-learning/ML-intro/2-3-RNN/">什么是RNN</a></li>
<li>机器学习-简介系列 <a href="/tutorials/machine-learning/ML-intro/2-4-LSTM/">什么是LSTM RNN</a></li>
<li>本代码基于网上这一份代码 <a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py">code</a></li>
</ul>
<h4 class="tut-h4-pad" id="设置 RNN 的参数">设置 RNN 的参数</h4>
<p>这次我们会使用 RNN 来进行分类的训练 (Classification). 会继续使用到手写数字 MNIST 数据集. 让 RNN 从每张图片的第一行像素读到最后一行, 然后再进行分类判断.
接下来我们导入 MNIST 数据并确定 RNN 的各种参数(hyper-parameters):</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.examples.tutorials.mnist</span> <span class="kn">import</span> <span class="n">input_data</span>
<span class="n">tf</span><span class="o">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>   <span class="c"># set random seed</span>

<span class="c"># 导入数据</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">read_data_sets</span><span class="p">(</span><span class="s">'MNIST_data'</span><span class="p">,</span> <span class="n">one_hot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c"># hyperparameters</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>                  <span class="c"># learning rate</span>
<span class="n">training_iters</span> <span class="o">=</span> <span class="mi">100000</span>     <span class="c"># train step 上限</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>            
<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span>               <span class="c"># MNIST data input (img shape: 28*28)</span>
<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">28</span>                <span class="c"># time steps</span>
<span class="n">n_hidden_units</span> <span class="o">=</span> <span class="mi">128</span>        <span class="c"># neurons in hidden layer</span>
<span class="n">n_classes</span> <span class="o">=</span> <span class="mi">10</span>              <span class="c"># MNIST classes (0-9 digits)</span>
</code></pre>
</div>
<p>接着定义 <code class="highlighter-rouge">x</code>, <code class="highlighter-rouge">y</code> 的 <code class="highlighter-rouge">placeholder</code> 和 <code class="highlighter-rouge">weights</code>, <code class="highlighter-rouge">biases</code> 的初始状况.</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># x y placeholder</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">])</span>

<span class="c"># 对 weights biases 初始值的定义</span>
<span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c"># shape (28, 128)</span>
    <span class="s">'in'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_hidden_units</span><span class="p">])),</span>
    <span class="c"># shape (128, 10)</span>
    <span class="s">'out'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden_units</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">]))</span>
<span class="p">}</span>
<span class="n">biases</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c"># shape (128, )</span>
    <span class="s">'in'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">n_hidden_units</span><span class="p">,</span> <span class="p">])),</span>
    <span class="c"># shape (10, )</span>
    <span class="s">'out'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">n_classes</span><span class="p">,</span> <span class="p">]))</span>
<span class="p">}</span>
</code></pre>
</div>
<h4 class="tut-h4-pad" id="定义 RNN 的主体结构">定义 RNN 的主体结构</h4>
<p>接着开始定义 RNN 主体结构, 这个 RNN 总共有 3 个组成部分 ( <code class="highlighter-rouge">input_layer</code>, <code class="highlighter-rouge">cell</code>, <code class="highlighter-rouge">output_layer</code>). 首先我们先定义 <code class="highlighter-rouge">input_layer</code>:</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">RNN</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">):</span>
    <span class="c"># 原始的 X 是 3 维数据, 我们需要把它变成 2 维数据才能使用 weights 的矩阵乘法</span>
    <span class="c"># X ==&gt; (128 batches * 28 steps, 28 inputs)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">])</span>

    <span class="c"># X_in = W*X + b</span>
    <span class="n">X_in</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'in'</span><span class="p">])</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s">'in'</span><span class="p">]</span>
    <span class="c"># X_in ==&gt; (128 batches, 28 steps, 128 hidden) 换回3维</span>
    <span class="n">X_in</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_in</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">n_hidden_units</span><span class="p">])</span>
</code></pre>
</div>
<p>接着是 <code class="highlighter-rouge">cell</code> 中的计算, 有两种途径:</p>
<ol>
<li>使用 <code class="highlighter-rouge">tf.nn.rnn(cell, inputs)</code> (不推荐<a href="http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/">原因</a>). 但是如果使用这种方法, 可以参考<a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py">这个代码</a>;</li>
<li>使用 <code class="highlighter-rouge">tf.nn.dynamic_rnn(cell, inputs)</code> (推荐). 这次的练习将使用这种方式.</li>
</ol>
<p>因 Tensorflow 版本升级原因, <code class="highlighter-rouge">state_is_tuple=True</code> 将在之后的版本中变为默认. 对于 <code class="highlighter-rouge">lstm</code> 来说, <code class="highlighter-rouge">state</code>可被分为<code class="highlighter-rouge">(c_state, h_state)</code>.</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code>    <span class="c"># 使用 basic LSTM Cell.</span>
    <span class="n">lstm_cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">n_hidden_units</span><span class="p">,</span> <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">init_state</span> <span class="o">=</span> <span class="n">lstm_cell</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="c"># 初始化全零 state</span>
</code></pre>
</div>
<p>如果使用<code class="highlighter-rouge">tf.nn.dynamic_rnn(cell, inputs)</code>, 我们要确定 <code class="highlighter-rouge">inputs</code> 的格式. <code class="highlighter-rouge">tf.nn.dynamic_rnn</code> 中的 <code class="highlighter-rouge">time_major</code> 参数会针对不同 <code class="highlighter-rouge">inputs</code> 格式有不同的值.</p>
<ol>
<li>如果 <code class="highlighter-rouge">inputs</code> 为 (batches, steps, inputs) ==&gt; <code class="highlighter-rouge">time_major=False</code>;</li>
<li>如果 <code class="highlighter-rouge">inputs</code> 为 (steps, batches, inputs) ==&gt; <code class="highlighter-rouge">time_major=True</code>;</li>
</ol>
<div class="language-python highlighter-rouge"><pre class="highlight"><code>    <span class="n">outputs</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dynamic_rnn</span><span class="p">(</span><span class="n">lstm_cell</span><span class="p">,</span> <span class="n">X_in</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">init_state</span><span class="p">,</span> <span class="n">time_major</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre>
</div>
<p>最后是 <code class="highlighter-rouge">output_layer</code> 和 <code class="highlighter-rouge">return</code> 的值. 因为这个例子的特殊性, 有两种方法可以求得 <code class="highlighter-rouge">results</code>.</p>
<p><strong>方式一:</strong>
直接调用<code class="highlighter-rouge">final_state</code> 中的 <code class="highlighter-rouge">h_state</code> (<code class="highlighter-rouge">final_state[1]</code>) 来进行运算:</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code>    <span class="n">results</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">final_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">weights</span><span class="p">[</span><span class="s">'out'</span><span class="p">])</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s">'out'</span><span class="p">]</span>
</code></pre>
</div>
<p><strong>方式二:</strong>
调用最后一个 <code class="highlighter-rouge">outputs</code> (在这个例子中,和上面的<code class="highlighter-rouge">final_state[1]</code>是一样的):</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code>    <span class="c"># 把 outputs 变成 列表 [(batch, outputs)..] * steps</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]))</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">weights</span><span class="p">[</span><span class="s">'out'</span><span class="p">])</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s">'out'</span><span class="p">]</span>    <span class="c">#选取最后一个 output</span>
</code></pre>
</div>
<p>在 <code class="highlighter-rouge">def RNN()</code> 的最后输出 <code class="highlighter-rouge">result</code></p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code>    <span class="k">return</span> <span class="n">results</span>
</code></pre>
</div>
<p>定义好了 RNN 主体结构后, 我们就可以来计算 <code class="highlighter-rouge">cost</code> 和 <code class="highlighter-rouge">train_op</code>:</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">pred</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
</code></pre>
</div>
<h4 class="tut-h4-pad" id="训练 RNN">训练 RNN</h4>
<p>训练时, 不断输出 <code class="highlighter-rouge">accuracy</code>, 观看结果:</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">correct_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct_pred</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="c"># init= tf.initialize_all_variables() # tf 马上就要废弃这种写法</span>
<span class="c"># 替换成下面的写法:</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
    <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">step</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">&lt;</span> <span class="n">training_iters</span><span class="p">:</span>
        <span class="n">batch_xs</span><span class="p">,</span> <span class="n">batch_ys</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">batch_xs</span> <span class="o">=</span> <span class="n">batch_xs</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">])</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">train_op</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
            <span class="n">x</span><span class="p">:</span> <span class="n">batch_xs</span><span class="p">,</span>
            <span class="n">y</span><span class="p">:</span> <span class="n">batch_ys</span><span class="p">,</span>
        <span class="p">})</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
            <span class="n">x</span><span class="p">:</span> <span class="n">batch_xs</span><span class="p">,</span>
            <span class="n">y</span><span class="p">:</span> <span class="n">batch_ys</span><span class="p">,</span>
        <span class="p">}))</span>
        <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre>
</div>
<p>最终 <code class="highlighter-rouge">accuracy</code> 的结果如下:</p>
<div class="highlighter-rouge"><pre class="highlight"><code>0.1875
0.65625
0.726562
0.757812
0.820312
0.796875
0.859375
0.921875
0.921875
0.898438
0.828125
0.890625
0.9375
0.921875
0.9375
0.929688
0.953125
....
</code></pre>
</div>
<p style="font-size: 0.8em; padding:4em 1em 0.5em 1em; margin: 0 auto;">
        如果你觉得这篇文章或视频对你的学习很有帮助, 请你也分享它, 让它能再次帮助到更多的需要学习的人.

        莫烦没有正式的经济来源, 如果你也想支持 <strong>莫烦Python</strong> 并看到更好的教学内容, 赞助他一点点, 作为鼓励他继续开源的动力.
      </p>
<!-- donation -->
<div id="bottom-donation-section">
<h3 id="bottom-donation-title">支持 让教学变得更优秀</h3>
<br>
<div>
<a href="/support/" id="bottom-donation-button"><strong>点我 赞助 莫烦</strong></a>
</div>
<br>
</br></br></div>
<hr>
</hr></hr></div>
    </body></html>